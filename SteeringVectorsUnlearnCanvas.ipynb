{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering Vectors for FLUX - UnlearnCanvas Benchmark Evaluation\n",
    "\n",
    "This notebook implements steering vectors for concept removal in FLUX and evaluates them using the **UnlearnCanvas benchmark** protocol.\n",
    "\n",
    "## Key Metrics (from UnlearnCanvas paper):\n",
    "- **UA (Unlearning Accuracy)**: Proportion of images NOT classified as target concept (higher = better unlearning)\n",
    "- **IRA (In-domain Retain Accuracy)**: Classification accuracy for other concepts in same domain (higher = better retention)\n",
    "- **CRA (Cross-domain Retain Accuracy)**: Classification accuracy for concepts in different domain (higher = better retention)\n",
    "- **FID**: Image quality metric (lower = better)\n",
    "- **CLIP Score**: Text-image alignment (higher = better)\n",
    "\n",
    "## Classification Method:\n",
    "This notebook uses **LLaVA-1.6-Vicuna-7B** as the classifier, following the methodology from the **TRACE paper** (ICLR 2026).\n",
    "\n",
    "The TRACE paper shows that UnlearnCanvas's original SD1.5-trained classifier generalizes poorly to modern models like FLUX (<6% accuracy). LLaVA provides accurate zero-shot classification using a numbered-list prompt format (see Appendix E.4, Figures 6-7 of TRACE paper).\n",
    "\n",
    "## Evaluation Approach:\n",
    "We generate images ourselves using FLUX + steering vectors, then evaluate using UnlearnCanvas protocol with LLaVA classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: INSTALLATIONS\n",
    "# ============================================================================\n",
    "\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install diffusers transformers accelerate --quiet\n",
    "!pip install clean-fid --quiet\n",
    "!pip install git+https://github.com/openai/CLIP.git --quiet\n",
    "!pip install timm --quiet\n",
    "!pip install pandas matplotlib pillow tqdm --quiet\n",
    "\n",
    "print(\"\u2713 All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from diffusers import FluxPipeline\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import contextmanager\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import gc\n",
    "from cleanfid import fid\n",
    "import clip\n",
    "from torchvision import transforms\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# GOOGLE DRIVE SETUP (Optional - for Colab)\n",
    "# ============================================================================\n",
    "USE_GOOGLE_DRIVE = True\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/UnlearnCanvas_Steering\"\n",
    "\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        os.makedirs(DRIVE_PATH, exist_ok=True)\n",
    "        ROOT_DIR = DRIVE_PATH\n",
    "        print(f\"\u2713 Google Drive mounted at: {ROOT_DIR}\")\n",
    "    except:\n",
    "        print(\"\u26a0 Not in Colab or Drive mounting failed. Using local storage.\")\n",
    "        ROOT_DIR = \".\"\n",
    "else:\n",
    "    ROOT_DIR = \".\"\n",
    "\n",
    "# ============================================================================\n",
    "# UNLEARNCANVAS BENCHMARK CONFIGURATION\n",
    "# Following the official UnlearnCanvas dataset structure:\n",
    "# - 60 styles (we use subset of 10 for efficiency)\n",
    "# - 20 object classes\n",
    "# ============================================================================\n",
    "\n",
    "# Full 60 styles from UnlearnCanvas (subset used for experiments)\n",
    "ALL_STYLES = [\n",
    "    \"Abstractionism\", \"Art_Brut\", \"Art_Deco\", \"Art_Informel\", \"Art_Nouveau\",\n",
    "    \"Baroque\", \"Biedermeier\", \"Byzantine\", \"Cartoon\", \"Classicism\",\n",
    "    \"Color_Field_Painting\", \"Constructivism\", \"Crayon\", \"Cubism\", \"Dadaism\",\n",
    "    \"Divisionism\", \"Early_Renaissance\", \"Expressionism\", \"Fauvism\", \"Graffiti\",\n",
    "    \"High_Renaissance\", \"Impressionism\", \"International_Gothic\", \"Japonism\", \"Lyrical_Abstraction\",\n",
    "    \"Magic_Realism\", \"Mannerism\", \"Minimalism\", \"Naive_Art\", \"Neo-Baroque\",\n",
    "    \"Neo-Expressionism\", \"Neo-Impressionism\", \"Neo-Romanticism\", \"Neoclassicism\", \"Northern_Renaissance\",\n",
    "    \"Orphism\", \"Photo\", \"Pop_Art\", \"Post-Impressionism\", \"Post-Minimalism\",\n",
    "    \"Precision\", \"Primitivism\", \"Realism\", \"Rococo\", \"Romanesque\",\n",
    "    \"Romanticism\", \"Sketch\", \"Social_Realism\", \"Spatialism\", \"Suprematism\",\n",
    "    \"Surrealism\", \"Symbolism\", \"Synthetism\", \"Tachisme\", \"Ukiyoe\",\n",
    "    \"Van_Gogh\", \"Warm_Love\", \"Watercolor\", \"Winter\", \"Bricks\"\n",
    "]\n",
    "\n",
    "# 10 styles from TRACE paper (ICLR 2026) for FLUX evaluation\n",
    "# Reference: TRACE Section 5.1 - main eval on Flux/SD3.5/Infinity\n",
    "# NOTE: TRACE Figure 6 (LLaVA prompt) shows 'Picasso' instead of 'Watercolor'\n",
    "# but Section 5.1 explicitly lists 'Watercolor' for the main FLUX evaluation.\n",
    "# 'Watercolor' is in the original 60 UnlearnCanvas styles; 'Picasso' is NOT.\n",
    "STYLES = [\n",
    "    \"Van_Gogh\", \"Watercolor\", \"Cartoon\", \"Cubism\", \"Winter\",\n",
    "    \"Pop_Art\", \"Ukiyoe\", \"Impressionism\", \"Byzantine\", \"Bricks\"\n",
    "]\n",
    "\n",
    "# All 20 object classes from UnlearnCanvas/TRACE paper (Figure 7)\n",
    "# Using singular form to match TRACE paper prompts exactly\n",
    "OBJECTS = [\n",
    "    \"Architecture\", \"Bear\", \"Bird\", \"Butterfly\", \"Cat\", \"Dog\",\n",
    "    \"Fish\", \"Flame\", \"Flowers\", \"Frog\", \"Horse\", \"Human\",\n",
    "    \"Jellyfish\", \"Rabbits\", \"Sandwich\", \"Sea\", \"Statue\",\n",
    "    \"Tower\", \"Tree\", \"Waterfalls\"\n",
    "]\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\u2713 Using device: {DEVICE}\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = \"black-forest-labs/FLUX.1-schnell\"\n",
    "N_STEPS = 4 if \"schnell\" in MODEL_ID.lower() else 28\n",
    "\n",
    "# Steering vector configuration\n",
    "LEARNING_SEEDS = list(range(0, 20))  # 20 seeds for learning vectors\n",
    "EVAL_SEEDS = list(range(20, 23))     # 3 seeds for eval (increase for paper)\n",
    "GLOBAL_BETA = 2.0                     # Steering strength (from CASteer paper)\n",
    "TOP_K_VECTORS = 15                    # Top-k steering vectors to use\n",
    "\n",
    "# ============================================================================\n",
    "# IMAGENET CLASSES FOR DIVERSE PROMPT PAIRS (from CASteer paper)\n",
    "# CASteer uses 50 ImageNet classes as base contexts for computing steering\n",
    "# vectors. This ensures the contrastive vector isolates the TARGET concept\n",
    "# rather than prompt-specific features. Critical for object unlearning.\n",
    "# ============================================================================\n",
    "IMAGENET_CLASSES = [\n",
    "    \"tench\", \"goldfish\", \"tiger shark\", \"hammerhead\", \"electric ray\",\n",
    "    \"hen\", \"ostrich\", \"brambling\", \"goldfinch\", \"house finch\",\n",
    "    \"junco\", \"indigo bunting\", \"robin\", \"bulbul\", \"jay\",\n",
    "    \"magpie\", \"chickadee\", \"water ouzel\", \"kite\", \"bald eagle\",\n",
    "    \"vulture\", \"great grey owl\", \"mud turtle\", \"box turtle\", \"banded gecko\",\n",
    "    \"common iguana\", \"whiptail lizard\", \"agama\", \"frilled lizard\", \"alligator lizard\",\n",
    "    \"green mamba\", \"thunder snake\", \"ringneck snake\", \"king snake\", \"garter snake\",\n",
    "    \"vine snake\", \"trilobite\", \"scorpion\", \"black widow\", \"tarantula\",\n",
    "    \"centipede\", \"grouse\", \"peacock\", \"quail\", \"partridge\",\n",
    "    \"macaw\", \"lorikeet\", \"coucal\", \"bee eater\", \"hornbill\"\n",
    "]\n",
    "\n",
    "def make_object_prompts(concept, num_prompts=50):\n",
    "    \"\"\"\n",
    "    Generate diverse prompt pairs for OBJECT concept steering (CASteer-style).\n",
    "    \n",
    "    Uses ImageNet classes as diverse base contexts:\n",
    "      Positive: \"tench with Dog\", \"goldfish with Dog\", ...\n",
    "      Negative: \"tench\", \"goldfish\", ...\n",
    "    \n",
    "    Averaging across many contexts ensures the contrastive vector isolates\n",
    "    the target object, not prompt-specific noise (layout, composition, etc.).\n",
    "    \n",
    "    Args:\n",
    "        concept: Object name (e.g., \"Dog\", \"Cat\")\n",
    "        num_prompts: Number of diverse prompt pairs (default: 50, as in CASteer)\n",
    "    \n",
    "    Returns:\n",
    "        List of (pos_prompt, neg_prompt) tuples\n",
    "    \"\"\"\n",
    "    n = min(num_prompts, len(IMAGENET_CLASSES))\n",
    "    pairs = []\n",
    "    for cls in IMAGENET_CLASSES[:n]:\n",
    "        pairs.append((f\"{cls} with {concept}\", f\"{cls}\"))\n",
    "    return pairs\n",
    "\n",
    "def make_style_prompts(concept, num_prompts=50):\n",
    "    \"\"\"\n",
    "    Generate diverse prompt pairs for STYLE concept steering (CASteer-style).\n",
    "    \n",
    "    Uses ImageNet classes as diverse base contexts:\n",
    "      Positive: \"tench, Van Gogh style\", \"goldfish, Van Gogh style\", ...\n",
    "      Negative: \"tench\", \"goldfish\", ...\n",
    "    \n",
    "    Args:\n",
    "        concept: Style name (e.g., \"Van Gogh\", \"Cartoon\")\n",
    "        num_prompts: Number of diverse prompt pairs (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "        List of (pos_prompt, neg_prompt) tuples\n",
    "    \"\"\"\n",
    "    n = min(num_prompts, len(IMAGENET_CLASSES))\n",
    "    pairs = []\n",
    "    for cls in IMAGENET_CLASSES[:n]:\n",
    "        pairs.append((f\"{cls}, {concept} style\", f\"{cls}\"))\n",
    "    return pairs\n",
    "\n",
    "NUM_DIVERSE_PROMPTS = 50  # Number of diverse prompt pairs for learning\n",
    "\n",
    "# Set True to run full benchmark across ALL 10 styles in Cell 13.\n",
    "# WARNING: generates 10x20x3 = 600 images PER target + LLaVA classification.\n",
    "RUN_FULL_BENCHMARK = False\n",
    "\n",
    "# Directory structure\n",
    "for subdir in [\"steering_vectors\", \"results\", \"baseline_images\", \"steered_images\", \"tables\"]:\n",
    "    os.makedirs(os.path.join(ROOT_DIR, subdir), exist_ok=True)\n",
    "\n",
    "VECTOR_DIR = os.path.join(ROOT_DIR, \"steering_vectors\")\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, \"results\")\n",
    "BASELINE_DIR = os.path.join(ROOT_DIR, \"baseline_images\")\n",
    "STEERED_DIR = os.path.join(ROOT_DIR, \"steered_images\")\n",
    "TABLES_DIR = os.path.join(ROOT_DIR, \"tables\")\n",
    "RESULTS_CSV = os.path.join(ROOT_DIR, \"benchmark_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UNLEARNCANVAS BENCHMARK CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Inference steps: {N_STEPS}\")\n",
    "print(f\"Learning seeds: {len(LEARNING_SEEDS)}\")\n",
    "print(f\"Evaluation seeds: {len(EVAL_SEEDS)}\")\n",
    "print(f\"Styles to evaluate: {len(STYLES)}\")\n",
    "print(f\"Objects to evaluate: {len(OBJECTS)}\")\n",
    "print(f\"Steering strength (\u03b2): {GLOBAL_BETA}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: FLUXSTEERING CLASS (4 MODES)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "FluxSteering: Implements activation steering for FLUX diffusion models.\n",
    "\n",
    "Supports four steering modes:\n",
    "\n",
    "MODE 1: \"entry_point\" (TRACE-paper-inspired, GAP-bug FIXED)\n",
    "    Steers at the two text-embedding entry points:\n",
    "      - context_embedder: Linear(4096 \u2192 3072) \u2014 projects T5 encoder output\n",
    "      - time_text_embed: MLP \u2014 fuses timestep + CLIP pooled text\n",
    "    CRITICAL FIX: Uses T5 attention mask so that only non-padding tokens\n",
    "    contribute to the steering vector for context_embedder (fixes the\n",
    "    Padding-Inclusive GAP error that collapsed steering magnitude by ~85x).\n",
    "\n",
    "MODE 2: \"block\" (CASteer-style, SingleStream modality-broadcast FIXED)\n",
    "    Steers at attention-output projections in double-stream (attn.to_out[0])\n",
    "    and single-stream (proj_out) blocks.\n",
    "    FIX: When learning from SingleStream, only the text slice is pooled\n",
    "    (with T5 mask). When applying, steering is only added to the text slice,\n",
    "    avoiding the Modality Broadcast Hazard.\n",
    "\n",
    "MODE 3: \"double_proj\" (EraseAnything-style \u2014 best for object unlearning)\n",
    "    Steers at add_k_proj and add_q_proj in the 19 DoubleStream blocks.\n",
    "    These are the text-side Key and Query projections in joint attention.\n",
    "    Breaking the query-key retrieval loop is more effective for objects.\n",
    "    Uses T5 attention mask for mask-aware pooling.\n",
    "\n",
    "MODE 4: \"all\" (combined: entry_point + double_proj + block double-stream)\n",
    "    Steers at all text-dependent surfaces simultaneously.\n",
    "    Maximum coverage; useful for hard-to-remove concepts.\n",
    "\n",
    "MODE 5: \"hybrid\" (TRACE entry points + CASteer-adapted add_k/add_q) \u2190 DEFAULT FOR STYLE\n",
    "    Combines TRACE and CASteer methodologies for FLUX:\n",
    "      - TRACE: context_embedder + time_text_embed (text entry bottlenecks)\n",
    "      - CASteer-adapted: add_k_proj + add_q_proj in 19 DoubleStream blocks\n",
    "        (where text keys/queries enter joint attention with image tokens)\n",
    "    Uses mask-aware pooling. Keeps all vectors (no top_k filtering).\n",
    "    Best for STYLE unlearning (style flows through modulation path via temb).\n",
    "\n",
    "MODE 6: \"object\" (image-side steering for object unlearning)\n",
    "    Steers at IMAGE-SIDE representations where objects are actually constructed:\n",
    "      - add_v_proj in 19 DoubleStream blocks (text Values \u2014 carries object\n",
    "        content that gets transferred to image tokens through attention)\n",
    "      - to_out[0] in 19 DoubleStream blocks (image attention output \u2014 the\n",
    "        image representation AFTER attending to text, analogous to CASteer)\n",
    "      - proj_out IMAGE SLICE in 38 SingleStream blocks (the crucial missing\n",
    "        piece \u2014 image tokens that have absorbed object info from text through\n",
    "        joint attention, which no other mode steers)\n",
    "    Uses top_k selection. Renormalizes activations after steering.\n",
    "    Best for OBJECT unlearning (objects flow through attention path).\n",
    "\n",
    "MODE 7: \"object_v2\" (CASteer+TRACE combined for object unlearning)\n",
    "    Combines insights from CASteer, TRACE, and FLUX architecture analysis:\n",
    "      - TRACE entry points: context_embedder + time_text_embed (prevents\n",
    "        object info from entering the network at the source)\n",
    "      - CASteer analog: to_out[0] in 19 DoubleStream blocks (image attention\n",
    "        output \u2014 the exact equivalent of CASteer's cross-attention output in SD.\n",
    "        CASteer ablation shows this is MORE effective than steering K/V directly)\n",
    "      - SingleStream image: proj_out IMAGE SLICE in 38 SingleStream blocks\n",
    "        (image tokens that absorbed object info through joint attention)\n",
    "    Key differences from \"object\" mode:\n",
    "      - ADDS entry points (TRACE found these necessary even for objects on FLUX)\n",
    "      - DROPS add_v_proj (CASteer ablation Table 16 shows K/V steering is\n",
    "        LESS effective than steering attention output: CS=62.79 vs CS=48.7)\n",
    "      - Designed for DIVERSE PROMPT PAIRS via learn_vectors_diverse()\n",
    "    Uses renormalization. Uses top_k selection.\n",
    "    REQUIRES: Call learn_vectors_diverse() with prompt pairs from\n",
    "    make_object_prompts() for best results. Single-prompt learning will NOT\n",
    "    work well for objects.\n",
    "\n",
    "MODE 8: \"joint_attn\" (double-stream attention output ONLY \u2014 simplest approach)\n",
    "    Replicates the colleague's approach that showed promise for Dog unlearning:\n",
    "      - ONLY hooks attn.to_out[0] in 19 DoubleStream blocks (joint attention output)\n",
    "      - Simple mean(dim=(0,1)) pooling \u2014 no mask-aware pooling, no slice separation\n",
    "      - Top-k selection (default 15) by activation strength\n",
    "      - NO renormalization during apply \u2014 simple projection removal only\n",
    "    Key philosophy: minimal intervention at the most architecturally relevant point.\n",
    "    In FLUX, to_out[0] is the image-stream output AFTER joint attention with text \u2014\n",
    "    the closest analog to SD's cross-attention output that CASteer targets.\n",
    "    Uses standard learn_vectors() with simple prompt pairs (e.g., \"Dog\" / \"Object\").\n",
    "\"\"\"\n",
    "\n",
    "class FluxSteering:\n",
    "    \"\"\"\n",
    "    Multi-mode FluxSteering class for inference-time concept removal in FLUX.\n",
    "\n",
    "    Modes: \"entry_point\", \"block\", \"double_proj\", \"all\", \"hybrid\", \"object\", \"object_v2\", \"joint_attn\"\n",
    "    \"\"\"\n",
    "\n",
    "    VALID_MODES = (\"entry_point\", \"block\", \"double_proj\", \"all\", \"hybrid\", \"object\", \"object_v2\", \"joint_attn\")\n",
    "\n",
    "    def __init__(self, pipe, device=\"cuda\", n_steps=4, mode=\"hybrid\"):\n",
    "        self.pipe = pipe\n",
    "        self.device = device\n",
    "        self.n_steps = n_steps\n",
    "        self.mode = mode\n",
    "        self._current_step = -1\n",
    "        self._handles = []\n",
    "        # Will hold the T5 attention mask during learning passes\n",
    "        self._current_attention_mask = None\n",
    "        # Will hold the image-sequence length for SingleStream text slicing\n",
    "        self._img_seq_len = None\n",
    "\n",
    "        if mode not in self.VALID_MODES:\n",
    "            raise ValueError(\n",
    "                f\"Unknown mode '{mode}'. Choose from {self.VALID_MODES}.\"\n",
    "            )\n",
    "\n",
    "        # ==============================================================\n",
    "        # Resolve all layer references we might need\n",
    "        # ==============================================================\n",
    "\n",
    "        # --- Entry-point layers (used by entry_point / all) ---\n",
    "        self.target_layers = {\n",
    "            \"context_embedder\": pipe.transformer.context_embedder,\n",
    "            \"time_text_embed\": pipe.transformer.time_text_embed,\n",
    "        }\n",
    "\n",
    "        # --- DoubleStream blocks ---\n",
    "        self.double_layers = [\n",
    "            m for m in pipe.transformer.modules()\n",
    "            if m.__class__.__name__ == \"FluxTransformerBlock\"\n",
    "        ]\n",
    "        self.double_layer_idxs = list(range(len(self.double_layers)))\n",
    "\n",
    "        # attn output projections (block mode)\n",
    "        self.double_proj_layers = {\n",
    "            li: self.double_layers[li].attn.to_out[0]\n",
    "            for li in self.double_layer_idxs\n",
    "        }\n",
    "\n",
    "        # text-side Key, Query, and Value projections\n",
    "        self.double_add_k = {}\n",
    "        self.double_add_q = {}\n",
    "        self.double_add_v = {}\n",
    "        for li in self.double_layer_idxs:\n",
    "            attn = self.double_layers[li].attn\n",
    "            if hasattr(attn, \"add_k_proj\"):\n",
    "                self.double_add_k[li] = attn.add_k_proj\n",
    "            if hasattr(attn, \"add_q_proj\"):\n",
    "                self.double_add_q[li] = attn.add_q_proj\n",
    "            if hasattr(attn, \"add_v_proj\"):\n",
    "                self.double_add_v[li] = attn.add_v_proj\n",
    "\n",
    "        # --- SingleStream blocks ---\n",
    "        self.single_layers = [\n",
    "            m for m in pipe.transformer.modules()\n",
    "            if m.__class__.__name__ == \"FluxSingleTransformerBlock\"\n",
    "        ]\n",
    "        self.single_layer_idxs = list(range(len(self.single_layers)))\n",
    "        self.single_proj_layers = {\n",
    "            li: self.single_layers[li].proj_out\n",
    "            for li in self.single_layer_idxs\n",
    "        }\n",
    "\n",
    "        # --- Print summary ---\n",
    "        summary = {\n",
    "            \"entry_point\": (\n",
    "                f\"  - context_embedder ({pipe.transformer.context_embedder.__class__.__name__})\\n\"\n",
    "                f\"  - time_text_embed ({pipe.transformer.time_text_embed.__class__.__name__})\\n\"\n",
    "                f\"  - Control points: 2 + mask-aware pooling (GAP fix)\"\n",
    "            ),\n",
    "            \"block\": (\n",
    "                f\"  - Double stream (to_out): {len(self.double_layers)} blocks\\n\"\n",
    "                f\"  - Single stream (proj_out): {len(self.single_layers)} blocks (text-only slice)\\n\"\n",
    "                f\"  - Total: {len(self.double_layers) + len(self.single_layers)} blocks\"\n",
    "            ),\n",
    "            \"double_proj\": (\n",
    "                f\"  - add_k_proj: {len(self.double_add_k)} layers\\n\"\n",
    "                f\"  - add_q_proj: {len(self.double_add_q)} layers\\n\"\n",
    "                f\"  - Control points: {len(self.double_add_k) + len(self.double_add_q)} + mask-aware pooling\"\n",
    "            ),\n",
    "            \"all\": (\n",
    "                f\"  - entry_point (2) + double_proj ({len(self.double_add_k) + len(self.double_add_q)}) \"\n",
    "                f\"+ double to_out ({len(self.double_layers)})\\n\"\n",
    "                f\"  - Total control points: \"\n",
    "                f\"{2 + len(self.double_add_k) + len(self.double_add_q) + len(self.double_layers)}\"\n",
    "            ),\n",
    "            \"hybrid\": (\n",
    "                f\"  - TRACE entry points: context_embedder + time_text_embed (2)\\n\"\n",
    "                f\"  - CASteer-adapted: add_k_proj ({len(self.double_add_k)}) + add_q_proj ({len(self.double_add_q)})\\n\"\n",
    "                f\"  - Total control points: {2 + len(self.double_add_k) + len(self.double_add_q)} + mask-aware pooling\"\n",
    "            ),\n",
    "            \"object\": (\n",
    "                f\"  - DoubleStream add_v_proj: {len(self.double_add_v)} layers (text Values \u2014 object content)\\n\"\n",
    "                f\"  - DoubleStream to_out[0]: {len(self.double_proj_layers)} layers (image attn output)\\n\"\n",
    "                f\"  - SingleStream proj_out IMAGE slice: {len(self.single_proj_layers)} layers (image tokens)\\n\"\n",
    "                f\"  - Total control points: {len(self.double_add_v) + len(self.double_proj_layers) + len(self.single_proj_layers)}\"\n",
    "                f\" + renormalization\"\n",
    "            ),\n",
    "            \"object_v2\": (\n",
    "                f\"  - TRACE entry points: context_embedder + time_text_embed (2)\\n\"\n",
    "                f\"  - CASteer analog: to_out[0] in {len(self.double_proj_layers)} DoubleStream blocks (image attn output)\\n\"\n",
    "                f\"  - SingleStream proj_out IMAGE slice: {len(self.single_proj_layers)} blocks (image tokens)\\n\"\n",
    "                f\"  - Total: {2 + len(self.double_proj_layers) + len(self.single_proj_layers)} control points + renormalization\\n\"\n",
    "                f\"  - REQUIRES: learn_vectors_diverse() with diverse prompt pairs\"\n",
    "            ),\n",
    "            \"joint_attn\": (\n",
    "                f\"  - DoubleStream attn.to_out[0]: {len(self.double_proj_layers)} blocks (joint attention output)\\n\"\n",
    "                f\"  - Simple mean pooling, top-k selection, NO renormalization\\n\"\n",
    "                f\"  - Minimal approach: only the image-side attention output after joint text+image attention\"\n",
    "            ),\n",
    "        }\n",
    "        print(f\"\u2713 FluxSteering initialized (mode={mode}):\")\n",
    "        print(summary[mode])\n",
    "\n",
    "    # ==================================================================\n",
    "    # Internal helpers\n",
    "    # ==================================================================\n",
    "    def _on_step_end(self, pipe, step, timestep, callback_kwargs):\n",
    "        \"\"\"Track current denoising step.\"\"\"\n",
    "        self._current_step = int(step.item()) if torch.is_tensor(step) else int(step)\n",
    "        return callback_kwargs\n",
    "\n",
    "    def _clear_hooks(self):\n",
    "        for h in self._handles:\n",
    "            h.remove()\n",
    "        self._handles = []\n",
    "\n",
    "    def _get_t5_mask(self, prompt):\n",
    "        \"\"\"\n",
    "        Get T5 attention mask for a prompt using the pipeline's tokenizer.\n",
    "        Uses the SAME settings as FluxPipeline._get_t5_prompt_embeds():\n",
    "          padding=\"max_length\", max_length=512, truncation=True\n",
    "        Returns: Tensor of shape (1, max_seq_len) on self.device.\n",
    "        \"\"\"\n",
    "        tokenizer = self.pipe.tokenizer_2\n",
    "        # Determine max_sequence_length the same way the pipeline does\n",
    "        max_seq = getattr(self.pipe, '_max_sequence_length', 512)\n",
    "        # tokenize\n",
    "        tok_out = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_seq,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        mask = tok_out.attention_mask  # (1, max_seq)\n",
    "        n_real = int(mask.sum().item())\n",
    "        return mask.to(self.device), n_real\n",
    "\n",
    "    def _masked_mean(self, act, mask):\n",
    "        \"\"\"\n",
    "        Compute mean activation over non-padding token positions.\n",
    "        act: (B, S, D), mask: (B, S) with 1 for real tokens, 0 for padding.\n",
    "        Returns: (D,) mean vector.\n",
    "        \"\"\"\n",
    "        mask_f = mask.to(device=act.device, dtype=act.dtype)  # (B, S)\n",
    "        mask_exp = mask_f.unsqueeze(-1)  # (B, S, 1)\n",
    "        # sum over batch and sequence, weighted by mask\n",
    "        weighted = (act * mask_exp).sum(dim=(0, 1))  # (D,)\n",
    "        count = mask_exp.sum(dim=(0, 1)).clamp(min=1.0)  # (D,) \u2014 same value per dim\n",
    "        return weighted / count\n",
    "\n",
    "    def _run_pipe_base(self, prompt, seed, steps=None):\n",
    "        \"\"\"Run the pipeline (no steering). Returns PIL Image.\"\"\"\n",
    "        steps = steps or self.n_steps\n",
    "        self._current_step = -1\n",
    "        g = torch.Generator(device=self.device).manual_seed(seed)\n",
    "        return self.pipe(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=steps,\n",
    "            generator=g,\n",
    "            callback_on_step_end=self._on_step_end\n",
    "        ).images[0]\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN VECTORS \u2014 dispatcher\n",
    "    # ==================================================================\n",
    "    @torch.no_grad()\n",
    "    def learn_vectors(self, pos_prompt, neg_prompt, seeds, top_k=15, verbose=True):\n",
    "        \"\"\"\n",
    "        Learn steering vectors from positive/negative prompt pairs.\n",
    "\n",
    "        v = E_seeds[ activation(pos_prompt) ] - E_seeds[ activation(neg_prompt) ]\n",
    "\n",
    "        Uses mask-aware pooling for any layer whose output has a T5 sequence\n",
    "        dimension (context_embedder, add_k_proj, add_q_proj, SingleStream text slice).\n",
    "        \"\"\"\n",
    "        if self.mode == \"entry_point\":\n",
    "            return self._learn_entry_point(pos_prompt, neg_prompt, seeds, verbose)\n",
    "        elif self.mode == \"block\":\n",
    "            return self._learn_block(pos_prompt, neg_prompt, seeds, top_k, verbose)\n",
    "        elif self.mode == \"double_proj\":\n",
    "            return self._learn_double_proj(pos_prompt, neg_prompt, seeds, top_k, verbose)\n",
    "        elif self.mode == \"all\":\n",
    "            return self._learn_all(pos_prompt, neg_prompt, seeds, top_k, verbose)\n",
    "        elif self.mode == \"hybrid\":\n",
    "            return self._learn_hybrid(pos_prompt, neg_prompt, seeds, verbose)\n",
    "        elif self.mode == \"object\":\n",
    "            return self._learn_object(pos_prompt, neg_prompt, seeds, top_k, verbose)\n",
    "        elif self.mode == \"object_v2\":\n",
    "            print(\"WARNING: object_v2 mode works best with learn_vectors_diverse().\")\n",
    "            print(\"         Using single-prompt learning as fallback, but results may be poor.\")\n",
    "            return self._learn_object_v2(pos_prompt, neg_prompt, seeds, top_k, verbose)\n",
    "        elif self.mode == \"joint_attn\":\n",
    "            return self._learn_joint_attn(pos_prompt, neg_prompt, seeds, top_k, verbose)\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: entry_point (Phase 1 \u2014 GAP fix)\n",
    "    # ==================================================================\n",
    "    def _learn_entry_point(self, pos_prompt, neg_prompt, seeds, verbose):\n",
    "        \"\"\"Learn vectors at context_embedder and time_text_embed with mask-aware pooling.\"\"\"\n",
    "        # Get T5 attention masks BEFORE running the pipeline\n",
    "        pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "        neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "        if verbose:\n",
    "            print(f\"T5 mask: pos has {pos_n} real tokens, neg has {neg_n} real tokens (out of {pos_mask.shape[1]})\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def collect_hook(layer_name, sign):\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    if layer_name == \"context_embedder\" and act.dim() == 3:\n",
    "                        # MASK-AWARE pooling \u2014 only non-padding positions\n",
    "                        mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                    else:\n",
    "                        # time_text_embed: (B, D) \u2014 no sequence dim, just batch-mean\n",
    "                        mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                    mean_diffs[layer_name][step] += (sign * mean_act)\n",
    "                    counts[layer_name][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        try:\n",
    "            # Positive pass\n",
    "            for seed in tqdm(seeds, desc=\"Learning (positive)\", disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = pos_mask\n",
    "                for name, mod in self.target_layers.items():\n",
    "                    self._handles.append(\n",
    "                        mod.register_forward_hook(collect_hook(name, +1))\n",
    "                    )\n",
    "                self._run_pipe_base(pos_prompt, seed)\n",
    "\n",
    "            # Negative pass\n",
    "            for seed in tqdm(seeds, desc=\"Learning (negative)\", disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = neg_mask\n",
    "                for name, mod in self.target_layers.items():\n",
    "                    self._handles.append(\n",
    "                        mod.register_forward_hook(collect_hook(name, -1))\n",
    "                    )\n",
    "                self._run_pipe_base(neg_prompt, seed)\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        return self._build_vectors_keep_all(mean_diffs, counts, len(seeds), verbose,\n",
    "                                             title=\"Entry-Point Steering Vectors (mask-aware)\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: double_proj (Phase 2 \u2014 add_k_proj + add_q_proj)\n",
    "    # ==================================================================\n",
    "    def _learn_double_proj(self, pos_prompt, neg_prompt, seeds, top_k, verbose):\n",
    "        \"\"\"Learn vectors at add_k_proj and add_q_proj in DoubleStream (mask-aware).\"\"\"\n",
    "        pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "        neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "        if verbose:\n",
    "            print(f\"T5 mask: pos has {pos_n} real tokens, neg has {neg_n} real tokens (out of {pos_mask.shape[1]})\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def collect_hook(layer_id, sign):\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    if act.dim() == 3:\n",
    "                        mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                    else:\n",
    "                        mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                    mean_diffs[layer_id][step] += (sign * mean_act)\n",
    "                    counts[layer_id][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        try:\n",
    "            for seed in tqdm(seeds, desc=\"Learning (positive)\", disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = pos_mask\n",
    "                for li, mod in self.double_add_k.items():\n",
    "                    self._handles.append(mod.register_forward_hook(collect_hook(f\"add_k_{li}\", +1)))\n",
    "                for li, mod in self.double_add_q.items():\n",
    "                    self._handles.append(mod.register_forward_hook(collect_hook(f\"add_q_{li}\", +1)))\n",
    "                self._run_pipe_base(pos_prompt, seed)\n",
    "\n",
    "            for seed in tqdm(seeds, desc=\"Learning (negative)\", disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = neg_mask\n",
    "                for li, mod in self.double_add_k.items():\n",
    "                    self._handles.append(mod.register_forward_hook(collect_hook(f\"add_k_{li}\", -1)))\n",
    "                for li, mod in self.double_add_q.items():\n",
    "                    self._handles.append(mod.register_forward_hook(collect_hook(f\"add_q_{li}\", -1)))\n",
    "                self._run_pipe_base(neg_prompt, seed)\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        return self._build_vectors_topk(mean_diffs, counts, len(seeds), top_k, verbose,\n",
    "                                         title=f\"Top {top_k} DoubleProj Steering Vectors (add_k + add_q)\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: block (Phase 3 \u2014 SingleStream text-slice fix)\n",
    "    # ==================================================================\n",
    "    def _learn_block(self, pos_prompt, neg_prompt, seeds, top_k, verbose):\n",
    "        \"\"\"\n",
    "        Learn vectors at per-block projections.\n",
    "        Double-stream: attn.to_out[0] \u2014 image-stream tensor, plain mean.\n",
    "        Single-stream: proj_out \u2014 text slice ONLY (mask-aware), avoids modality mixing.\n",
    "        \"\"\"\n",
    "        pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "        neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "        if verbose:\n",
    "            print(f\"T5 mask: pos has {pos_n} tokens, neg has {neg_n} tokens\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def collect_double_hook(layer_idx, sign):\n",
    "            \"\"\"Double-stream to_out[0]: image-stream only; plain mean.\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    mean_act = output.detach().float().mean(dim=(0, 1))\n",
    "                    mean_diffs[f\"double_{layer_idx}\"][step] += (sign * mean_act)\n",
    "                    counts[f\"double_{layer_idx}\"][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def collect_single_hook(layer_idx, sign):\n",
    "            \"\"\"Single-stream proj_out: extract TEXT SLICE only, mask-aware pooling.\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    # output shape: (B, N_img + N_txt, D)\n",
    "                    # Infer N_img: total seq - T5 max_seq_len\n",
    "                    n_txt = self._current_attention_mask.shape[1]  # T5 max_seq_len\n",
    "                    n_img = act.shape[1] - n_txt\n",
    "                    # Extract text slice and apply mask\n",
    "                    txt_slice = act[:, n_img:, :]  # (B, n_txt, D)\n",
    "                    mean_act = self._masked_mean(txt_slice, self._current_attention_mask)\n",
    "                    mean_diffs[f\"single_{layer_idx}\"][step] += (sign * mean_act)\n",
    "                    counts[f\"single_{layer_idx}\"][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        try:\n",
    "            for seed in tqdm(seeds, desc=\"Learning (positive)\", disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = pos_mask\n",
    "                for li, proj in self.double_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(collect_double_hook(li, +1)))\n",
    "                for li, proj in self.single_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(collect_single_hook(li, +1)))\n",
    "                self._run_pipe_base(pos_prompt, seed)\n",
    "\n",
    "            for seed in tqdm(seeds, desc=\"Learning (negative)\", disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = neg_mask\n",
    "                for li, proj in self.double_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(collect_double_hook(li, -1)))\n",
    "                for li, proj in self.single_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(collect_single_hook(li, -1)))\n",
    "                self._run_pipe_base(neg_prompt, seed)\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        return self._build_vectors_topk(mean_diffs, counts, len(seeds), top_k, verbose,\n",
    "                                         title=f\"Top {top_k} Block-Level Steering Vectors\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: all (combined)\n",
    "    # ==================================================================\n",
    "    def _learn_all(self, pos_prompt, neg_prompt, seeds, top_k, verbose):\n",
    "        \"\"\"\n",
    "        Learn from ALL text-dependent surfaces:\n",
    "          - context_embedder + time_text_embed (entry-point, mask-aware)\n",
    "          - add_k_proj + add_q_proj (double_proj, mask-aware)\n",
    "          - to_out[0] in DoubleStream (plain mean \u2014 image stream)\n",
    "        SingleStream is excluded in 'all' mode to avoid complexity;\n",
    "        use 'block' mode directly if you need it.\n",
    "        \"\"\"\n",
    "        pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "        neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "        if verbose:\n",
    "            print(f\"T5 mask: pos {pos_n} tokens, neg {neg_n} tokens\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def hook_masked(layer_id, sign):\n",
    "            \"\"\"For layers with T5 sequence output (context_embedder, add_k/q).\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    if act.dim() == 3:\n",
    "                        mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                    else:\n",
    "                        mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                    mean_diffs[layer_id][step] += (sign * mean_act)\n",
    "                    counts[layer_id][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def hook_plain(layer_id, sign):\n",
    "            \"\"\"For image-stream layers (double to_out).\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    mean_act = output.detach().float().mean(dim=(0, 1))\n",
    "                    mean_diffs[layer_id][step] += (sign * mean_act)\n",
    "                    counts[layer_id][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        try:\n",
    "            for seed in tqdm(seeds, desc=\"Learning (positive)\", disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = pos_mask\n",
    "                # Entry-point layers\n",
    "                for name, mod in self.target_layers.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_masked(name, +1)))\n",
    "                # add_k / add_q\n",
    "                for li, mod in self.double_add_k.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_masked(f\"add_k_{li}\", +1)))\n",
    "                for li, mod in self.double_add_q.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_masked(f\"add_q_{li}\", +1)))\n",
    "                # double to_out\n",
    "                for li, proj in self.double_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(hook_plain(f\"double_{li}\", +1)))\n",
    "                self._run_pipe_base(pos_prompt, seed)\n",
    "\n",
    "            for seed in tqdm(seeds, desc=\"Learning (negative)\", disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = neg_mask\n",
    "                for name, mod in self.target_layers.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_masked(name, -1)))\n",
    "                for li, mod in self.double_add_k.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_masked(f\"add_k_{li}\", -1)))\n",
    "                for li, mod in self.double_add_q.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_masked(f\"add_q_{li}\", -1)))\n",
    "                for li, proj in self.double_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(hook_plain(f\"double_{li}\", -1)))\n",
    "                self._run_pipe_base(neg_prompt, seed)\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        return self._build_vectors_topk(mean_diffs, counts, len(seeds), top_k, verbose,\n",
    "                                         title=f\"Top {top_k} Combined Steering Vectors (all surfaces)\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: hybrid (TRACE entry points + CASteer add_k/add_q)\n",
    "    # ==================================================================\n",
    "    def _learn_hybrid(self, pos_prompt, neg_prompt, seeds, verbose):\n",
    "        \"\"\"\n",
    "        Hybrid approach combining TRACE and CASteer for FLUX:\n",
    "          - TRACE: Hook entry points (context_embedder, time_text_embed)\n",
    "          - CASteer-adapted: Hook text-side K/Q projections (add_k_proj, add_q_proj)\n",
    "            in DoubleStream blocks \u2014 where text queries/keys enter joint attention.\n",
    "\n",
    "        All layers use a single unified hook with mask-aware pooling.\n",
    "        Keeps ALL vectors (no top_k filtering) since entry+add_k+add_q is\n",
    "        a focused set already.\n",
    "        \"\"\"\n",
    "        pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "        neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "        if verbose:\n",
    "            print(f\"T5 mask: pos has {pos_n} real tokens, neg has {neg_n} real tokens (out of {pos_mask.shape[1]})\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def hook_fn(layer_name, sign):\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    # Masked mean if 3D (T5 sequence), else standard mean\n",
    "                    if act.dim() == 3:\n",
    "                        mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                    else:\n",
    "                        mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                    mean_diffs[layer_name][step] += (sign * mean_act)\n",
    "                    counts[layer_name][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def _run_learning_pass(prompt, mask, sign, desc):\n",
    "            self._current_attention_mask = mask\n",
    "            for seed in tqdm(seeds, desc=desc, disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                # TRACE: entry-point hooks\n",
    "                for name, mod in self.target_layers.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn(name, sign)))\n",
    "                # CASteer-adapted: add_k_proj and add_q_proj hooks\n",
    "                for idx, mod in self.double_add_k.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn(f\"add_k_{idx}\", sign)))\n",
    "                for idx, mod in self.double_add_q.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn(f\"add_q_{idx}\", sign)))\n",
    "                self._run_pipe_base(prompt, seed)\n",
    "\n",
    "        try:\n",
    "            _run_learning_pass(pos_prompt, pos_mask, +1, \"Hybrid learning (+)\")\n",
    "            _run_learning_pass(neg_prompt, neg_mask, -1, \"Hybrid learning (-)\")\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        return self._build_vectors_keep_all(mean_diffs, counts, len(seeds), verbose,\n",
    "                                             title=\"Hybrid Steering Vectors (TRACE entry + CASteer add_k/add_q)\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: object (image-side steering for object unlearning)\n",
    "    # ==================================================================\n",
    "    def _learn_object(self, pos_prompt, neg_prompt, seeds, top_k, verbose):\n",
    "        \"\"\"\n",
    "        Learn steering vectors for OBJECT unlearning.\n",
    "\n",
    "        Objects are spatially localized concepts constructed through attention,\n",
    "        not through global modulation. We must steer where the object actually\n",
    "        lives: in the IMAGE representations.\n",
    "\n",
    "        Hooks three types of layers:\n",
    "          1. add_v_proj (DoubleStream): Text Values \u2014 the actual content that\n",
    "             gets transferred to image tokens through joint attention. Steering\n",
    "             V prevents object content from reaching image tokens. (mask-aware)\n",
    "          2. to_out[0] (DoubleStream): Image attention output \u2014 the image\n",
    "             representation AFTER attending to text. Analogous to CASteer's\n",
    "             cross-attention output in SD. (plain mean over spatial dims)\n",
    "          3. proj_out IMAGE SLICE (SingleStream): The image tokens in the\n",
    "             joint text+image sequence. These 38 blocks are where image tokens\n",
    "             continuously absorb object info from text through attention.\n",
    "             Steering image tokens here is the KEY missing piece. (plain mean)\n",
    "        \"\"\"\n",
    "        pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "        neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "        if verbose:\n",
    "            print(f\"T5 mask: pos has {pos_n} real tokens, neg has {neg_n} real tokens (out of {pos_mask.shape[1]})\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def hook_addv(layer_idx, sign):\n",
    "            \"\"\"add_v_proj: text Values (mask-aware pooling).\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    if act.dim() == 3:\n",
    "                        mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                    else:\n",
    "                        mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                    mean_diffs[f\"add_v_{layer_idx}\"][step] += (sign * mean_act)\n",
    "                    counts[f\"add_v_{layer_idx}\"][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def hook_double_out(layer_idx, sign):\n",
    "            \"\"\"to_out[0]: image-side attention output (plain mean).\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    mean_act = output.detach().float().mean(dim=(0, 1))\n",
    "                    mean_diffs[f\"double_{layer_idx}\"][step] += (sign * mean_act)\n",
    "                    counts[f\"double_{layer_idx}\"][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def hook_single_img(layer_idx, sign):\n",
    "            \"\"\"proj_out: extract IMAGE SLICE (not text), plain mean.\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    # output: (B, n_img + n_txt, D)\n",
    "                    n_txt = self._current_attention_mask.shape[1]  # T5 max_seq (512)\n",
    "                    n_img = act.shape[1] - n_txt\n",
    "                    # Extract IMAGE slice (first n_img positions)\n",
    "                    img_slice = act[:, :n_img, :]  # (B, n_img, D)\n",
    "                    mean_act = img_slice.mean(dim=(0, 1))  # (D,)\n",
    "                    mean_diffs[f\"single_img_{layer_idx}\"][step] += (sign * mean_act)\n",
    "                    counts[f\"single_img_{layer_idx}\"][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def _run_learning_pass(prompt, mask, sign, desc):\n",
    "            self._current_attention_mask = mask\n",
    "            for seed in tqdm(seeds, desc=desc, disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                # 1. add_v_proj in DoubleStream (text Values)\n",
    "                for li, mod in self.double_add_v.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_addv(li, sign)))\n",
    "                # 2. to_out[0] in DoubleStream (image attn output)\n",
    "                for li, proj in self.double_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(hook_double_out(li, sign)))\n",
    "                # 3. proj_out in SingleStream (IMAGE slice)\n",
    "                for li, proj in self.single_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(hook_single_img(li, sign)))\n",
    "                self._run_pipe_base(prompt, seed)\n",
    "\n",
    "        try:\n",
    "            _run_learning_pass(pos_prompt, pos_mask, +1, \"Object learning (+)\")\n",
    "            _run_learning_pass(neg_prompt, neg_mask, -1, \"Object learning (-)\")\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        return self._build_vectors_topk(mean_diffs, counts, len(seeds), top_k, verbose,\n",
    "                                         title=f\"Top {top_k} Object Steering Vectors (add_v + to_out + SingleStream image)\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: object_v2 (RECOMMENDED \u2014 entry points + attn output + SingleStream image)\n",
    "    # ==================================================================\n",
    "    def _learn_object_v2(self, pos_prompt, neg_prompt, seeds, top_k, verbose):\n",
    "        \"\"\"\n",
    "        Learn steering vectors for object_v2 mode (single prompt pair, multiple seeds).\n",
    "        Combines entry points (TRACE) + attention output (CASteer) + SingleStream image.\n",
    "        \n",
    "        For best results, use learn_vectors_diverse() instead.\n",
    "        \"\"\"\n",
    "        pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "        neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "        if verbose:\n",
    "            print(f\"T5 mask: pos has {pos_n} real tokens, neg has {neg_n} real tokens (out of {pos_mask.shape[1]})\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def hook_entry(layer_name, sign):\n",
    "            \"\"\"Entry point layers: mask-aware for context_embedder, standard for time_text_embed.\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    if layer_name == \"context_embedder\" and act.dim() == 3:\n",
    "                        mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                    else:\n",
    "                        mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                    mean_diffs[layer_name][step] += (sign * mean_act)\n",
    "                    counts[layer_name][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def hook_double_out(layer_idx, sign):\n",
    "            \"\"\"to_out[0]: image-side attention output (CASteer analog, plain mean).\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    mean_act = output.detach().float().mean(dim=(0, 1))\n",
    "                    mean_diffs[f\"double_{layer_idx}\"][step] += (sign * mean_act)\n",
    "                    counts[f\"double_{layer_idx}\"][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def hook_single_img(layer_idx, sign):\n",
    "            \"\"\"proj_out: extract IMAGE SLICE, plain mean.\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    n_txt = self._current_attention_mask.shape[1]\n",
    "                    n_img = act.shape[1] - n_txt\n",
    "                    img_slice = act[:, :n_img, :]\n",
    "                    mean_act = img_slice.mean(dim=(0, 1))\n",
    "                    mean_diffs[f\"single_img_{layer_idx}\"][step] += (sign * mean_act)\n",
    "                    counts[f\"single_img_{layer_idx}\"][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def _run_learning_pass(prompt, mask, sign, desc):\n",
    "            self._current_attention_mask = mask\n",
    "            for seed in tqdm(seeds, desc=desc, disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                # Entry points (TRACE)\n",
    "                for name, mod in self.target_layers.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_entry(name, sign)))\n",
    "                # to_out[0] in DoubleStream (CASteer analog)\n",
    "                for li, proj in self.double_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(hook_double_out(li, sign)))\n",
    "                # proj_out IMAGE slice in SingleStream\n",
    "                for li, proj in self.single_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(hook_single_img(li, sign)))\n",
    "                self._run_pipe_base(prompt, seed)\n",
    "\n",
    "        try:\n",
    "            _run_learning_pass(pos_prompt, pos_mask, +1, \"Object_v2 learning (+)\")\n",
    "            _run_learning_pass(neg_prompt, neg_mask, -1, \"Object_v2 learning (-)\")\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        # Use ALL vectors for objects (not top_k) \u2014 need full coverage for 4-step FLUX\n",
    "        return self._build_vectors_keep_all(mean_diffs, counts, len(seeds), verbose,\n",
    "                                             title=f\"ALL Object_v2 Vectors (entry + to_out + SingleStream img)\")\n",
    "\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: joint_attn (double-stream to_out ONLY \u2014 colleague's approach)\n",
    "    # ==================================================================\n",
    "    def _learn_joint_attn(self, pos_prompt, neg_prompt, seeds, top_k, verbose):\n",
    "        \"\"\"\n",
    "        Learn steering vectors from joint attention output in double stream blocks ONLY.\n",
    "\n",
    "        This replicates the colleague's approach:\n",
    "          - Hook ONLY attn.to_out[0] in 19 DoubleStream blocks\n",
    "          - Simple mean(dim=(0,1)) over the full output tensor\n",
    "          - No mask-aware pooling, no text/image slice separation\n",
    "          - Top-k selection by activation strength\n",
    "\n",
    "        The to_out[0] projection in DoubleStream blocks outputs the image-stream\n",
    "        result after joint attention with text tokens \u2014 the closest FLUX analog\n",
    "        to SD's cross-attention output that CASteer targets.\n",
    "        \"\"\"\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def collect_hook(layer_idx, sign):\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    mean_act = output.detach().mean(dim=(0, 1))\n",
    "                    mean_diffs[f\"double_{layer_idx}\"][step] += (sign * mean_act)\n",
    "                    counts[f\"double_{layer_idx}\"][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        try:\n",
    "            for seed in tqdm(seeds, desc=\"Learning (positive)\", disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                for li, proj in self.double_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(collect_hook(li, +1)))\n",
    "                self._run_pipe_base(pos_prompt, seed)\n",
    "\n",
    "            for seed in tqdm(seeds, desc=\"Learning (negative)\", disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                for li, proj in self.double_proj_layers.items():\n",
    "                    self._handles.append(proj.register_forward_hook(collect_hook(li, -1)))\n",
    "                self._run_pipe_base(neg_prompt, seed)\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "\n",
    "        return self._build_vectors_topk(mean_diffs, counts, len(seeds), top_k, verbose,\n",
    "                                         title=f\"Top {top_k} Joint-Attention Steering Vectors (double to_out only)\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: learn_vectors_diverse (MULTI-PROMPT \u2014 the KEY method for objects)\n",
    "    # ==================================================================\n",
    "    @torch.no_grad()\n",
    "    def learn_vectors_diverse(self, prompt_pairs, seed=0, top_k=15, verbose=True):\n",
    "        \"\"\"\n",
    "        Learn steering vectors from DIVERSE prompt pairs (CASteer methodology).\n",
    "\n",
    "        This is the CRITICAL method for object unlearning. Instead of using a\n",
    "        single prompt pair with multiple seeds, it uses MANY diverse prompt pairs\n",
    "        (each with the SAME seed) to isolate the target concept direction.\n",
    "\n",
    "        How it works:\n",
    "          For each (pos_prompt, neg_prompt) pair:\n",
    "            1. Run pipeline with pos_prompt (seed=seed) \u2192 collect activations\n",
    "            2. Run pipeline with neg_prompt (seed=seed) \u2192 collect activations\n",
    "            3. Accumulate: diff += activation(pos) - activation(neg)\n",
    "          Final vector = mean(diffs) / ||mean(diffs)||\n",
    "\n",
    "        Why diverse prompts matter (from CASteer paper):\n",
    "          - With 1 pair + many seeds: vector captures \"Dog in THIS context\"\n",
    "          - With 50 pairs + 1 seed: vector captures \"Dog in GENERAL\"\n",
    "          - The diverse contexts (tench, goldfish, shark...) cancel out,\n",
    "            leaving only the concept-specific direction.\n",
    "\n",
    "        Usage:\n",
    "          pairs = make_object_prompts(\"Dog\", num_prompts=50)\n",
    "          vectors = steerer.learn_vectors_diverse(pairs, seed=0, top_k=15)\n",
    "\n",
    "        Args:\n",
    "            prompt_pairs: List of (pos_prompt, neg_prompt) tuples\n",
    "            seed: Single seed to use for ALL pairs (same seed per pair, as in CASteer)\n",
    "            top_k: Number of strongest vectors to keep\n",
    "            verbose: Print progress\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of steering vectors {layer_id: {step: direction_tensor}}\n",
    "        \"\"\"\n",
    "        if self.mode not in (\"object_v2\", \"hybrid\", \"entry_point\", \"block\", \"double_proj\", \"all\", \"object\", \"joint_attn\"):\n",
    "            raise ValueError(f\"learn_vectors_diverse not supported for mode '{self.mode}'\")\n",
    "\n",
    "        n_pairs = len(prompt_pairs)\n",
    "        if verbose:\n",
    "            print(f\"Learning from {n_pairs} diverse prompt pairs (seed={seed})\")\n",
    "            print(f\"Mode: {self.mode}\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        # Choose hooks based on mode\n",
    "        def _get_hooks_for_mode(sign):\n",
    "            \"\"\"Return list of (module, hook_fn) based on current mode.\"\"\"\n",
    "            hooks = []\n",
    "\n",
    "            if self.mode in (\"object_v2\", \"entry_point\", \"all\", \"hybrid\"):\n",
    "                # Entry points\n",
    "                for name, mod in self.target_layers.items():\n",
    "                    def make_entry_hook(layer_name, s):\n",
    "                        def hook(module, inputs, output):\n",
    "                            step = self._current_step + 1\n",
    "                            if 0 <= step < self.n_steps:\n",
    "                                act = output.detach().float()\n",
    "                                if layer_name == \"context_embedder\" and act.dim() == 3:\n",
    "                                    mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                                else:\n",
    "                                    mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                                mean_diffs[layer_name][step] += (s * mean_act)\n",
    "                                counts[layer_name][step] += 1\n",
    "                            return output\n",
    "                        return hook\n",
    "                    hooks.append((mod, make_entry_hook(name, sign)))\n",
    "\n",
    "            if self.mode in (\"object_v2\", \"object\", \"block\", \"all\", \"joint_attn\"):\n",
    "                # to_out[0] in DoubleStream (CASteer CA output analog)\n",
    "                for li, proj in self.double_proj_layers.items():\n",
    "                    def make_double_hook(layer_idx, s):\n",
    "                        def hook(module, inputs, output):\n",
    "                            step = self._current_step + 1\n",
    "                            if 0 <= step < self.n_steps:\n",
    "                                mean_act = output.detach().float().mean(dim=(0, 1))\n",
    "                                mean_diffs[f\"double_{layer_idx}\"][step] += (s * mean_act)\n",
    "                                counts[f\"double_{layer_idx}\"][step] += 1\n",
    "                            return output\n",
    "                        return hook\n",
    "                    hooks.append((proj, make_double_hook(li, sign)))\n",
    "\n",
    "            if self.mode in (\"object_v2\", \"object\"):\n",
    "                # proj_out IMAGE SLICE in SingleStream\n",
    "                for li, proj in self.single_proj_layers.items():\n",
    "                    def make_single_img_hook(layer_idx, s):\n",
    "                        def hook(module, inputs, output):\n",
    "                            step = self._current_step + 1\n",
    "                            if 0 <= step < self.n_steps:\n",
    "                                act = output.detach().float()\n",
    "                                n_txt = self._current_attention_mask.shape[1]\n",
    "                                n_img = act.shape[1] - n_txt\n",
    "                                img_slice = act[:, :n_img, :]\n",
    "                                mean_act = img_slice.mean(dim=(0, 1))\n",
    "                                mean_diffs[f\"single_img_{layer_idx}\"][step] += (s * mean_act)\n",
    "                                counts[f\"single_img_{layer_idx}\"][step] += 1\n",
    "                            return output\n",
    "                        return hook\n",
    "                    hooks.append((proj, make_single_img_hook(li, sign)))\n",
    "\n",
    "            if self.mode == \"object\":\n",
    "                # add_v_proj (text Values) \u2014 original object mode only\n",
    "                for li, mod in self.double_add_v.items():\n",
    "                    def make_addv_hook(layer_idx, s):\n",
    "                        def hook(module, inputs, output):\n",
    "                            step = self._current_step + 1\n",
    "                            if 0 <= step < self.n_steps:\n",
    "                                act = output.detach().float()\n",
    "                                if act.dim() == 3:\n",
    "                                    mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                                else:\n",
    "                                    mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                                mean_diffs[f\"add_v_{layer_idx}\"][step] += (s * mean_act)\n",
    "                                counts[f\"add_v_{layer_idx}\"][step] += 1\n",
    "                            return output\n",
    "                        return hook\n",
    "                    hooks.append((mod, make_addv_hook(li, sign)))\n",
    "\n",
    "            if self.mode in (\"hybrid\", \"double_proj\", \"all\"):\n",
    "                # add_k_proj and add_q_proj\n",
    "                for li, mod in self.double_add_k.items():\n",
    "                    def make_addk_hook(layer_idx, s):\n",
    "                        def hook(module, inputs, output):\n",
    "                            step = self._current_step + 1\n",
    "                            if 0 <= step < self.n_steps:\n",
    "                                act = output.detach().float()\n",
    "                                if act.dim() == 3:\n",
    "                                    mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                                else:\n",
    "                                    mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                                mean_diffs[f\"add_k_{layer_idx}\"][step] += (s * mean_act)\n",
    "                                counts[f\"add_k_{layer_idx}\"][step] += 1\n",
    "                            return output\n",
    "                        return hook\n",
    "                    hooks.append((mod, make_addk_hook(li, sign)))\n",
    "                for li, mod in self.double_add_q.items():\n",
    "                    def make_addq_hook(layer_idx, s):\n",
    "                        def hook(module, inputs, output):\n",
    "                            step = self._current_step + 1\n",
    "                            if 0 <= step < self.n_steps:\n",
    "                                act = output.detach().float()\n",
    "                                if act.dim() == 3:\n",
    "                                    mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                                else:\n",
    "                                    mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                                mean_diffs[f\"add_q_{layer_idx}\"][step] += (s * mean_act)\n",
    "                                counts[f\"add_q_{layer_idx}\"][step] += 1\n",
    "                            return output\n",
    "                        return hook\n",
    "                    hooks.append((mod, make_addq_hook(li, sign)))\n",
    "\n",
    "            if self.mode == \"block\":\n",
    "                # SingleStream text slice for block mode\n",
    "                for li, proj in self.single_proj_layers.items():\n",
    "                    def make_single_txt_hook(layer_idx, s):\n",
    "                        def hook(module, inputs, output):\n",
    "                            step = self._current_step + 1\n",
    "                            if 0 <= step < self.n_steps:\n",
    "                                act = output.detach().float()\n",
    "                                n_txt = self._current_attention_mask.shape[1]\n",
    "                                n_img = act.shape[1] - n_txt\n",
    "                                txt_slice = act[:, n_img:, :]\n",
    "                                mean_act = self._masked_mean(txt_slice, self._current_attention_mask)\n",
    "                                mean_diffs[f\"single_{layer_idx}\"][step] += (s * mean_act)\n",
    "                                counts[f\"single_{layer_idx}\"][step] += 1\n",
    "                            return output\n",
    "                        return hook\n",
    "                    hooks.append((proj, make_single_txt_hook(li, sign)))\n",
    "\n",
    "            return hooks\n",
    "\n",
    "        try:\n",
    "            for pair_idx, (pos_prompt, neg_prompt) in enumerate(\n",
    "                tqdm(prompt_pairs, desc=\"Diverse prompt pairs\", disable=not verbose)\n",
    "            ):\n",
    "                # Get masks for THIS specific prompt pair\n",
    "                pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "                neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "\n",
    "                # Positive pass\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = pos_mask\n",
    "                for mod, hook_fn in _get_hooks_for_mode(+1):\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn))\n",
    "                self._run_pipe_base(pos_prompt, seed)\n",
    "\n",
    "                # Negative pass\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = neg_mask\n",
    "                for mod, hook_fn in _get_hooks_for_mode(-1):\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn))\n",
    "                self._run_pipe_base(neg_prompt, seed)\n",
    "\n",
    "                if verbose and (pair_idx + 1) % 10 == 0:\n",
    "                    print(f\"  Completed {pair_idx + 1}/{n_pairs} prompt pairs\")\n",
    "\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        # Build vectors \u2014 divide by number of pairs (not seeds)\n",
    "        # For object modes: use ALL vectors (not top_k). With 4-step FLUX,\n",
    "        # top_k=15 covers only 6% of layers \u2014 far too few. CASteer steers ALL layers.\n",
    "        if self.mode in (\"object_v2\", \"object\"):\n",
    "            if verbose:\n",
    "                print(f\"\\nUsing ALL vectors (object mode \u2014 need maximum coverage for 4-step FLUX)\")\n",
    "            return self._build_vectors_keep_all(mean_diffs, counts, n_pairs, verbose,\n",
    "                                                 title=f\"ALL Diverse-Prompt Steering Vectors ({self.mode}, {n_pairs} pairs)\")\n",
    "        else:\n",
    "            return self._build_vectors_topk(mean_diffs, counts, n_pairs, top_k, verbose,\n",
    "                                             title=f\"Top {top_k} Diverse-Prompt Steering Vectors ({self.mode}, {n_pairs} pairs)\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # Vector builders (shared)\n",
    "    # ==================================================================\n",
    "    def _build_vectors_keep_all(self, mean_diffs, counts, n_seeds, verbose, title=\"Steering Vectors\"):\n",
    "        \"\"\"Build normalized direction vectors, keeping ALL candidates.\"\"\"\n",
    "        vectors = defaultdict(dict)\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(title)\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"{'Layer':<25} {'Step':<6} {'Strength':<12}\")\n",
    "            print(f\"{'-'*70}\")\n",
    "\n",
    "        for name in sorted(mean_diffs.keys()):\n",
    "            for step in sorted(mean_diffs[name].keys()):\n",
    "                if counts[name][step] == 0:\n",
    "                    continue\n",
    "                avg_diff = mean_diffs[name][step] / n_seeds\n",
    "                strength = float(avg_diff.norm())\n",
    "                direction = avg_diff / (avg_diff.norm() + 1e-8)\n",
    "                vectors[name][step] = direction\n",
    "                if verbose:\n",
    "                    print(f\"{name:<25} {step:<6} {strength:<12.4f}\")\n",
    "\n",
    "        if verbose:\n",
    "            total = sum(len(v) for v in vectors.values())\n",
    "            print(f\"{'-'*70}\")\n",
    "            print(f\"Total vectors: {total}\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "        return dict(vectors)\n",
    "\n",
    "    def _build_vectors_topk(self, mean_diffs, counts, n_seeds, top_k, verbose, title=\"Steering Vectors\"):\n",
    "        \"\"\"Build normalized direction vectors, keeping top_k by strength.\"\"\"\n",
    "        candidates = []\n",
    "        for li in mean_diffs:\n",
    "            for step in mean_diffs[li]:\n",
    "                if counts[li][step] == 0:\n",
    "                    continue\n",
    "                avg_diff = mean_diffs[li][step] / n_seeds\n",
    "                candidates.append((float(avg_diff.norm()), li, step, avg_diff))\n",
    "\n",
    "        candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "        top_candidates = candidates[:top_k]\n",
    "\n",
    "        vectors = defaultdict(dict)\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(title)\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"{'Rank':<6} {'Layer':<25} {'Step':<6} {'Strength':<12}\")\n",
    "            print(f\"{'-'*70}\")\n",
    "\n",
    "        for rank, (strength, li, step, diff) in enumerate(top_candidates, start=1):\n",
    "            direction = diff / (diff.norm() + 1e-8)\n",
    "            vectors[li][step] = direction\n",
    "            if verbose:\n",
    "                print(f\"{rank:<6} {str(li):<25} {step:<6} {strength:<12.4f}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{'='*70}\\n\")\n",
    "        return dict(vectors)\n",
    "\n",
    "    # ==================================================================\n",
    "    # APPLY VECTORS\n",
    "    # ==================================================================\n",
    "    @contextmanager\n",
    "    def apply_vectors(self, vectors, beta=2.0, clip_negative=True):\n",
    "        \"\"\"\n",
    "        Context manager to apply steering vectors during generation.\n",
    "\n",
    "        output' = output - beta * clamp(output . d, min=0) * d\n",
    "\n",
    "        Handles all modes:\n",
    "        - Standard hook for entry points, add_k, add_q, add_v, double to_out\n",
    "        - Text-slice hook for SingleStream text steering (block mode)\n",
    "        - IMAGE-slice hook for SingleStream image steering (object mode)\n",
    "        - Renormalization (CASteer-style) for object mode hooks\n",
    "        \"\"\"\n",
    "        def steer_hook(layer_vectors):\n",
    "            \"\"\"Standard projection removal for (B, D) or (B, S, D) tensors.\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if step in layer_vectors:\n",
    "                    target_dir = layer_vectors[step].to(output.device, output.dtype)\n",
    "                    score = (output @ target_dir)\n",
    "                    if clip_negative:\n",
    "                        score = torch.clamp(score, min=0.0)\n",
    "                    update = (beta * score).unsqueeze(-1) * target_dir\n",
    "                    return output - update\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def steer_hook_renorm(layer_vectors):\n",
    "            \"\"\"Projection removal WITH renormalization (CASteer-style).\n",
    "            Preserves activation norm after steering \u2014 critical for object mode\n",
    "            to avoid norm shrinkage across many layers.\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if step in layer_vectors:\n",
    "                    target_dir = layer_vectors[step].to(output.device, output.dtype)\n",
    "                    # Save original norm\n",
    "                    orig_norm = output.norm(dim=-1, keepdim=True)\n",
    "                    # Compute projection removal\n",
    "                    score = (output @ target_dir)\n",
    "                    if clip_negative:\n",
    "                        score = torch.clamp(score, min=0.0)\n",
    "                    update = (beta * score).unsqueeze(-1) * target_dir\n",
    "                    steered = output - update\n",
    "                    # Renormalize to preserve original activation magnitude\n",
    "                    steered_norm = steered.norm(dim=-1, keepdim=True).clamp(min=1e-8)\n",
    "                    steered = steered / steered_norm * orig_norm\n",
    "                    return steered\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def steer_hook_single(layer_vectors, txt_seq_len):\n",
    "            \"\"\"Projection removal on the TEXT SLICE ONLY of SingleStream output.\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if step in layer_vectors:\n",
    "                    target_dir = layer_vectors[step].to(output.device, output.dtype)\n",
    "                    # output: (B, N_img + N_txt, D). Only steer text portion.\n",
    "                    n_img = output.shape[1] - txt_seq_len\n",
    "                    txt_slice = output[:, n_img:, :]  # (B, N_txt, D)\n",
    "                    score = (txt_slice @ target_dir)  # (B, N_txt)\n",
    "                    if clip_negative:\n",
    "                        score = torch.clamp(score, min=0.0)\n",
    "                    update = (beta * score).unsqueeze(-1) * target_dir\n",
    "                    out = output.clone()\n",
    "                    out[:, n_img:, :] = txt_slice - update\n",
    "                    return out\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def steer_hook_single_image(layer_vectors, txt_seq_len):\n",
    "            \"\"\"Projection removal on the IMAGE SLICE of SingleStream output.\n",
    "            This is the KEY hook for object unlearning \u2014 steers the image tokens\n",
    "            that have absorbed object info from text through joint attention.\n",
    "            Includes renormalization to preserve activation magnitude.\"\"\"\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if step in layer_vectors:\n",
    "                    target_dir = layer_vectors[step].to(output.device, output.dtype)\n",
    "                    # output: (B, N_img + N_txt, D). Steer IMAGE portion.\n",
    "                    n_img = output.shape[1] - txt_seq_len\n",
    "                    img_slice = output[:, :n_img, :]  # (B, N_img, D)\n",
    "                    # Save original norm\n",
    "                    orig_norm = img_slice.norm(dim=-1, keepdim=True)\n",
    "                    # Compute projection removal\n",
    "                    score = (img_slice @ target_dir)  # (B, N_img)\n",
    "                    if clip_negative:\n",
    "                        score = torch.clamp(score, min=0.0)\n",
    "                    update = (beta * score).unsqueeze(-1) * target_dir\n",
    "                    steered_img = img_slice - update\n",
    "                    # Renormalize\n",
    "                    steered_norm = steered_img.norm(dim=-1, keepdim=True).clamp(min=1e-8)\n",
    "                    steered_img = steered_img / steered_norm * orig_norm\n",
    "                    # Replace image slice only; leave text tokens untouched\n",
    "                    out = output.clone()\n",
    "                    out[:, :n_img, :] = steered_img\n",
    "                    return out\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        try:\n",
    "            self._clear_hooks()\n",
    "\n",
    "            # Infer T5 sequence length for SingleStream slicing\n",
    "            max_seq = getattr(self.pipe, '_max_sequence_length', 512)\n",
    "            if hasattr(self.pipe, 'tokenizer_2'):\n",
    "                max_seq = 512  # Flux default\n",
    "\n",
    "            # ----- Register hooks based on layer name prefix -----\n",
    "            for li, step_vecs in vectors.items():\n",
    "                li_str = str(li)\n",
    "\n",
    "                # Entry-point layers\n",
    "                if li_str in (\"context_embedder\", \"time_text_embed\"):\n",
    "                    if li_str in self.target_layers:\n",
    "                        self._handles.append(\n",
    "                            self.target_layers[li_str].register_forward_hook(steer_hook(step_vecs))\n",
    "                        )\n",
    "\n",
    "                # add_k_proj\n",
    "                elif li_str.startswith(\"add_k_\"):\n",
    "                    idx = int(li_str.split(\"_\")[-1])\n",
    "                    if idx in self.double_add_k:\n",
    "                        self._handles.append(\n",
    "                            self.double_add_k[idx].register_forward_hook(steer_hook(step_vecs))\n",
    "                        )\n",
    "\n",
    "                # add_q_proj\n",
    "                elif li_str.startswith(\"add_q_\"):\n",
    "                    idx = int(li_str.split(\"_\")[-1])\n",
    "                    if idx in self.double_add_q:\n",
    "                        self._handles.append(\n",
    "                            self.double_add_q[idx].register_forward_hook(steer_hook(step_vecs))\n",
    "                        )\n",
    "\n",
    "                # add_v_proj (object mode \u2014 text Values with renorm)\n",
    "                elif li_str.startswith(\"add_v_\"):\n",
    "                    idx = int(li_str.split(\"_\")[-1])\n",
    "                    if idx in self.double_add_v:\n",
    "                        self._handles.append(\n",
    "                            self.double_add_v[idx].register_forward_hook(steer_hook_renorm(step_vecs))\n",
    "                        )\n",
    "\n",
    "                # Double-stream to_out\n",
    "                elif li_str.startswith(\"double_\"):\n",
    "                    idx = int(li_str.split(\"_\")[1])\n",
    "                    if idx in self.double_proj_layers:\n",
    "                        # Renorm only for object/object_v2; simple projection for others\n",
    "                        if self.mode in (\"object\", \"object_v2\"):\n",
    "                            hook_fn = steer_hook_renorm(step_vecs)\n",
    "                        else:\n",
    "                            hook_fn = steer_hook(step_vecs)\n",
    "                        self._handles.append(\n",
    "                            self.double_proj_layers[idx].register_forward_hook(hook_fn)\n",
    "                        )\n",
    "\n",
    "                # Single-stream proj_out \u2014 IMAGE SLICE (object mode)\n",
    "                elif li_str.startswith(\"single_img_\"):\n",
    "                    idx = int(li_str.split(\"_\")[-1])\n",
    "                    if idx in self.single_proj_layers:\n",
    "                        self._handles.append(\n",
    "                            self.single_proj_layers[idx].register_forward_hook(\n",
    "                                steer_hook_single_image(step_vecs, max_seq)\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                # Single-stream proj_out \u2014 TEXT SLICE (block mode)\n",
    "                elif li_str.startswith(\"single_\"):\n",
    "                    idx = int(li_str.split(\"_\")[1])\n",
    "                    if idx in self.single_proj_layers:\n",
    "                        self._handles.append(\n",
    "                            self.single_proj_layers[idx].register_forward_hook(\n",
    "                                steer_hook_single(step_vecs, max_seq)\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            yield\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "\n",
    "    # ==================================================================\n",
    "    # GENERATE\n",
    "    # ==================================================================\n",
    "    def generate(self, prompt, seed, vectors=None, beta=2.0, clip_negative=True):\n",
    "        \"\"\"Generate image with optional steering.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Text prompt\n",
    "            seed: Random seed\n",
    "            vectors: Steering vectors dict (from learn_vectors or learn_vectors_diverse)\n",
    "            beta: Steering strength. For 4-step FLUX Schnell:\n",
    "                  - Style: beta=2-5 works well\n",
    "                  - Objects: beta=10-20 needed (CASteer uses beta=10 with 50 steps;\n",
    "                    with 4 steps we need proportionally more per step)\n",
    "            clip_negative: If True, only remove positive projections onto the\n",
    "                  concept direction (standard CASteer). If False, also steer\n",
    "                  negative projections. Try False if objects produce identical\n",
    "                  images \u2014 the direction might be flipped for the eval prompts.\n",
    "        \"\"\"\n",
    "        if vectors:\n",
    "            with self.apply_vectors(vectors, beta=beta, clip_negative=clip_negative):\n",
    "                return self._run_pipe_base(prompt, seed)\n",
    "        else:\n",
    "            return self._run_pipe_base(prompt, seed)\n",
    "\n",
    "    # ==================================================================\n",
    "    # SAVE / LOAD\n",
    "    # ==================================================================\n",
    "    def save_vectors(self, vectors, filepath):\n",
    "        \"\"\"Save steering vectors to .pt file.\"\"\"\n",
    "        save_dict = {}\n",
    "        for layer_id, step_dict in vectors.items():\n",
    "            save_dict[layer_id] = {step: t.cpu() for step, t in step_dict.items()}\n",
    "        torch.save(save_dict, filepath)\n",
    "        print(f\"\u2713 Saved steering vectors to: {filepath}\")\n",
    "\n",
    "    def load_vectors(self, filepath):\n",
    "        \"\"\"Load steering vectors from .pt file.\"\"\"\n",
    "        save_dict = torch.load(filepath, map_location=self.device)\n",
    "        vectors = {}\n",
    "        for layer_id, step_dict in save_dict.items():\n",
    "            vectors[layer_id] = {step: t.to(self.device) for step, t in step_dict.items()}\n",
    "        print(f\"\u2713 Loaded steering vectors from: {filepath}\")\n",
    "        return vectors\n",
    "\n",
    "print(\"\u2713 FluxSteering class defined!\")\n",
    "print(\"  Style unlearning:  mode='hybrid' (entry points + add_k/add_q)\")\n",
    "print(\"  Object unlearning: mode='joint_attn' (double to_out only, simple mean, no renorm)\")\n",
    "print(\"                     Use learn_vectors() with simple prompts (e.g., 'Dog' / 'Object')\")\n",
    "print(\"  Alt object:        mode='object_v2' (entry points + to_out + SingleStream img)\")\n",
    "print(\"                     Use learn_vectors_diverse() with make_object_prompts() for 50+ diverse pairs\")\n",
    "print(\"  Other modes: entry_point, block, double_proj, all, object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3b: VERIFY TEXT-EMBEDDING ENTRY POINTS (run after models are loaded)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Empirical verification of where text embeddings first enter the FLUX transformer.\n",
    "\n",
    "Method:\n",
    "  1. Hook EVERY named module in the transformer.\n",
    "  2. Run ONE denoising step with prompt A (same seed, same timestep schedule).\n",
    "  3. Run ONE denoising step with prompt B (same seed, same timestep schedule).\n",
    "  4. Compare outputs: modules whose output CHANGED are text-dependent.\n",
    "  5. The first such modules in forward-pass order are the entry points.\n",
    "\n",
    "This proves, via code, that context_embedder and time_text_embed are the\n",
    "only places where raw text embeddings are directly consumed.\n",
    "\"\"\"\n",
    "\n",
    "def verify_text_entry_points(pipe, device=\"cuda\", prompt_a=\"a dog in Van Gogh style\",\n",
    "                              prompt_b=\"a dog in Cartoon style\"):\n",
    "    \"\"\"\n",
    "    Empirically identify which transformer modules are text-dependent.\n",
    "    Returns a list of (module_name, output_changed: bool) in forward-pass order.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    transformer = pipe.transformer\n",
    "\n",
    "    # Storage for outputs from two runs\n",
    "    outputs_a = OrderedDict()\n",
    "    outputs_b = OrderedDict()\n",
    "\n",
    "    def make_hook(storage, name):\n",
    "        def hook(module, inputs, output):\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                storage[name] = output.detach().cpu().float()\n",
    "            elif isinstance(output, tuple) and len(output) > 0 and isinstance(output[0], torch.Tensor):\n",
    "                storage[name] = output[0].detach().cpu().float()\n",
    "        return hook\n",
    "\n",
    "    def run_one_step(prompt, storage):\n",
    "        \"\"\"Run exactly 1 denoising step with hooks on all modules.\"\"\"\n",
    "        handles = []\n",
    "        try:\n",
    "            for name, mod in transformer.named_modules():\n",
    "                if name == \"\":  # skip root\n",
    "                    continue\n",
    "                handles.append(mod.register_forward_hook(make_hook(storage, name)))\n",
    "\n",
    "            g = torch.Generator(device=device).manual_seed(42)\n",
    "            pipe(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=1,\n",
    "                generator=g,\n",
    "                output_type=\"latent\",\n",
    "            )\n",
    "        finally:\n",
    "            for h in handles:\n",
    "                h.remove()\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"VERIFYING TEXT-EMBEDDING ENTRY POINTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Prompt A: \\\"{prompt_a}\\\"\")\n",
    "    print(f\"  Prompt B: \\\"{prompt_b}\\\"\")\n",
    "    print(f\"  Same seed (42), same scheduler, 1 step each\")\n",
    "    print()\n",
    "\n",
    "    # Run both prompts\n",
    "    print(\"Running prompt A...\")\n",
    "    run_one_step(prompt_a, outputs_a)\n",
    "    print(\"Running prompt B...\")\n",
    "    run_one_step(prompt_b, outputs_b)\n",
    "\n",
    "    # Compare outputs\n",
    "    common = [n for n in outputs_a if n in outputs_b]\n",
    "    text_dependent = []\n",
    "    text_independent = []\n",
    "\n",
    "    for name in common:\n",
    "        a, b = outputs_a[name], outputs_b[name]\n",
    "        if a.shape == b.shape:\n",
    "            diff = (a - b).abs().max().item()\n",
    "            changed = diff > 1e-6\n",
    "        else:\n",
    "            changed = True\n",
    "            diff = float(\"inf\")\n",
    "\n",
    "        if changed:\n",
    "            text_dependent.append((name, diff))\n",
    "        else:\n",
    "            text_independent.append(name)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS: {len(text_dependent)} text-dependent modules (out of {len(common)} total)\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\n--- TEXT-INDEPENDENT modules (output identical for both prompts): ---\")\n",
    "    if text_independent:\n",
    "        for n in text_independent[:10]:\n",
    "            print(f\"  {n}\")\n",
    "        if len(text_independent) > 10:\n",
    "            print(f\"  ... and {len(text_independent) - 10} more\")\n",
    "    else:\n",
    "        print(\"  (none \u2014 all modules are text-dependent)\")\n",
    "\n",
    "    print(f\"\\n--- TEXT-DEPENDENT modules (output changed between prompts): ---\")\n",
    "    # Sort by max diff to highlight strongest\n",
    "    text_dependent.sort(key=lambda x: -x[1])\n",
    "    for name, diff in text_dependent:\n",
    "        marker = \"\"\n",
    "        if name in (\"context_embedder\", \"time_text_embed\"):\n",
    "            marker = \"  \u25c0 TEXT ENTRY POINT\"\n",
    "        elif name.startswith(\"context_embedder.\") or name.startswith(\"time_text_embed.\"):\n",
    "            marker = \"  (sub-module of entry point)\"\n",
    "        print(f\"  {name:<60} max_diff={diff:.6f}{marker}\")\n",
    "\n",
    "    # Identify true entry points: text-dependent modules that are NOT children\n",
    "    # of other text-dependent modules (i.e., the roots of text-dependent subtrees)\n",
    "    dep_names = set(n for n, _ in text_dependent)\n",
    "    entry_points = []\n",
    "    for name, diff in text_dependent:\n",
    "        # Check if any proper parent is also text-dependent\n",
    "        parts = name.split(\".\")\n",
    "        is_child = False\n",
    "        for i in range(1, len(parts)):\n",
    "            parent = \".\".join(parts[:i])\n",
    "            if parent in dep_names:\n",
    "                is_child = True\n",
    "                break\n",
    "        if not is_child:\n",
    "            entry_points.append(name)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ENTRY POINTS (root text-dependent modules):\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for ep in entry_points:\n",
    "        mod = dict(transformer.named_modules())[ep]\n",
    "        print(f\"  transformer.{ep}\")\n",
    "        print(f\"    type: {mod.__class__.__name__}\")\n",
    "        # Print shape info if it's a Linear layer\n",
    "        if hasattr(mod, 'in_features'):\n",
    "            print(f\"    shape: Linear({mod.in_features} \u2192 {mod.out_features})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    return entry_points, text_dependent, text_independent\n",
    "\n",
    "# NOTE: Run this AFTER Cell 6 (model loading). Uncomment and execute:\n",
    "# entry_points, dep, indep = verify_text_entry_points(pipe, device=DEVICE)\n",
    "\n",
    "print(\"\u2713 verify_text_entry_points() defined. Run it after loading models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: QUALITY METRICS (FID, CLIP Score)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Quality metrics following UnlearnCanvas evaluation protocol.\n",
    "\"\"\"\n",
    "\n",
    "class QualityMetrics:\n",
    "    \"\"\"Calculate image quality metrics for UnlearnCanvas evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        print(\"Loading quality metric models...\")\n",
    "        \n",
    "        # Load CLIP for text-image alignment\n",
    "        try:\n",
    "            self.clip_model, self.clip_preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "            self.clip_model.eval()\n",
    "            print(\"  \u2713 CLIP ViT-L/14 loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \u2717 CLIP loading failed: {e}\")\n",
    "            self.clip_model = None\n",
    "\n",
    "    def calculate_clip_score(self, images, prompts):\n",
    "        \"\"\"\n",
    "        Calculate CLIP score between images and text prompts.\n",
    "        Higher = better text-image alignment.\n",
    "        \"\"\"\n",
    "        if self.clip_model is None:\n",
    "            return None\n",
    "\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for img, prompt in zip(images, prompts):\n",
    "                image_input = self.clip_preprocess(img).unsqueeze(0).to(self.device)\n",
    "                text_input = clip.tokenize([prompt], truncate=True).to(self.device)\n",
    "                \n",
    "                image_features = self.clip_model.encode_image(image_input)\n",
    "                text_features = self.clip_model.encode_text(text_input)\n",
    "                \n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                similarity = (image_features @ text_features.T).item()\n",
    "                scores.append(similarity)\n",
    "\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def calculate_fid(self, real_path, generated_path):\n",
    "        \"\"\"\n",
    "        Calculate FID between two image directories.\n",
    "        Lower = better (generated images closer to real distribution).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            score = fid.compute_fid(\n",
    "                real_path,\n",
    "                generated_path,\n",
    "                mode=\"clean\",\n",
    "                num_workers=0,\n",
    "                batch_size=8,\n",
    "                device=torch.device(self.device)\n",
    "            )\n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0 FID calculation error: {e}\")\n",
    "            return None\n",
    "\n",
    "print(\"\u2713 QualityMetrics class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4B: LLAVA CLASSIFIER (Alternative to CLIP - More Accurate)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LLaVA-based classification for more accurate style/object recognition.\n",
    "\n",
    "Pros:\n",
    "- More nuanced understanding of artistic styles\n",
    "- Can handle ambiguous cases better\n",
    "- Similar to human judgment\n",
    "\n",
    "Cons:\n",
    "- Slower (~2-5s per image vs 0.1s for CLIP)\n",
    "- Requires more VRAM (~14GB additional)\n",
    "\n",
    "Usage:\n",
    "  Set USE_LLAVA = True in the configuration cell to use LLaVA instead of CLIP.\n",
    "\"\"\"\n",
    "\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "class LLaVAClassifier:\n",
    "    \"\"\"\n",
    "    LLaVA-based image classifier for UnlearnCanvas evaluation.\n",
    "    \n",
    "    This implementation follows the EXACT methodology from the TRACE paper\n",
    "    (Appendix E.4, Figures 6-7), which uses numbered options and expects\n",
    "    the model to respond with ONLY a number for reliable parsing.\n",
    "    \n",
    "    Reference: TRACE: Transcoder-based Concept Editing (ICLR 2026)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_id=\"llava-hf/llava-v1.6-vicuna-7b-hf\", device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self.model_id = model_id\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"Load LLaVA model (call separately to manage VRAM).\"\"\"\n",
    "        if self.model is not None:\n",
    "            return\n",
    "            \n",
    "        print(f\"Loading LLaVA: {self.model_id}...\")\n",
    "        self.processor = LlavaNextProcessor.from_pretrained(self.model_id)\n",
    "        self.model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            self.model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"\u2713 LLaVA loaded!\")\n",
    "    \n",
    "    def unload(self):\n",
    "        \"\"\"Unload model to free VRAM.\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            del self.processor\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"\u2713 LLaVA unloaded\")\n",
    "    \n",
    "    def _generate_response(self, image, prompt):\n",
    "        \"\"\"Generate response from LLaVA given image and prompt.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.load()\n",
    "        \n",
    "        # Load image if path provided, ensure RGB\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "        elif isinstance(image, Image.Image):\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        # Format for LLaVA\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        prompt_formatted = self.processor.apply_chat_template(\n",
    "            conversation, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = self.processor(images=image, text=prompt_formatted, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,  # Only need a number\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated part\n",
    "        response = self.processor.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "    \n",
    "    def _parse_number_response(self, response, max_options):\n",
    "        \"\"\"\n",
    "        Parse a number from LLaVA response.\n",
    "        Returns 0-indexed class index or None if invalid.\n",
    "        \"\"\"\n",
    "        # Try to extract a number from the response\n",
    "        import re\n",
    "        numbers = re.findall(r'\\d+', response)\n",
    "        if numbers:\n",
    "            num = int(numbers[0])\n",
    "            if 1 <= num <= max_options:\n",
    "                return num - 1  # Convert to 0-indexed\n",
    "        return None\n",
    "    \n",
    "    def classify_style(self, image, styles=None, debug=False):\n",
    "        \"\"\"\n",
    "        Classify image style using TRACE paper's exact prompt format.\n",
    "        \n",
    "        From TRACE paper Figure 6:\n",
    "        'You are an image classifier. Classify the artistic style of the given image.\n",
    "         Instruction: Choose exactly one option from the numbered list below. \n",
    "         Respond with only the number.'\n",
    "        \"\"\"\n",
    "        styles = styles or STYLES\n",
    "        \n",
    "        # Build numbered options list (TRACE paper format)\n",
    "        options_list = '\\n'.join([f\"{i+1}. {s.replace('_', ' ')}\" for i, s in enumerate(styles)])\n",
    "        \n",
    "        prompt = (\n",
    "            \"You are an image classifier. Classify the artistic style of the given image.\\n\"\n",
    "            \"Instruction: Choose exactly one option from the numbered list below. \"\n",
    "            \"Respond with only the number.\\n\"\n",
    "            f\"Options:\\n{options_list}\"\n",
    "        )\n",
    "        \n",
    "        response = self._generate_response(image, prompt)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"LLaVA style response: '{response}'\")\n",
    "        \n",
    "        idx = self._parse_number_response(response, len(styles))\n",
    "        if idx is not None:\n",
    "            return styles[idx]\n",
    "        \n",
    "        # Fallback: try to match style name in response\n",
    "        response_lower = response.lower()\n",
    "        for style in styles:\n",
    "            if style.lower().replace('_', ' ') in response_lower:\n",
    "                return style\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def classify_object(self, image, objects=None, debug=False):\n",
    "        \"\"\"\n",
    "        Classify image object using TRACE paper's exact prompt format.\n",
    "        \n",
    "        From TRACE paper Figure 7:\n",
    "        'Classify the object depicted in this image.\n",
    "         Choose exactly one option from the numbered list.\n",
    "         Respond with only the number.'\n",
    "        \"\"\"\n",
    "        objects = objects or OBJECTS\n",
    "        \n",
    "        # Build numbered options list (TRACE paper format)\n",
    "        options_list = '\\n'.join([f\"{i+1}. {o.replace('_', ' ')}\" for i, o in enumerate(objects)])\n",
    "        \n",
    "        prompt = (\n",
    "            \"Classify the object depicted in this image.\\n\"\n",
    "            \"Choose exactly one option from the numbered list.\\n\"\n",
    "            \"Respond with only the number.\\n\"\n",
    "            f\"Object categories:\\n{options_list}\"\n",
    "        )\n",
    "        \n",
    "        response = self._generate_response(image, prompt)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"LLaVA object response: '{response}'\")\n",
    "        \n",
    "        idx = self._parse_number_response(response, len(objects))\n",
    "        if idx is not None:\n",
    "            return objects[idx]\n",
    "        \n",
    "        # Fallback: try to match object name in response\n",
    "        response_lower = response.lower()\n",
    "        for obj in objects:\n",
    "            if obj.lower().replace('_', ' ') in response_lower:\n",
    "                return obj\n",
    "        \n",
    "        return None\n",
    "\n",
    "print(\"\u2713 LLaVAClassifier class defined (TRACE paper format)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: UNLEARNCANVAS EVALUATOR (Supports both CLIP and LLaVA)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "UnlearnCanvas-style evaluation with configurable classifier.\n",
    "\n",
    "Classifier Options:\n",
    "- CLIP: Fast zero-shot classification (~0.1s/image)\n",
    "- LLaVA: More accurate VLM-based classification (~2-5s/image)\n",
    "\n",
    "Set USE_LLAVA = True to use LLaVA, False for CLIP.\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================================================\n",
    "# CLASSIFIER CONFIGURATION - CHANGE THIS\n",
    "# ==========================================================================\n",
    "USE_LLAVA = True  # True = LLaVA (more accurate), False = CLIP (faster)\n",
    "\n",
    "class UnlearnCanvasEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate unlearning performance using UnlearnCanvas metrics.\n",
    "    \n",
    "    Supports both CLIP (fast) and LLaVA (accurate) classification.\n",
    "    \n",
    "    Metrics:\n",
    "    - UA (Unlearning Accuracy): 1 - accuracy on target concept\n",
    "    - IRA (In-domain Retain Accuracy): accuracy on same-domain concepts\n",
    "    - CRA (Cross-domain Retain Accuracy): accuracy on other-domain concepts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cuda\", use_llava=None):\n",
    "        self.device = device\n",
    "        self.use_llava = use_llava if use_llava is not None else USE_LLAVA\n",
    "        \n",
    "        print(f\"Initializing UnlearnCanvas Evaluator (classifier: {'LLaVA' if self.use_llava else 'CLIP'})...\")\n",
    "        \n",
    "        if self.use_llava:\n",
    "            self.llava = LLaVAClassifier(device=device)\n",
    "            print(\"  \u2192 LLaVA classifier selected (will load on first use)\")\n",
    "        else:\n",
    "            self.llava = None\n",
    "            print(\"  \u2192 CLIP classifier selected (fast mode)\")\n",
    "        \n",
    "        # Load CLIP (also needed for CLIP Score metric)\n",
    "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "        self.clip_model.eval()\n",
    "        \n",
    "        # Pre-compute text embeddings only for CLIP classifier mode\n",
    "        if not self.use_llava:\n",
    "            self._precompute_text_embeddings()\n",
    "        print(\"\u2713 UnlearnCanvas Evaluator ready!\")\n",
    "    \n",
    "    def _precompute_text_embeddings(self):\n",
    "        \"\"\"Pre-compute CLIP text embeddings for all styles and objects.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Style embeddings\n",
    "            style_texts = [f\"A painting in {s.replace('_', ' ')} style\" for s in STYLES]\n",
    "            style_tokens = clip.tokenize(style_texts).to(self.device)\n",
    "            self.style_embeddings = self.clip_model.encode_text(style_tokens)\n",
    "            self.style_embeddings = self.style_embeddings / self.style_embeddings.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Object embeddings\n",
    "            object_texts = [f\"A painting of {o.replace('_', ' ')}\" for o in OBJECTS]\n",
    "            object_tokens = clip.tokenize(object_texts).to(self.device)\n",
    "            self.object_embeddings = self.clip_model.encode_text(object_tokens)\n",
    "            self.object_embeddings = self.object_embeddings / self.object_embeddings.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    def classify_image(self, image, domain=\"style\"):\n",
    "        \"\"\"\n",
    "        Classify an image into style or object category.\n",
    "        Uses LLaVA if configured, otherwise CLIP.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            domain: \"style\" or \"object\"\n",
    "            \n",
    "        Returns:\n",
    "            Predicted class name\n",
    "        \"\"\"\n",
    "        # Use LLaVA if configured\n",
    "        if self.use_llava and self.llava is not None:\n",
    "            if domain == \"style\":\n",
    "                result = self.llava.classify_style(image)\n",
    "            else:\n",
    "                result = self.llava.classify_object(image)\n",
    "            # Return if valid, otherwise fallback to CLIP\n",
    "            if result is not None:\n",
    "                return result\n",
    "        \n",
    "        # CLIP classification (default or fallback)\n",
    "        if not hasattr(self, 'style_embeddings'):\n",
    "            self._precompute_text_embeddings()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            image_input = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
    "            image_features = self.clip_model.encode_image(image_input)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            if domain == \"style\":\n",
    "                similarities = (image_features @ self.style_embeddings.T).squeeze(0)\n",
    "                pred_idx = similarities.argmax().item()\n",
    "                return STYLES[pred_idx]\n",
    "            else:\n",
    "                similarities = (image_features @ self.object_embeddings.T).squeeze(0)\n",
    "                pred_idx = similarities.argmax().item()\n",
    "                return OBJECTS[pred_idx]\n",
    "    \n",
    "    def evaluate_unlearning(\n",
    "        self,\n",
    "        steerer,\n",
    "        vectors,\n",
    "        target_concept,\n",
    "        target_type=\"style\",\n",
    "        beta=2.0,\n",
    "        clip_negative=True,\n",
    "        eval_seeds=None,\n",
    "        save_images=True,\n",
    "        output_dir=None,\n",
    "        generate_baselines=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate unlearning performance following UnlearnCanvas protocol.\n",
    "        \n",
    "        TWO-PHASE approach to manage VRAM:\n",
    "          Phase 1: Generate ALL images with FLUX (steerer) and save to disk.\n",
    "                   Skips images that already exist (resume support).\n",
    "          Phase 2: Unload FLUX from VRAM, load LLaVA, classify all saved\n",
    "                   images from disk to compute UA, IRA, CRA.\n",
    "        \n",
    "        Full grid: ALL styles x ALL objects x ALL eval seeds.\n",
    "        \"\"\"\n",
    "        eval_seeds = eval_seeds or EVAL_SEEDS\n",
    "        output_dir = output_dir or os.path.join(STEERED_DIR, f\"{target_concept}_{steerer.mode}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Baseline dir for comparison images (without steering)\n",
    "        baseline_dir = os.path.join(BASELINE_DIR, target_concept)\n",
    "        if generate_baselines:\n",
    "            os.makedirs(baseline_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EVALUATING UNLEARNING: {target_concept} ({target_type})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Build full grid of test cases\n",
    "        test_cases = []\n",
    "        for style in STYLES:\n",
    "            for obj in OBJECTS:\n",
    "                for seed in eval_seeds:\n",
    "                    filename = f\"{style}_{obj}_seed{seed}.jpg\"\n",
    "                    prompt = f\"A {obj.replace('_', ' ')} image in {style.replace('_', ' ')} style.\"\n",
    "                    test_cases.append({\n",
    "                        \"prompt\": prompt,\n",
    "                        \"seed\": seed,\n",
    "                        \"gt_style\": style,\n",
    "                        \"gt_object\": obj,\n",
    "                        \"filename\": filename,\n",
    "                    })\n",
    "        \n",
    "        total_images = len(test_cases)\n",
    "        \n",
    "        # ==============================================================\n",
    "        # PHASE 1: Generate all images with FLUX (with resume support)\n",
    "        # ==============================================================\n",
    "        skipped = 0\n",
    "        generated = 0\n",
    "        print(f\"\\n--- PHASE 1: IMAGE GENERATION ---\")\n",
    "        print(f\"Grid: {len(STYLES)} styles x {len(OBJECTS)} objects x {len(eval_seeds)} seeds = {total_images} images\")\n",
    "        print(f\"Steering: beta={beta}\")\n",
    "        print(f\"Output: {output_dir}\")\n",
    "        \n",
    "        for i, case in enumerate(tqdm(test_cases, desc=\"Phase 1: Generating\")):\n",
    "            save_path = os.path.join(output_dir, case[\"filename\"])\n",
    "            \n",
    "            # Resume support: skip if image already exists\n",
    "            if os.path.exists(save_path):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            img = steerer.generate(\n",
    "                case[\"prompt\"],\n",
    "                case[\"seed\"],\n",
    "                vectors=vectors,\n",
    "                beta=beta,\n",
    "                clip_negative=clip_negative\n",
    "            )\n",
    "            img.save(save_path)\n",
    "            generated += 1\n",
    "        \n",
    "        print(f\"Phase 1 done: {generated} generated, {skipped} skipped (already existed)\")\n",
    "        \n",
    "        # Generate baseline comparison images (without steering)\n",
    "        if generate_baselines:\n",
    "            print(\"Generating baseline images (no steering) for comparison...\")\n",
    "            sample_configs = []\n",
    "            if target_type == \"style\":\n",
    "                sample_configs = [\n",
    "                    (target_concept, \"Dog\"), (target_concept, \"Cat\"), (target_concept, \"Bird\")\n",
    "                ]\n",
    "            else:\n",
    "                sample_configs = [\n",
    "                    (\"Van_Gogh\", target_concept), (\"Cartoon\", target_concept), (\"Pop_Art\", target_concept)\n",
    "                ]\n",
    "            for s, o in sample_configs:\n",
    "                fname = f\"{s}_{o}_seed{eval_seeds[0]}.jpg\"\n",
    "                base_path = os.path.join(baseline_dir, fname)\n",
    "                if not os.path.exists(base_path):\n",
    "                    prompt = f\"A {o.replace('_', ' ')} image in {s.replace('_', ' ')} style.\"\n",
    "                    img_base = steerer.generate(prompt, seed=eval_seeds[0], vectors=None)\n",
    "                    img_base.save(base_path)\n",
    "        \n",
    "        # ==============================================================\n",
    "        # FREE FLUX VRAM before loading LLaVA\n",
    "        # ==============================================================\n",
    "        print(\"\\nFreeing FLUX VRAM before classification phase...\")\n",
    "        steerer.pipe.to('cpu')\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"FLUX moved to CPU. VRAM freed for LLaVA.\")\n",
    "        \n",
    "        # ==============================================================\n",
    "        # PHASE 2: Classify all saved images from disk\n",
    "        # ==============================================================\n",
    "        print(f\"\\n--- PHASE 2: CLASSIFICATION ({total_images} images) ---\")\n",
    "        \n",
    "        results = {\n",
    "            \"target_correct\": 0, \"target_total\": 0,\n",
    "            \"ira_correct\": 0, \"ira_total\": 0,\n",
    "            \"cra_correct\": 0, \"cra_total\": 0,\n",
    "            \"prompts\": []\n",
    "        }\n",
    "        \n",
    "        for i, case in enumerate(tqdm(test_cases, desc=\"Phase 2: Classifying\")):\n",
    "            img_path = os.path.join(output_dir, case[\"filename\"])\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"  WARNING: Missing image {case['filename']}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Load image from disk (not keeping in RAM)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            results[\"prompts\"].append(case[\"prompt\"])\n",
    "            \n",
    "            gt_style = case[\"gt_style\"]\n",
    "            gt_object = case[\"gt_object\"]\n",
    "            \n",
    "            if target_type == \"style\":\n",
    "                pred_style = self.classify_image(img, domain=\"style\")\n",
    "                pred_object = self.classify_image(img, domain=\"object\")\n",
    "                \n",
    "                # UA: images whose prompt IS the target style\n",
    "                if gt_style == target_concept:\n",
    "                    results[\"target_total\"] += 1\n",
    "                    if pred_style == target_concept:\n",
    "                        results[\"target_correct\"] += 1\n",
    "                # IRA: other styles in same domain\n",
    "                else:\n",
    "                    results[\"ira_total\"] += 1\n",
    "                    if pred_style == gt_style:\n",
    "                        results[\"ira_correct\"] += 1\n",
    "                \n",
    "                # CRA: object accuracy across ALL images\n",
    "                results[\"cra_total\"] += 1\n",
    "                if pred_object == gt_object:\n",
    "                    results[\"cra_correct\"] += 1\n",
    "            \n",
    "            else:  # target_type == \"object\"\n",
    "                pred_object = self.classify_image(img, domain=\"object\")\n",
    "                pred_style = self.classify_image(img, domain=\"style\")\n",
    "                \n",
    "                if gt_object == target_concept:\n",
    "                    results[\"target_total\"] += 1\n",
    "                    if pred_object == target_concept:\n",
    "                        results[\"target_correct\"] += 1\n",
    "                else:\n",
    "                    results[\"ira_total\"] += 1\n",
    "                    if pred_object == gt_object:\n",
    "                        results[\"ira_correct\"] += 1\n",
    "                \n",
    "                results[\"cra_total\"] += 1\n",
    "                if pred_style == gt_style:\n",
    "                    results[\"cra_correct\"] += 1\n",
    "            \n",
    "            # Progress log every 50 images\n",
    "            if (i + 1) % 50 == 0:\n",
    "                _ua = 1.0 - (results[\"target_correct\"] / max(results[\"target_total\"], 1))\n",
    "                _ira = results[\"ira_correct\"] / max(results[\"ira_total\"], 1)\n",
    "                _cra = results[\"cra_correct\"] / max(results[\"cra_total\"], 1)\n",
    "                print(f\"  [{i+1}/{total_images}] Running UA={_ua:.1%} IRA={_ira:.1%} CRA={_cra:.1%}\")\n",
    "        \n",
    "        # ==============================================================\n",
    "        # PHASE 2 DONE: Unload LLaVA, reload FLUX back to GPU\n",
    "        # ==============================================================\n",
    "        print(\"\\nClassification done. Unloading LLaVA, reloading FLUX to GPU...\")\n",
    "        if self.use_llava and self.llava is not None:\n",
    "            self.llava.unload()\n",
    "        steerer.pipe.to(steerer.device)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"FLUX reloaded to GPU.\")\n",
    "        \n",
    "        # ==============================================================\n",
    "        # Calculate final metrics\n",
    "        # ==============================================================\n",
    "        ua = 1.0 - (results[\"target_correct\"] / max(results[\"target_total\"], 1))\n",
    "        ira = results[\"ira_correct\"] / max(results[\"ira_total\"], 1)\n",
    "        cra = results[\"cra_correct\"] / max(results[\"cra_total\"], 1)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FINAL RESULTS: {target_concept}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"UA  (Unlearning Accuracy):     {ua:.2%}  ({results['target_total'] - results['target_correct']}/{results['target_total']} not classified as target)\")\n",
    "        print(f\"IRA (In-Domain Retain):        {ira:.2%}  ({results['ira_correct']}/{results['ira_total']} correct in same domain)\")\n",
    "        print(f\"CRA (Cross-Domain Retain):     {cra:.2%}  ({results['cra_correct']}/{results['cra_total']} correct in cross domain)\")\n",
    "        print(f\"Total images:                  {total_images}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        return {\n",
    "            \"UA\": ua,\n",
    "            \"IRA\": ira,\n",
    "            \"CRA\": cra,\n",
    "            \"target_concept\": target_concept,\n",
    "            \"target_type\": target_type,\n",
    "            \"beta\": beta,\n",
    "            \"n_images\": total_images,\n",
    "            \"prompts\": results[\"prompts\"]\n",
    "        }\n",
    "\n",
    "print(\"\u2713 UnlearnCanvasEvaluator class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: LOAD MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# HuggingFace login (for gated models)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    from huggingface_hub import login\n",
    "    hf_token = userdata.get(\"hf_token\")\n",
    "    login(hf_token)\n",
    "    print(\"\u2713 HuggingFace authenticated\")\n",
    "except:\n",
    "    hf_token = None\n",
    "    print(\"\u26a0 No HuggingFace token found, some models may not load\")\n",
    "\n",
    "# Load FLUX pipeline\n",
    "print(f\"\\nLoading FLUX pipeline: {MODEL_ID}...\")\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=hf_token,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(DEVICE)\n",
    "print(\"\u2713 FLUX pipeline loaded\")\n",
    "\n",
    "# Initialize FluxSteering\n",
    "# Available modes (pick ONE):\n",
    "#   \"entry_point\"  \u2192 context_embedder + time_text_embed (TRACE-style, GAP-fixed)\n",
    "#   \"block\"        \u2192 per-block to_out + proj_out (CASteer-style, SingleStream text-only)\n",
    "#   \"double_proj\"  \u2192 add_k_proj + add_q_proj in DoubleStream\n",
    "#   \"all\"          \u2192 entry_point + double_proj + double to_out (maximum coverage)\n",
    "#   \"hybrid\"       \u2192 entry points + add_k/add_q (BEST for STYLE unlearning)\n",
    "#   \"joint_attn\"   \u2192 double to_out only, simple mean, no renorm (OBJECT unlearning)\n",
    "#                    Use with learn_vectors() + simple prompts (\"Dog\" / \"Object\")\n",
    "#   \"object_v2\"    \u2192 entry points + to_out + SingleStream img (alt OBJECT approach)\n",
    "#                    Use with learn_vectors_diverse() + make_object_prompts()\n",
    "STEERING_MODE = \"joint_attn\"  # Change to \"hybrid\" for style unlearning\n",
    "print(f\"\\nInitializing FluxSteering (mode={STEERING_MODE})...\")\n",
    "steerer = FluxSteering(pipe, device=DEVICE, n_steps=N_STEPS, mode=STEERING_MODE)\n",
    "\n",
    "# Initialize evaluators\n",
    "print(\"\\nInitializing evaluators...\")\n",
    "evaluator = UnlearnCanvasEvaluator(device=DEVICE)\n",
    "quality_metrics = QualityMetrics(device=DEVICE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 ALL MODELS LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# CELL 7: EXPERIMENT CONFIGURATION\n# ============================================================================\n\"\"\"\nConfigure which concept to unlearn.\nChange TARGET_CONCEPT and TARGET_TYPE for different experiments.\n\"\"\"\n\n# ==========================================================================\n# EXPERIMENT CONFIGURATION - MODIFY THESE\n# ==========================================================================\nTARGET_CONCEPT = \"Dog\"        # Concept to unlearn (e.g., \"Dog\", \"Van_Gogh\")\nTARGET_TYPE = \"object\"        # \"style\" or \"object\"\n# Steering strength:\n#   Style (hybrid mode): beta=2-5 works well\n#   Objects (joint_attn mode): start with beta=5.0, tune from there\nif TARGET_TYPE == \"style\":\n    BETA = 2.0\nelif STEERING_MODE == \"joint_attn\":\n    BETA = 5.0\nelse:\n    BETA = 15.0\n\n# ==========================================================================\n# Automatic configuration\n# ==========================================================================\n# Include mode in output dir to prevent cross-mode caching!\nOUTPUT_DIR = os.path.join(RESULTS_DIR, f\"{TARGET_CONCEPT}_{STEERING_MODE}\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# ---- PROMPT CONFIGURATION ----\n# All object modes now use diverse CASteer-style prompt pairs.\n# The 50 diverse contexts cancel out, isolating the target concept direction.\n# This is CRITICAL for objects: ensures the vector captures \"Dog\" specifically,\n# not \"general content\" or \"things about this particular scene.\"\nif TARGET_TYPE == \"style\":\n    DIVERSE_PROMPT_PAIRS = make_style_prompts(TARGET_CONCEPT.replace('_', ' '), NUM_DIVERSE_PROMPTS)\n    pos_prompt = f\"{TARGET_CONCEPT.replace('_', ' ')} style\"\n    neg_prompt = \"neutral style\"\nelse:\n    DIVERSE_PROMPT_PAIRS = make_object_prompts(TARGET_CONCEPT.replace('_', ' '), NUM_DIVERSE_PROMPTS)\n    pos_prompt = f\"{TARGET_CONCEPT.replace('_', ' ')}\"\n    neg_prompt = \"Object\"\n\nprint(\"=\"*70)\nprint(\"EXPERIMENT CONFIGURATION\")\nprint(\"=\"*70)\nprint(f\"Target Concept:    {TARGET_CONCEPT}\")\nprint(f\"Target Type:       {TARGET_TYPE}\")\nprint(f\"Steering Mode:     {STEERING_MODE}\")\nprint(f\"Steering Strength: \\u03b2 = {BETA}\")\nif TARGET_TYPE == \"object\":\n    print(f\"Prompt Strategy:   {len(DIVERSE_PROMPT_PAIRS)} diverse pairs (CASteer methodology)\")\n    print(f\"  Example pos:     '{DIVERSE_PROMPT_PAIRS[0][0]}'\")\n    print(f\"  Example neg:     '{DIVERSE_PROMPT_PAIRS[0][1]}'\")\n    print(f\"  Rationale:       Averaging across 50 contexts cancels non-Dog info,\")\n    print(f\"                   isolating the 'Dog' direction so ONLY the dog is removed.\")\nelse:\n    print(f\"Prompt Strategy:   {len(DIVERSE_PROMPT_PAIRS)} diverse pairs\")\n    print(f\"  Example pos:     '{DIVERSE_PROMPT_PAIRS[0][0]}'\")\n    print(f\"  Example neg:     '{DIVERSE_PROMPT_PAIRS[0][1]}'\")\nprint(f\"Eval Seeds:        {len(EVAL_SEEDS)}\")\nprint(f\"Output Directory:  {OUTPUT_DIR}\")\nprint(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# CELL 8: LEARN STEERING VECTORS\n# ============================================================================\n\nprint(f\"Learning steering vectors for: {TARGET_CONCEPT}\")\nprint(f\"Mode: {STEERING_MODE}\")\n\n# ===========================================================================\n# ALL modes now use diverse prompt pairs for object unlearning.\n# This is the CASteer methodology: 50 diverse contexts (\"tench with Dog\" vs\n# \"tench\", \"goldfish with Dog\" vs \"goldfish\", etc.) with the SAME seed.\n# Averaging cancels context-specific noise, isolating the target concept.\n# ===========================================================================\nif TARGET_TYPE == \"object\":\n    print(f\"Using {len(DIVERSE_PROMPT_PAIRS)} diverse prompt pairs (CASteer methodology)\")\n    print(f\"  Example: '{DIVERSE_PROMPT_PAIRS[0][0]}' vs '{DIVERSE_PROMPT_PAIRS[0][1]}'\\n\")\n    vectors = steerer.learn_vectors_diverse(\n        prompt_pairs=DIVERSE_PROMPT_PAIRS,\n        seed=0,\n        top_k=TOP_K_VECTORS,\n        verbose=True\n    )\n    vector_path = os.path.join(VECTOR_DIR, f\"{TARGET_CONCEPT}_{STEERING_MODE}_diverse_vectors.pt\")\nelse:\n    # Style unlearning: diverse prompts also work well\n    print(f\"Using {len(DIVERSE_PROMPT_PAIRS)} diverse prompt pairs\\n\")\n    vectors = steerer.learn_vectors_diverse(\n        prompt_pairs=DIVERSE_PROMPT_PAIRS,\n        seed=0,\n        top_k=TOP_K_VECTORS,\n        verbose=True\n    )\n    vector_path = os.path.join(VECTOR_DIR, f\"{TARGET_CONCEPT}_{STEERING_MODE}_diverse_vectors.pt\")\n\n# Save vectors for reproducibility\nsteerer.save_vectors(vectors, vector_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8B: QUICK STEERING TEST \u2014 Run BEFORE full evaluation!\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Tests steering with multiple beta values + clip_negative settings.\n",
    "Generates 6 images total \u2014 takes ~30 seconds. Shows immediate visual results.\n",
    "\n",
    "KEY: If ALL images look identical to baseline, there's a fundamental issue.\n",
    "     If higher beta or clip_negative=False produces different images, we have\n",
    "     the right settings. Then run the full evaluation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "DIAG_PROMPT = f\"A {TARGET_CONCEPT.replace('_', ' ')} image in Van Gogh style.\"\n",
    "DIAG_SEED = 42\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"QUICK STEERING TEST: {TARGET_CONCEPT} ({STEERING_MODE})\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: '{DIAG_PROMPT}'\")\n",
    "print(f\"Vectors: {len(vectors)} layers, \"\n",
    "      f\"{sum(len(v) for v in vectors.values())} (layer,step) pairs\\n\")\n",
    "\n",
    "# --- Test 0: Destructive hook test (confirm hooks fire at all) ---\n",
    "print(\"TEST 0: DESTRUCTIVE HOOK \u2014 confirm hooks fire during generation\")\n",
    "print(\"  Zeroing out context_embedder output (should produce garbled image)...\")\n",
    "_handle = steerer.target_layers[\"context_embedder\"].register_forward_hook(\n",
    "    lambda m, i, o: o * 0  # Kill all text info\n",
    ")\n",
    "destructive_img = steerer._run_pipe_base(DIAG_PROMPT, DIAG_SEED)\n",
    "_handle.remove()\n",
    "\n",
    "# --- Generate baseline ---\n",
    "print(\"\\nGenerating baseline (no steering)...\")\n",
    "baseline_img = steerer.generate(DIAG_PROMPT, DIAG_SEED, vectors=None)\n",
    "\n",
    "# --- Test configurations ---\n",
    "configs = [\n",
    "    (\"\u03b2=2, clip_neg=True\",   2.0,  True),\n",
    "    (\"\u03b2=15, clip_neg=True\",  15.0, True),\n",
    "    (\"\u03b2=50, clip_neg=True\",  50.0, True),\n",
    "    (\"\u03b2=15, clip_neg=False\", 15.0, False),\n",
    "    (\"\u03b2=50, clip_neg=False\", 50.0, False),\n",
    "]\n",
    "\n",
    "test_images = []\n",
    "for label, beta_val, clip_val in configs:\n",
    "    print(f\"  Generating: {label}...\")\n",
    "    img = steerer.generate(DIAG_PROMPT, DIAG_SEED, vectors=vectors,\n",
    "                           beta=beta_val, clip_negative=clip_val)\n",
    "    test_images.append((label, img))\n",
    "\n",
    "# --- Visual comparison ---\n",
    "n_imgs = 2 + len(test_images)  # destructive + baseline + tests\n",
    "fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Destructive test\n",
    "axes[0].imshow(destructive_img)\n",
    "axes[0].set_title(\"TEST 0: context_embedder=0\\n(should be garbled)\", fontsize=10, color='red')\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Baseline\n",
    "axes[1].imshow(baseline_img)\n",
    "axes[1].set_title(\"Baseline (no steering)\", fontsize=10)\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Test images\n",
    "baseline_arr = np.array(baseline_img).astype(float)\n",
    "for i, (label, img) in enumerate(test_images):\n",
    "    ax = axes[i + 2]\n",
    "    ax.imshow(img)\n",
    "    # Compute pixel difference\n",
    "    diff = np.abs(np.array(img).astype(float) - baseline_arr)\n",
    "    mean_diff = diff.mean()\n",
    "    max_diff = diff.max()\n",
    "    pct_changed = (diff > 1.0).mean() * 100\n",
    "    ax.set_title(f\"{label}\\ndiff: mean={mean_diff:.1f}, {pct_changed:.0f}% changed\", fontsize=10,\n",
    "                 color='green' if pct_changed > 10 else 'orange' if pct_changed > 1 else 'red')\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Hide unused axes\n",
    "for i in range(n_imgs, len(axes)):\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Object Steering Test: {TARGET_CONCEPT} ({STEERING_MODE})\\n\"\n",
    "             f\"Vectors: {sum(len(v) for v in vectors.values())} total\",\n",
    "             fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check destructive test\n",
    "dest_diff = np.abs(np.array(destructive_img).astype(float) - baseline_arr).mean()\n",
    "if dest_diff > 10:\n",
    "    print(\"\u2713 TEST 0 PASSED: Hooks DO fire (destructive test produced different image)\")\n",
    "else:\n",
    "    print(\"\u2717 TEST 0 FAILED: Hooks DON'T fire! context_embedder\u00d70 had no effect!\")\n",
    "    print(\"  \u2192 This means PyTorch hooks don't work with this FLUX pipeline.\")\n",
    "    print(\"  \u2192 Try: steerer.pipe.transformer = torch.compile(steerer.pipe.transformer, mode='reduce-overhead')\")\n",
    "    print(\"  \u2192 Or: monkey-patch the forward method directly instead of using hooks.\")\n",
    "\n",
    "print()\n",
    "for label, img in test_images:\n",
    "    diff = np.abs(np.array(img).astype(float) - baseline_arr)\n",
    "    mean_diff = diff.mean()\n",
    "    pct = (diff > 1.0).mean() * 100\n",
    "    status = \"\u2713 WORKING\" if pct > 10 else \"\u26a0 WEAK\" if pct > 1 else \"\u2717 NO EFFECT\"\n",
    "    print(f\"  {status}: {label:<25} mean_diff={mean_diff:>6.1f}  pixels_changed={pct:>5.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"NEXT STEPS:\")\n",
    "best_config = None\n",
    "for label, img in test_images:\n",
    "    diff = np.abs(np.array(img).astype(float) - baseline_arr)\n",
    "    pct = (diff > 1.0).mean() * 100\n",
    "    if pct > 10:\n",
    "        best_config = label\n",
    "        break\n",
    "if best_config:\n",
    "    print(f\"  \u2192 Best config: {best_config}\")\n",
    "    print(f\"  \u2192 Update BETA and CLIP_NEGATIVE in Cell 7, then run full evaluation\")\n",
    "else:\n",
    "    print(\"  \u2192 No configuration produced visible steering effect\")\n",
    "    print(\"  \u2192 If TEST 0 passed: the learned DIRECTIONS don't capture 'Dog'\")\n",
    "    print(\"     at the evaluation prompt format. Try different prompt templates.\")\n",
    "    print(\"  \u2192 If TEST 0 failed: hooks don't work, need different approach.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: UNLEARNCANVAS EVALUATION (UA, IRA, CRA)\n",
    "# ============================================================================\n",
    "# TWO-PHASE EVALUATION:\n",
    "#   Phase 1: Generate ALL images with FLUX (skips existing = resume support)\n",
    "#   Phase 2: Unload FLUX, load LLaVA, classify all images from disk\n",
    "# Total images = len(STYLES) * len(OBJECTS) * len(EVAL_SEEDS)\n",
    "# e.g., 10 styles x 20 objects x 3 seeds = 600 images\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Running UnlearnCanvas evaluation (two-phase: generate then classify)...\")\n",
    "print(f\"This will generate {len(STYLES) * len(OBJECTS) * len(EVAL_SEEDS)} images.\\n\")\n",
    "\n",
    "# Evaluate unlearning on full grid\n",
    "# clip_negative=False for objects: allows steering even if dot products are negative\n",
    "# (learned direction from \"tench with Dog\" may be flipped relative to \"A Dog image in...\")\n",
    "CLIP_NEGATIVE = True if TARGET_TYPE == \"style\" else False\n",
    "\n",
    "eval_results = evaluator.evaluate_unlearning(\n",
    "    steerer=steerer,\n",
    "    vectors=vectors,\n",
    "    target_concept=TARGET_CONCEPT,\n",
    "    target_type=TARGET_TYPE,\n",
    "    beta=BETA,\n",
    "    clip_negative=CLIP_NEGATIVE,\n",
    "    eval_seeds=EVAL_SEEDS,\n",
    "    save_images=True,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    generate_baselines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: QUALITY METRICS (FID, CLIP Score)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CALCULATING QUALITY METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "quality_results = {}\n",
    "\n",
    "# Load generated images from disk for quality metrics\n",
    "print(\"Loading generated images from disk for quality metrics...\")\n",
    "_gen_images = []\n",
    "_gen_prompts = []\n",
    "for fname in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if fname.endswith(\".jpg\") or fname.endswith(\".png\"):\n",
    "        _gen_images.append(Image.open(os.path.join(OUTPUT_DIR, fname)).convert(\"RGB\"))\n",
    "        # Reconstruct prompt from filename: Style_Object_seedN.jpg\n",
    "        parts = fname.rsplit(\"_seed\", 1)[0]  # \"Style_Object\"\n",
    "        style_obj = parts.split(\"_\", 1) if \"_\" in parts else [parts, \"\"]\n",
    "        _gen_prompts.append(f\"A {style_obj[-1].replace('_', ' ')} image in {style_obj[0].replace('_', ' ')} style.\")\n",
    "print(f\"Loaded {len(_gen_images)} images from {OUTPUT_DIR}\")\n",
    "\n",
    "# CLIP Score\n",
    "print(\"\\n1. CLIP Score (text-image alignment)...\")\n",
    "clip_score = quality_metrics.calculate_clip_score(\n",
    "    _gen_images,\n",
    "    eval_results[\"prompts\"] if eval_results[\"prompts\"] else _gen_prompts\n",
    ")\n",
    "quality_results[\"CLIP_Score\"] = clip_score\n",
    "if clip_score:\n",
    "    print(f\"   \u2713 CLIP Score: {clip_score:.4f}\")\n",
    "\n",
    "# FID (requires baseline images)\n",
    "print(\"\\n2. FID Score (image quality)...\")\n",
    "baseline_path = os.path.join(BASELINE_DIR, TARGET_CONCEPT)\n",
    "if os.path.exists(baseline_path) and len(os.listdir(baseline_path)) > 0:\n",
    "    fid_score = quality_metrics.calculate_fid(baseline_path, OUTPUT_DIR)\n",
    "    quality_results[\"FID\"] = fid_score\n",
    "    if fid_score:\n",
    "        print(f\"   \u2713 FID: {fid_score:.2f}\")\n",
    "else:\n",
    "    print(f\"   \u26a0 Baseline images not found at {baseline_path}\")\n",
    "    print(\"   \u2192 Generate baseline images first (without steering)\")\n",
    "    quality_results[\"FID\"] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: COMPILE AND SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "# Compile all results\n",
    "final_results = {\n",
    "    \"Target_Concept\": TARGET_CONCEPT,\n",
    "    \"Target_Type\": TARGET_TYPE,\n",
    "    \"Beta\": BETA,\n",
    "    \"UA\": eval_results[\"UA\"],\n",
    "    \"IRA\": eval_results[\"IRA\"],\n",
    "    \"CRA\": eval_results[\"CRA\"],\n",
    "    \"CLIP_Score\": quality_results.get(\"CLIP_Score\"),\n",
    "    \"FID\": quality_results.get(\"FID\"),\n",
    "    \"n_images\": eval_results[\"n_images\"],\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save to CSV (append mode)\n",
    "df_new = pd.DataFrame([final_results])\n",
    "if os.path.exists(RESULTS_CSV):\n",
    "    df_existing = pd.read_csv(RESULTS_CSV)\n",
    "    df_all = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "else:\n",
    "    df_all = df_new\n",
    "df_all.to_csv(RESULTS_CSV, index=False)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTarget: {TARGET_CONCEPT} ({TARGET_TYPE})\")\n",
    "print(f\"Steering \u03b2: {BETA}\")\n",
    "print(f\"\\n--- UnlearnCanvas Metrics ---\")\n",
    "print(f\"UA  (Unlearning Accuracy):     {eval_results['UA']:.2%}\")\n",
    "print(f\"IRA (In-Domain Retain):        {eval_results['IRA']:.2%}\")\n",
    "print(f\"CRA (Cross-Domain Retain):     {eval_results['CRA']:.2%}\")\n",
    "print(f\"\\n--- Quality Metrics ---\")\n",
    "if quality_results.get(\"CLIP_Score\"):\n",
    "    print(f\"CLIP Score:                    {quality_results['CLIP_Score']:.4f}\")\n",
    "if quality_results.get(\"FID\"):\n",
    "    print(f\"FID:                           {quality_results['FID']:.2f}\")\n",
    "print(f\"\\n--- Files ---\")\n",
    "print(f\"Results CSV: {RESULTS_CSV}\")\n",
    "print(f\"Vectors:     {vector_path}\")\n",
    "print(f\"Images:      {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: GENERATE COMPARISON TABLE (vs Baselines)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Compare our results against baselines from the TRACE paper (ICLR 2026).\n",
    "- Table 1 (FLUX baselines): LOCOEDIT, UCE, TRACE - most relevant comparison\n",
    "- Table 2 (SD1.5 baselines): broader context from UnlearnCanvas benchmark\n",
    "\"\"\"\n",
    "\n",
    "# --- Table A: FLUX baselines from TRACE Table 1 (most relevant) ---\n",
    "flux_baselines = {\n",
    "    \"Method\": [\"LOCOEDIT (Flux)\", \"UCE (Flux)\", \"TRACE (Flux)\", \"Ours (Steering)\"],\n",
    "    \"UA\": [66.45, 67.43, 88.60, eval_results[\"UA\"]*100],\n",
    "    \"IRA\": [33.23, 34.78, 36.10, eval_results[\"IRA\"]*100],\n",
    "    \"CRA\": [83.44, 76.56, 96.40, eval_results[\"CRA\"]*100],\n",
    "    \"FID\": [55.56, 58.90, 51.67, quality_results.get(\"FID\", \"N/A\")]\n",
    "}\n",
    "\n",
    "df_flux = pd.DataFrame(flux_baselines)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON WITH FLUX BASELINES (TRACE Table 1 - Style Removal)\")\n",
    "print(\"=\"*70)\n",
    "print(df_flux.to_string(index=False))\n",
    "print(\"\\nSource: TRACE paper (ICLR 2026), Table 1\")\n",
    "print(\"Higher UA = better unlearning, Higher IRA/CRA = better retention\")\n",
    "print(\"Lower FID = better image quality\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Table B: SD1.5 baselines from TRACE Table 2 (broader context) ---\n",
    "sd15_baselines = {\n",
    "    \"Method\": [\"ESD\", \"FMN\", \"UCE\", \"CA\", \"SalUn\", \"SEOT\", \"SPM\", \"EDiff\", \"SHS\", \"SAeUron\", \"TRACE\"],\n",
    "    \"UA\": [98.58, 88.48, 98.40, 60.82, 86.26, 56.90, 60.94, 92.42, 95.84, 95.80, 95.02],\n",
    "    \"IRA\": [80.97, 56.77, 60.22, 96.01, 90.39, 94.68, 92.39, 73.91, 80.42, 99.10, 93.84],\n",
    "    \"CRA\": [93.96, 46.60, 47.71, 92.70, 95.08, 84.31, 84.33, 98.93, 43.27, 99.40, 86.22],\n",
    "}\n",
    "\n",
    "df_sd15 = pd.DataFrame(sd15_baselines)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SD1.5 BASELINES FOR BROADER CONTEXT (TRACE Table 2)\")\n",
    "print(\"=\"*70)\n",
    "print(df_sd15.to_string(index=False))\n",
    "print(\"\\nNote: SD1.5 numbers are NOT directly comparable to FLUX results.\")\n",
    "print(\"They are provided for broader context only.\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save comparison tables\n",
    "comparison_path = os.path.join(TABLES_DIR, f\"comparison_{TARGET_CONCEPT}_flux.csv\")\n",
    "df_flux.to_csv(comparison_path, index=False)\n",
    "sd15_path = os.path.join(TABLES_DIR, f\"comparison_sd15_baselines.csv\")\n",
    "df_sd15.to_csv(sd15_path, index=False)\n",
    "print(f\"\\nSaved FLUX comparison: {comparison_path}\")\n",
    "print(f\"Saved SD1.5 baselines: {sd15_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: RUN FULL BENCHMARK (Multiple Concepts)\n",
    "# ============================================================================\n",
    "# Controlled by RUN_FULL_BENCHMARK flag in config cell.\n",
    "# Two-phase approach per target: generate ALL images, then classify ALL.\n",
    "# Supports resume: if interrupted, re-run and existing images are skipped.\n",
    "# ============================================================================\n",
    "\n",
    "if not RUN_FULL_BENCHMARK:\n",
    "    print(\"Skipping full benchmark. Set RUN_FULL_BENCHMARK = True in config cell to run.\")\n",
    "else:\n",
    "    import time as _time\n",
    "    STYLES_TO_EVAL = STYLES  # All 10 styles from TRACE paper\n",
    "    all_results = []\n",
    "    _bench_start = _time.time()\n",
    "    \n",
    "    for style_idx, style in enumerate(STYLES_TO_EVAL):\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# [{style_idx+1}/{len(STYLES_TO_EVAL)}] EVALUATING: {style}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        # Check if results already exist (resume support)\n",
    "        if os.path.exists(RESULTS_CSV):\n",
    "            existing_df = pd.read_csv(RESULTS_CSV)\n",
    "            if style in existing_df['style'].values:\n",
    "                print(f\"  Results for {style} already in CSV, skipping.\")\n",
    "                row = existing_df[existing_df['style'] == style].iloc[0]\n",
    "                all_results.append({\n",
    "                    'target_concept': style, 'UA': row['ua']/100,\n",
    "                    'IRA': row['ira']/100, 'CRA': row['cra']/100\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        # Ensure FLUX is on GPU for vector learning\n",
    "        steerer.pipe.to(steerer.device)\n",
    "        \n",
    "        # Check for saved vectors first (resume support)\n",
    "        vpath = os.path.join(VECTOR_DIR, f\"{style}_{STEERING_MODE}_diverse_vectors.pt\")\n",
    "        if os.path.exists(vpath):\n",
    "            print(f\"  Loading saved vectors from {vpath}\")\n",
    "            vectors = steerer.load_vectors(vpath)\n",
    "        else:\n",
    "            # Use diverse prompt pairs (CASteer methodology)\n",
    "            style_pairs = make_style_prompts(style.replace('_', ' '), NUM_DIVERSE_PROMPTS)\n",
    "            vectors = steerer.learn_vectors_diverse(\n",
    "                prompt_pairs=style_pairs,\n",
    "                seed=0,\n",
    "                top_k=TOP_K_VECTORS,\n",
    "                verbose=False\n",
    "            )\n",
    "            steerer.save_vectors(vectors, vpath)\n",
    "        \n",
    "        # evaluate_unlearning handles: generate -> free FLUX -> classify -> reload FLUX\n",
    "        results = evaluator.evaluate_unlearning(\n",
    "            steerer=steerer,\n",
    "            vectors=vectors,\n",
    "            target_concept=style,\n",
    "            target_type=\"style\",\n",
    "            beta=BETA,\n",
    "            eval_seeds=EVAL_SEEDS,\n",
    "            save_images=True,\n",
    "            generate_baselines=(style_idx == 0)  # baselines only for first style\n",
    "        )\n",
    "        \n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Append per-style results to CSV incrementally\n",
    "        pd.DataFrame([{\n",
    "            \"style\": style,\n",
    "            \"ua\": results[\"UA\"] * 100,\n",
    "            \"ira\": results[\"IRA\"] * 100,\n",
    "            \"cra\": results[\"CRA\"] * 100\n",
    "        }]).to_csv(RESULTS_CSV, mode='a', header=not os.path.exists(RESULTS_CSV), index=False)\n",
    "        \n",
    "        elapsed = _time.time() - _bench_start\n",
    "        eta = elapsed / (style_idx + 1) * (len(STYLES_TO_EVAL) - style_idx - 1)\n",
    "        print(f\"  Elapsed: {elapsed/60:.1f} min | ETA: {eta/60:.1f} min\")\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Summary table\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FULL BENCHMARK SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    df_summary = pd.DataFrame([{\n",
    "        \"Concept\": r[\"target_concept\"],\n",
    "        \"UA%\": f\"{r['UA']*100:.1f}\",\n",
    "        \"IRA%\": f\"{r['IRA']*100:.1f}\",\n",
    "        \"CRA%\": f\"{r['CRA']*100:.1f}\"\n",
    "    } for r in all_results])\n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    avg_ua = np.mean([r['UA'] for r in all_results]) * 100\n",
    "    avg_ira = np.mean([r['IRA'] for r in all_results]) * 100\n",
    "    avg_cra = np.mean([r['CRA'] for r in all_results]) * 100\n",
    "    total_time = (_time.time() - _bench_start) / 60\n",
    "    print(f\"\\nAVERAGE:  UA={avg_ua:.1f}%  IRA={avg_ira:.1f}%  CRA={avg_cra:.1f}%\")\n",
    "    print(f\"Total time: {total_time:.1f} minutes\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: VISUALIZATION - BASELINE vs STEERED COMPARISON\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Side-by-side visual comparison of baseline (no steering) vs steered images.\n",
    "Uses the same sample prompts and seeds as the evaluator's baseline generation.\n",
    "Run AFTER Cell 9 (evaluation) so that both baseline and steered images exist.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING COMPARISON VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_concept_dir = os.path.join(BASELINE_DIR, TARGET_CONCEPT)\n",
    "vis_seed = EVAL_SEEDS[0]  # same seed used for baseline generation in evaluator\n",
    "\n",
    "# Use the same sample configs as evaluate_unlearning's baseline generation\n",
    "if TARGET_TYPE == \"style\":\n",
    "    vis_configs = [\n",
    "        (TARGET_CONCEPT, \"Dog\"),\n",
    "        (TARGET_CONCEPT, \"Cat\"),\n",
    "        (TARGET_CONCEPT, \"Bird\"),\n",
    "    ]\n",
    "else:\n",
    "    vis_configs = [\n",
    "        (\"Van_Gogh\", TARGET_CONCEPT),\n",
    "        (\"Cartoon\", TARGET_CONCEPT),\n",
    "        (\"Pop_Art\", TARGET_CONCEPT),\n",
    "    ]\n",
    "\n",
    "# Collect valid pairs (both baseline and steered must exist)\n",
    "pairs = []\n",
    "for style, obj in vis_configs:\n",
    "    filename = f\"{style}_{obj}_seed{vis_seed}.jpg\"\n",
    "    prompt = f\"A {obj.replace('_', ' ')} image in {style.replace('_', ' ')} style.\"\n",
    "    b_path = os.path.join(baseline_concept_dir, filename)\n",
    "    s_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    has_baseline = os.path.exists(b_path)\n",
    "    has_steered = os.path.exists(s_path)\n",
    "\n",
    "    if has_baseline and has_steered:\n",
    "        pairs.append((prompt, b_path, s_path))\n",
    "    else:\n",
    "        missing = []\n",
    "        if not has_baseline:\n",
    "            missing.append(f\"baseline ({b_path})\")\n",
    "        if not has_steered:\n",
    "            missing.append(f\"steered  ({s_path})\")\n",
    "        print(f\"  Skipping '{filename}' \u2014 missing: {', '.join(missing)}\")\n",
    "\n",
    "if len(pairs) == 0:\n",
    "    print(\"\\nNo matching baseline/steered pairs found.\")\n",
    "    print(\"Make sure you ran Cell 9 (evaluation) with generate_baselines=True first.\")\n",
    "else:\n",
    "    n = len(pairs)\n",
    "    fig, axes = plt.subplots(n, 2, figsize=(12, 5 * n))\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, (prompt, b_path, s_path) in enumerate(pairs):\n",
    "        baseline_img = Image.open(b_path).convert(\"RGB\")\n",
    "        steered_img = Image.open(s_path).convert(\"RGB\")\n",
    "\n",
    "        axes[i, 0].imshow(baseline_img)\n",
    "        axes[i, 0].set_title(f\"Baseline (no steering)\\n{prompt}\", fontsize=10)\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(steered_img)\n",
    "        axes[i, 1].set_title(f\"Steered (beta={BETA})\\n{prompt}\", fontsize=10)\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Unlearning: {TARGET_CONCEPT} ({TARGET_TYPE}) | Mode: {STEERING_MODE} | beta={BETA}\",\n",
    "        fontsize=14, fontweight=\"bold\", y=1.01\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    vis_path = os.path.join(OUTPUT_DIR, f\"comparison_{TARGET_CONCEPT}.png\")\n",
    "    plt.savefig(vis_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nSaved comparison figure: {vis_path}\")\n",
    "    print(f\"Displayed {n} baseline vs steered pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Results\n",
    "- **UA (Unlearning Accuracy)**: Measures how well the target concept is removed\n",
    "- **IRA (In-Domain Retain Accuracy)**: Measures preservation of related concepts\n",
    "- **CRA (Cross-Domain Retain Accuracy)**: Measures preservation of unrelated concepts\n",
    "\n",
    "### Comparison with Traditional Unlearning Methods\n",
    "Our steering vectors approach is inference-time and does NOT require:\n",
    "- Model retraining\n",
    "- Access to training data\n",
    "- Gradient computation\n",
    "\n",
    "This makes it significantly more efficient than methods like ESD, SalUn, etc.\n",
    "\n",
    "### Notes for Publication\n",
    "1. Use the same prompt format as UnlearnCanvas: `\"A painting of {object} in {style} style\"`\n",
    "2. Report metrics averaged over multiple concepts for robustness\n",
    "3. Consider using the official UnlearnCanvas classifiers for exact comparison"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}