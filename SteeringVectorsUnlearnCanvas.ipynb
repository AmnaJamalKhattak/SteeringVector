{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering Vectors for FLUX - UnlearnCanvas Benchmark Evaluation\n",
    "\n",
    "This notebook implements steering vectors for concept removal in FLUX and evaluates them using the **UnlearnCanvas benchmark** protocol.\n",
    "\n",
    "## Key Metrics (from UnlearnCanvas paper):\n",
    "- **UA (Unlearning Accuracy)**: Proportion of images NOT classified as target concept (higher = better unlearning)\n",
    "- **IRA (In-domain Retain Accuracy)**: Classification accuracy for other concepts in same domain (higher = better retention)\n",
    "- **CRA (Cross-domain Retain Accuracy)**: Classification accuracy for concepts in different domain (higher = better retention)\n",
    "- **FID**: Image quality metric (lower = better)\n",
    "- **CLIP Score**: Text-image alignment (higher = better)\n",
    "\n",
    "## Classification Method:\n",
    "This notebook uses **LLaVA-1.6-Vicuna-7B** as the classifier, following the methodology from the **TRACE paper** (ICLR 2026).\n",
    "\n",
    "The TRACE paper shows that UnlearnCanvas's original SD1.5-trained classifier generalizes poorly to modern models like FLUX (<6% accuracy). LLaVA provides accurate zero-shot classification using a numbered-list prompt format (see Appendix E.4, Figures 6-7 of TRACE paper).\n",
    "\n",
    "## Evaluation Approach:\n",
    "We generate images ourselves using FLUX + steering vectors, then evaluate using UnlearnCanvas protocol with LLaVA classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: INSTALLATIONS\n",
    "# ============================================================================\n",
    "\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install diffusers transformers accelerate --quiet\n",
    "!pip install clean-fid --quiet\n",
    "!pip install git+https://github.com/openai/CLIP.git --quiet\n",
    "!pip install timm --quiet\n",
    "!pip install pandas matplotlib pillow tqdm --quiet\n",
    "\n",
    "print(\"\u2713 All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from diffusers import FluxPipeline\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import contextmanager\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import gc\n",
    "from cleanfid import fid\n",
    "import clip\n",
    "from torchvision import transforms\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# GOOGLE DRIVE SETUP (Optional - for Colab)\n",
    "# ============================================================================\n",
    "USE_GOOGLE_DRIVE = True\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/UnlearnCanvas_Steering\"\n",
    "\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        os.makedirs(DRIVE_PATH, exist_ok=True)\n",
    "        ROOT_DIR = DRIVE_PATH\n",
    "        print(f\"\u2713 Google Drive mounted at: {ROOT_DIR}\")\n",
    "    except:\n",
    "        print(\"\u26a0 Not in Colab or Drive mounting failed. Using local storage.\")\n",
    "        ROOT_DIR = \".\"\n",
    "else:\n",
    "    ROOT_DIR = \".\"\n",
    "\n",
    "# ============================================================================\n",
    "# UNLEARNCANVAS BENCHMARK CONFIGURATION\n",
    "# Following the official UnlearnCanvas dataset structure:\n",
    "# - 60 styles (we use subset of 10 for efficiency)\n",
    "# - 20 object classes\n",
    "# ============================================================================\n",
    "\n",
    "# Full 60 styles from UnlearnCanvas (subset used for experiments)\n",
    "ALL_STYLES = [\n",
    "    \"Abstractionism\", \"Art_Brut\", \"Art_Deco\", \"Art_Informel\", \"Art_Nouveau\",\n",
    "    \"Baroque\", \"Biedermeier\", \"Byzantine\", \"Cartoon\", \"Classicism\",\n",
    "    \"Color_Field_Painting\", \"Constructivism\", \"Crayon\", \"Cubism\", \"Dadaism\",\n",
    "    \"Divisionism\", \"Early_Renaissance\", \"Expressionism\", \"Fauvism\", \"Graffiti\",\n",
    "    \"High_Renaissance\", \"Impressionism\", \"International_Gothic\", \"Japonism\", \"Lyrical_Abstraction\",\n",
    "    \"Magic_Realism\", \"Mannerism\", \"Minimalism\", \"Naive_Art\", \"Neo-Baroque\",\n",
    "    \"Neo-Expressionism\", \"Neo-Impressionism\", \"Neo-Romanticism\", \"Neoclassicism\", \"Northern_Renaissance\",\n",
    "    \"Orphism\", \"Photo\", \"Pop_Art\", \"Post-Impressionism\", \"Post-Minimalism\",\n",
    "    \"Precision\", \"Primitivism\", \"Realism\", \"Rococo\", \"Romanesque\",\n",
    "    \"Romanticism\", \"Sketch\", \"Social_Realism\", \"Spatialism\", \"Suprematism\",\n",
    "    \"Surrealism\", \"Symbolism\", \"Synthetism\", \"Tachisme\", \"Ukiyoe\",\n",
    "    \"Van_Gogh\", \"Warm_Love\", \"Watercolor\", \"Winter\", \"Bricks\"\n",
    "]\n",
    "\n",
    "# 10 styles from TRACE paper (ICLR 2026) for FLUX evaluation\n",
    "# Reference: TRACE Section 5.1 - main eval on Flux/SD3.5/Infinity\n",
    "# NOTE: TRACE Figure 6 (LLaVA prompt) shows 'Picasso' instead of 'Watercolor'\n",
    "# but Section 5.1 explicitly lists 'Watercolor' for the main FLUX evaluation.\n",
    "# 'Watercolor' is in the original 60 UnlearnCanvas styles; 'Picasso' is NOT.\n",
    "STYLES = [\n",
    "    \"Van_Gogh\", \"Watercolor\", \"Cartoon\", \"Cubism\", \"Winter\",\n",
    "    \"Pop_Art\", \"Ukiyoe\", \"Impressionism\", \"Byzantine\", \"Bricks\"\n",
    "]\n",
    "\n",
    "# All 20 object classes from UnlearnCanvas/TRACE paper (Figure 7)\n",
    "# Using singular form to match TRACE paper prompts exactly\n",
    "OBJECTS = [\n",
    "    \"Architecture\", \"Bear\", \"Bird\", \"Butterfly\", \"Cat\", \"Dog\",\n",
    "    \"Fish\", \"Flame\", \"Flowers\", \"Frog\", \"Horse\", \"Human\",\n",
    "    \"Jellyfish\", \"Rabbits\", \"Sandwich\", \"Sea\", \"Statue\",\n",
    "    \"Tower\", \"Tree\", \"Waterfalls\"\n",
    "]\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\u2713 Using device: {DEVICE}\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = \"black-forest-labs/FLUX.1-schnell\"\n",
    "N_STEPS = 4 if \"schnell\" in MODEL_ID.lower() else 28\n",
    "\n",
    "# Steering vector configuration\n",
    "LEARNING_SEEDS = list(range(0, 20))  # 20 seeds for learning vectors\n",
    "EVAL_SEEDS = list(range(20, 23))     # 3 seeds for eval (increase for paper)\n",
    "GLOBAL_BETA = 2.0                     # Steering strength (from CASteer paper)\n",
    "TOP_K_VECTORS = 15                    # Top-k steering vectors to use\n",
    "\n",
    "# ============================================================================\n",
    "# IMAGENET CLASSES FOR DIVERSE PROMPT PAIRS (from CASteer paper)\n",
    "# CASteer uses 50 ImageNet classes as base contexts for computing steering\n",
    "# vectors. This ensures the contrastive vector isolates the TARGET concept\n",
    "# rather than prompt-specific features. Critical for object unlearning.\n",
    "# ============================================================================\n",
    "IMAGENET_CLASSES = [\n",
    "    \"tench\", \"goldfish\", \"tiger shark\", \"hammerhead\", \"electric ray\",\n",
    "    \"hen\", \"ostrich\", \"brambling\", \"goldfinch\", \"house finch\",\n",
    "    \"junco\", \"indigo bunting\", \"robin\", \"bulbul\", \"jay\",\n",
    "    \"magpie\", \"chickadee\", \"water ouzel\", \"kite\", \"bald eagle\",\n",
    "    \"vulture\", \"great grey owl\", \"mud turtle\", \"box turtle\", \"banded gecko\",\n",
    "    \"common iguana\", \"whiptail lizard\", \"agama\", \"frilled lizard\", \"alligator lizard\",\n",
    "    \"green mamba\", \"thunder snake\", \"ringneck snake\", \"king snake\", \"garter snake\",\n",
    "    \"vine snake\", \"trilobite\", \"scorpion\", \"black widow\", \"tarantula\",\n",
    "    \"centipede\", \"grouse\", \"peacock\", \"quail\", \"partridge\",\n",
    "    \"macaw\", \"lorikeet\", \"coucal\", \"bee eater\", \"hornbill\"\n",
    "]\n",
    "\n",
    "def make_object_prompts(concept, num_prompts=50):\n",
    "    \"\"\"\n",
    "    Generate diverse prompt pairs for OBJECT concept steering (CASteer-style).\n",
    "    \n",
    "    Uses ImageNet classes as diverse base contexts:\n",
    "      Positive: \"tench with Dog\", \"goldfish with Dog\", ...\n",
    "      Negative: \"tench\", \"goldfish\", ...\n",
    "    \n",
    "    Averaging across many contexts ensures the contrastive vector isolates\n",
    "    the target object, not prompt-specific noise (layout, composition, etc.).\n",
    "    \n",
    "    Args:\n",
    "        concept: Object name (e.g., \"Dog\", \"Cat\")\n",
    "        num_prompts: Number of diverse prompt pairs (default: 50, as in CASteer)\n",
    "    \n",
    "    Returns:\n",
    "        List of (pos_prompt, neg_prompt) tuples\n",
    "    \"\"\"\n",
    "    n = min(num_prompts, len(IMAGENET_CLASSES))\n",
    "    pairs = []\n",
    "    for cls in IMAGENET_CLASSES[:n]:\n",
    "        pairs.append((f\"{cls} with {concept}\", f\"{cls}\"))\n",
    "    return pairs\n",
    "\n",
    "def make_style_prompts(concept, num_prompts=50):\n",
    "    \"\"\"\n",
    "    Generate diverse prompt pairs for STYLE concept steering (CASteer-style).\n",
    "    \n",
    "    Uses ImageNet classes as diverse base contexts:\n",
    "      Positive: \"tench, Van Gogh style\", \"goldfish, Van Gogh style\", ...\n",
    "      Negative: \"tench\", \"goldfish\", ...\n",
    "    \n",
    "    Args:\n",
    "        concept: Style name (e.g., \"Van Gogh\", \"Cartoon\")\n",
    "        num_prompts: Number of diverse prompt pairs (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "        List of (pos_prompt, neg_prompt) tuples\n",
    "    \"\"\"\n",
    "    n = min(num_prompts, len(IMAGENET_CLASSES))\n",
    "    pairs = []\n",
    "    for cls in IMAGENET_CLASSES[:n]:\n",
    "        pairs.append((f\"{cls}, {concept} style\", f\"{cls}\"))\n",
    "    return pairs\n",
    "\n",
    "NUM_DIVERSE_PROMPTS = 50  # Number of diverse prompt pairs for learning\n",
    "\n",
    "# Set True to run full benchmark across ALL 10 styles in Cell 13.\n",
    "# WARNING: generates 10x20x3 = 600 images PER target + LLaVA classification.\n",
    "RUN_FULL_BENCHMARK = False\n",
    "\n",
    "# Directory structure\n",
    "for subdir in [\"steering_vectors\", \"results\", \"baseline_images\", \"steered_images\", \"tables\"]:\n",
    "    os.makedirs(os.path.join(ROOT_DIR, subdir), exist_ok=True)\n",
    "\n",
    "VECTOR_DIR = os.path.join(ROOT_DIR, \"steering_vectors\")\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, \"results\")\n",
    "BASELINE_DIR = os.path.join(ROOT_DIR, \"baseline_images\")\n",
    "STEERED_DIR = os.path.join(ROOT_DIR, \"steered_images\")\n",
    "TABLES_DIR = os.path.join(ROOT_DIR, \"tables\")\n",
    "RESULTS_CSV = os.path.join(ROOT_DIR, \"benchmark_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UNLEARNCANVAS BENCHMARK CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Inference steps: {N_STEPS}\")\n",
    "print(f\"Learning seeds: {len(LEARNING_SEEDS)}\")\n",
    "print(f\"Evaluation seeds: {len(EVAL_SEEDS)}\")\n",
    "print(f\"Styles to evaluate: {len(STYLES)}\")\n",
    "print(f\"Objects to evaluate: {len(OBJECTS)}\")\n",
    "print(f\"Steering strength (\u03b2): {GLOBAL_BETA}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: FLUXSTEERING CLASS (2 MODES: hybrid + pincer_v2)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "FluxSteering: Implements activation steering for FLUX diffusion models.\n",
    "\n",
    "Two modes, each targeting the right architectural surfaces:\n",
    "\n",
    "MODE 1: \"hybrid\" (TRACE entry points + CASteer add_k/add_q) -- STYLE UNLEARNING\n",
    "    Steers at:\n",
    "      - context_embedder: Linear(4096 -> 3072) -- projects T5 encoder output\n",
    "      - time_text_embed: MLP -- fuses timestep + CLIP pooled text\n",
    "      - add_k_proj + add_q_proj in 19 DoubleStream blocks -- text-side K/Q\n",
    "    Uses mask-aware pooling. Keeps all vectors. Best for style removal.\n",
    "\n",
    "MODE 2: \"pincer_v2\" (CLIP global + text-side K/Q interception) -- OBJECT UNLEARNING\n",
    "    Diagnostic insight: zeroing time_text_embed = noise (it IS the concept\n",
    "    source), zeroing context_embedder = no effect (T5 irrelevant for objects).\n",
    "\n",
    "    Steers at:\n",
    "      - time_text_embed (CLIP only, NO T5) -- weaken the global concept signal\n",
    "      - add_k_proj + add_q_proj in DoubleStream blocks -- surgically block the\n",
    "        concept from entering attention BEFORE it mixes into spatial structure\n",
    "\n",
    "    Key innovation: PER-COMPONENT BETA. CLIP embedding is fragile (carries all\n",
    "    conditioning), so it needs gentle beta. Attention K/Q projections are more\n",
    "    concept-specific and can take aggressive beta. Pass beta as:\n",
    "      beta={\"clip\": 3.0, \"attn\": 12.0}\n",
    "\n",
    "    Why NOT to_out[0]: to_out is the output AFTER joint attention already mixed\n",
    "    text+image. Steering there is too late -- you're trying to un-mix info\n",
    "    that's already baked in. add_k/add_q intercept BEFORE mixing.\n",
    "\"\"\"\n",
    "\n",
    "class FluxSteering:\n",
    "    \"\"\"\n",
    "    FluxSteering: inference-time concept removal in FLUX.\n",
    "\n",
    "    Modes:\n",
    "      \"hybrid\"    -- style unlearning (TRACE entry points + CASteer add_k/add_q)\n",
    "      \"pincer_v2\" -- object unlearning (CLIP + add_k/add_q, per-component beta)\n",
    "    \"\"\"\n",
    "\n",
    "    VALID_MODES = (\"hybrid\", \"pincer_v2\")\n",
    "\n",
    "    def __init__(self, pipe, device=\"cuda\", n_steps=4, mode=\"hybrid\"):\n",
    "        self.pipe = pipe\n",
    "        self.device = device\n",
    "        self.n_steps = n_steps\n",
    "        self.mode = mode\n",
    "        self._current_step = -1\n",
    "        self._handles = []\n",
    "        self._current_attention_mask = None\n",
    "\n",
    "        if mode not in self.VALID_MODES:\n",
    "            raise ValueError(\n",
    "                f\"Unknown mode '{mode}'. Choose from {self.VALID_MODES}.\"\n",
    "            )\n",
    "\n",
    "        # ==============================================================\n",
    "        # Resolve layer references\n",
    "        # ==============================================================\n",
    "\n",
    "        # --- Entry-point layers ---\n",
    "        self.target_layers = {\n",
    "            \"context_embedder\": pipe.transformer.context_embedder,\n",
    "            \"time_text_embed\": pipe.transformer.time_text_embed,\n",
    "        }\n",
    "\n",
    "        # --- DoubleStream blocks ---\n",
    "        self.double_layers = [\n",
    "            m for m in pipe.transformer.modules()\n",
    "            if m.__class__.__name__ == \"FluxTransformerBlock\"\n",
    "        ]\n",
    "        self.double_layer_idxs = list(range(len(self.double_layers)))\n",
    "\n",
    "        # text-side Key and Query projections\n",
    "        self.double_add_k = {}\n",
    "        self.double_add_q = {}\n",
    "        for li in self.double_layer_idxs:\n",
    "            attn = self.double_layers[li].attn\n",
    "            if hasattr(attn, \"add_k_proj\"):\n",
    "                self.double_add_k[li] = attn.add_k_proj\n",
    "            if hasattr(attn, \"add_q_proj\"):\n",
    "                self.double_add_q[li] = attn.add_q_proj\n",
    "\n",
    "        # --- Print summary ---\n",
    "        summary = {\n",
    "            \"hybrid\": (\n",
    "                f\"  - TRACE entry points: context_embedder + time_text_embed (2)\\n\"\n",
    "                f\"  - CASteer-adapted: add_k_proj ({len(self.double_add_k)}) + add_q_proj ({len(self.double_add_q)})\\n\"\n",
    "                f\"  - Total control points: {2 + len(self.double_add_k) + len(self.double_add_q)} + mask-aware pooling\"\n",
    "            ),\n",
    "            \"pincer_v2\": (\n",
    "                f\"  - CLIP (time_text_embed): global concept source (1 layer, gentle beta)\\n\"\n",
    "                f\"  - Text-side K/Q: add_k_proj ({len(self.double_add_k)}) + add_q_proj ({len(self.double_add_q)})\\n\"\n",
    "                f\"  - Total control points: {1 + len(self.double_add_k) + len(self.double_add_q)} + mask-aware pooling\\n\"\n",
    "                f\"  - Per-component beta: beta={{'clip': 3.0, 'attn': 12.0}}\"\n",
    "            ),\n",
    "        }\n",
    "        print(f\"FluxSteering initialized (mode={mode}):\")\n",
    "        print(summary[mode])\n",
    "\n",
    "    # ==================================================================\n",
    "    # Internal helpers\n",
    "    # ==================================================================\n",
    "    def _on_step_end(self, pipe, step, timestep, callback_kwargs):\n",
    "        self._current_step = int(step.item()) if torch.is_tensor(step) else int(step)\n",
    "        return callback_kwargs\n",
    "\n",
    "    def _clear_hooks(self):\n",
    "        for h in self._handles:\n",
    "            h.remove()\n",
    "        self._handles = []\n",
    "\n",
    "    def _get_t5_mask(self, prompt):\n",
    "        tokenizer = self.pipe.tokenizer_2\n",
    "        max_seq = getattr(self.pipe, '_max_sequence_length', 512)\n",
    "        tok_out = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_seq,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        mask = tok_out.attention_mask\n",
    "        n_real = int(mask.sum().item())\n",
    "        return mask.to(self.device), n_real\n",
    "\n",
    "    def _masked_mean(self, act, mask):\n",
    "        mask_f = mask.to(device=act.device, dtype=act.dtype)\n",
    "        mask_exp = mask_f.unsqueeze(-1)\n",
    "        weighted = (act * mask_exp).sum(dim=(0, 1))\n",
    "        count = mask_exp.sum(dim=(0, 1)).clamp(min=1.0)\n",
    "        return weighted / count\n",
    "\n",
    "    def _run_pipe_base(self, prompt, seed, steps=None):\n",
    "        steps = steps or self.n_steps\n",
    "        self._current_step = -1\n",
    "        g = torch.Generator(device=self.device).manual_seed(seed)\n",
    "        return self.pipe(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=steps,\n",
    "            generator=g,\n",
    "            callback_on_step_end=self._on_step_end\n",
    "        ).images[0]\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN VECTORS -- dispatcher\n",
    "    # ==================================================================\n",
    "    @torch.no_grad()\n",
    "    def learn_vectors(self, pos_prompt, neg_prompt, seeds, top_k=15, verbose=True):\n",
    "        \"\"\"Learn steering vectors from positive/negative prompt pairs.\"\"\"\n",
    "        if self.mode == \"hybrid\":\n",
    "            return self._learn_hybrid(pos_prompt, neg_prompt, seeds, verbose)\n",
    "        elif self.mode == \"pincer_v2\":\n",
    "            return self._learn_pincer_v2(pos_prompt, neg_prompt, seeds, verbose)\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: hybrid (TRACE entry points + CASteer add_k/add_q)\n",
    "    # ==================================================================\n",
    "    def _learn_hybrid(self, pos_prompt, neg_prompt, seeds, verbose):\n",
    "        \"\"\"\n",
    "        Hybrid: context_embedder + time_text_embed + add_k_proj + add_q_proj.\n",
    "        Mask-aware pooling. Keeps ALL vectors.\n",
    "        \"\"\"\n",
    "        pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "        neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "        if verbose:\n",
    "            print(f\"T5 mask: pos has {pos_n} real tokens, neg has {neg_n} real tokens (out of {pos_mask.shape[1]})\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def hook_fn(layer_name, sign):\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    if act.dim() == 3:\n",
    "                        mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                    else:\n",
    "                        mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                    mean_diffs[layer_name][step] += (sign * mean_act)\n",
    "                    counts[layer_name][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def _run_pass(prompt, mask, sign, desc):\n",
    "            self._current_attention_mask = mask\n",
    "            for seed in tqdm(seeds, desc=desc, disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                for name, mod in self.target_layers.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn(name, sign)))\n",
    "                for idx, mod in self.double_add_k.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn(f\"add_k_{idx}\", sign)))\n",
    "                for idx, mod in self.double_add_q.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn(f\"add_q_{idx}\", sign)))\n",
    "                self._run_pipe_base(prompt, seed)\n",
    "\n",
    "        try:\n",
    "            _run_pass(pos_prompt, pos_mask, +1, \"Hybrid learning (+)\")\n",
    "            _run_pass(neg_prompt, neg_mask, -1, \"Hybrid learning (-)\")\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        return self._build_vectors_keep_all(mean_diffs, counts, len(seeds), verbose,\n",
    "                                             title=\"Hybrid Steering Vectors (TRACE entry + CASteer add_k/add_q)\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: pincer_v2 (CLIP + add_k/add_q -- object unlearning)\n",
    "    # ==================================================================\n",
    "    def _learn_pincer_v2(self, pos_prompt, neg_prompt, seeds, verbose):\n",
    "        \"\"\"\n",
    "        Pincer v2: time_text_embed (CLIP only) + add_k_proj + add_q_proj.\n",
    "        NO context_embedder (T5 is irrelevant for object identity).\n",
    "        NO to_out (too late -- text already mixed into image).\n",
    "        Mask-aware pooling for K/Q. Keeps ALL vectors.\n",
    "        \"\"\"\n",
    "        pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "        neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "        if verbose:\n",
    "            print(f\"T5 mask: pos has {pos_n} real tokens, neg has {neg_n} real tokens (out of {pos_mask.shape[1]})\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def hook_fn(layer_name, sign):\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if 0 <= step < self.n_steps:\n",
    "                    act = output.detach().float()\n",
    "                    if act.dim() == 3:\n",
    "                        mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                    else:\n",
    "                        mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                    mean_diffs[layer_name][step] += (sign * mean_act)\n",
    "                    counts[layer_name][step] += 1\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def _run_pass(prompt, mask, sign, desc):\n",
    "            self._current_attention_mask = mask\n",
    "            for seed in tqdm(seeds, desc=desc, disable=not verbose):\n",
    "                self._clear_hooks()\n",
    "                # CLIP only -- no context_embedder\n",
    "                self._handles.append(\n",
    "                    self.target_layers[\"time_text_embed\"].register_forward_hook(\n",
    "                        hook_fn(\"time_text_embed\", sign)\n",
    "                    )\n",
    "                )\n",
    "                # Text-side K/Q\n",
    "                for idx, mod in self.double_add_k.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn(f\"add_k_{idx}\", sign)))\n",
    "                for idx, mod in self.double_add_q.items():\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn(f\"add_q_{idx}\", sign)))\n",
    "                self._run_pipe_base(prompt, seed)\n",
    "\n",
    "        try:\n",
    "            _run_pass(pos_prompt, pos_mask, +1, \"Pincer_v2 learning (+)\")\n",
    "            _run_pass(neg_prompt, neg_mask, -1, \"Pincer_v2 learning (-)\")\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        return self._build_vectors_keep_all(mean_diffs, counts, len(seeds), verbose,\n",
    "                                             title=\"Pincer_v2 Vectors (CLIP + add_k/add_q)\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # LEARN: learn_vectors_diverse (MULTI-PROMPT)\n",
    "    # ==================================================================\n",
    "    @torch.no_grad()\n",
    "    def learn_vectors_diverse(self, prompt_pairs, seed=0, top_k=15, verbose=True):\n",
    "        \"\"\"\n",
    "        Learn steering vectors from DIVERSE prompt pairs (CASteer methodology).\n",
    "\n",
    "        For each (pos_prompt, neg_prompt) pair:\n",
    "          1. Run pipeline with pos_prompt (seed=seed) -> collect activations\n",
    "          2. Run pipeline with neg_prompt (seed=seed) -> collect activations\n",
    "          3. Accumulate: diff += activation(pos) - activation(neg)\n",
    "        Final vector = mean(diffs) / ||mean(diffs)||\n",
    "\n",
    "        The diverse contexts cancel out, leaving only the concept-specific direction.\n",
    "        \"\"\"\n",
    "        n_pairs = len(prompt_pairs)\n",
    "        if verbose:\n",
    "            print(f\"Learning from {n_pairs} diverse prompt pairs (seed={seed})\")\n",
    "            print(f\"Mode: {self.mode}\")\n",
    "\n",
    "        mean_diffs = defaultdict(lambda: defaultdict(float))\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        def _get_hooks_for_mode(sign):\n",
    "            hooks = []\n",
    "\n",
    "            # Entry points\n",
    "            if self.mode == \"hybrid\":\n",
    "                # Both context_embedder + time_text_embed\n",
    "                for name, mod in self.target_layers.items():\n",
    "                    def make_entry_hook(layer_name, s):\n",
    "                        def hook(module, inputs, output):\n",
    "                            step = self._current_step + 1\n",
    "                            if 0 <= step < self.n_steps:\n",
    "                                act = output.detach().float()\n",
    "                                if layer_name == \"context_embedder\" and act.dim() == 3:\n",
    "                                    mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                                else:\n",
    "                                    mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                                mean_diffs[layer_name][step] += (s * mean_act)\n",
    "                                counts[layer_name][step] += 1\n",
    "                            return output\n",
    "                        return hook\n",
    "                    hooks.append((mod, make_entry_hook(name, sign)))\n",
    "\n",
    "            elif self.mode == \"pincer_v2\":\n",
    "                # CLIP only -- no T5\n",
    "                mod = self.target_layers[\"time_text_embed\"]\n",
    "                def make_clip_hook(s):\n",
    "                    def hook(module, inputs, output):\n",
    "                        step = self._current_step + 1\n",
    "                        if 0 <= step < self.n_steps:\n",
    "                            act = output.detach().float()\n",
    "                            mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                            mean_diffs[\"time_text_embed\"][step] += (s * mean_act)\n",
    "                            counts[\"time_text_embed\"][step] += 1\n",
    "                        return output\n",
    "                    return hook\n",
    "                hooks.append((mod, make_clip_hook(sign)))\n",
    "\n",
    "            # add_k_proj and add_q_proj (both modes)\n",
    "            for li, mod in self.double_add_k.items():\n",
    "                def make_addk_hook(layer_idx, s):\n",
    "                    def hook(module, inputs, output):\n",
    "                        step = self._current_step + 1\n",
    "                        if 0 <= step < self.n_steps:\n",
    "                            act = output.detach().float()\n",
    "                            if act.dim() == 3:\n",
    "                                mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                            else:\n",
    "                                mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                            mean_diffs[f\"add_k_{layer_idx}\"][step] += (s * mean_act)\n",
    "                            counts[f\"add_k_{layer_idx}\"][step] += 1\n",
    "                        return output\n",
    "                    return hook\n",
    "                hooks.append((mod, make_addk_hook(li, sign)))\n",
    "\n",
    "            for li, mod in self.double_add_q.items():\n",
    "                def make_addq_hook(layer_idx, s):\n",
    "                    def hook(module, inputs, output):\n",
    "                        step = self._current_step + 1\n",
    "                        if 0 <= step < self.n_steps:\n",
    "                            act = output.detach().float()\n",
    "                            if act.dim() == 3:\n",
    "                                mean_act = self._masked_mean(act, self._current_attention_mask)\n",
    "                            else:\n",
    "                                mean_act = act.mean(dim=tuple(range(act.dim() - 1)))\n",
    "                            mean_diffs[f\"add_q_{layer_idx}\"][step] += (s * mean_act)\n",
    "                            counts[f\"add_q_{layer_idx}\"][step] += 1\n",
    "                        return output\n",
    "                    return hook\n",
    "                hooks.append((mod, make_addq_hook(li, sign)))\n",
    "\n",
    "            return hooks\n",
    "\n",
    "        try:\n",
    "            for pair_idx, (pos_prompt, neg_prompt) in enumerate(\n",
    "                tqdm(prompt_pairs, desc=\"Diverse prompt pairs\", disable=not verbose)\n",
    "            ):\n",
    "                pos_mask, pos_n = self._get_t5_mask(pos_prompt)\n",
    "                neg_mask, neg_n = self._get_t5_mask(neg_prompt)\n",
    "\n",
    "                # Positive\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = pos_mask\n",
    "                for mod, hook_fn in _get_hooks_for_mode(+1):\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn))\n",
    "                self._run_pipe_base(pos_prompt, seed)\n",
    "\n",
    "                # Negative\n",
    "                self._clear_hooks()\n",
    "                self._current_attention_mask = neg_mask\n",
    "                for mod, hook_fn in _get_hooks_for_mode(-1):\n",
    "                    self._handles.append(mod.register_forward_hook(hook_fn))\n",
    "                self._run_pipe_base(neg_prompt, seed)\n",
    "\n",
    "                if verbose and (pair_idx + 1) % 10 == 0:\n",
    "                    print(f\"  Completed {pair_idx + 1}/{n_pairs} prompt pairs\")\n",
    "\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "            self._current_attention_mask = None\n",
    "\n",
    "        return self._build_vectors_keep_all(mean_diffs, counts, n_pairs, verbose,\n",
    "                                             title=f\"Diverse-Prompt Steering Vectors ({self.mode}, {n_pairs} pairs)\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # Vector builders\n",
    "    # ==================================================================\n",
    "    def _build_vectors_keep_all(self, mean_diffs, counts, n_seeds, verbose, title=\"Steering Vectors\"):\n",
    "        vectors = defaultdict(dict)\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(title)\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"{'Layer':<25} {'Step':<6} {'Strength':<12}\")\n",
    "            print(f\"{'-'*70}\")\n",
    "\n",
    "        for name in sorted(mean_diffs.keys()):\n",
    "            for step in sorted(mean_diffs[name].keys()):\n",
    "                if counts[name][step] == 0:\n",
    "                    continue\n",
    "                avg_diff = mean_diffs[name][step] / n_seeds\n",
    "                strength = float(avg_diff.norm())\n",
    "                direction = avg_diff / (avg_diff.norm() + 1e-8)\n",
    "                vectors[name][step] = direction\n",
    "                if verbose:\n",
    "                    print(f\"{name:<25} {step:<6} {strength:<12.4f}\")\n",
    "\n",
    "        if verbose:\n",
    "            total = sum(len(v) for v in vectors.values())\n",
    "            print(f\"{'-'*70}\")\n",
    "            print(f\"Total vectors: {total}\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "        return dict(vectors)\n",
    "\n",
    "    def _build_vectors_topk(self, mean_diffs, counts, n_seeds, top_k, verbose, title=\"Steering Vectors\"):\n",
    "        candidates = []\n",
    "        for li in mean_diffs:\n",
    "            for step in mean_diffs[li]:\n",
    "                if counts[li][step] == 0:\n",
    "                    continue\n",
    "                avg_diff = mean_diffs[li][step] / n_seeds\n",
    "                candidates.append((float(avg_diff.norm()), li, step, avg_diff))\n",
    "\n",
    "        candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "        top_candidates = candidates[:top_k]\n",
    "\n",
    "        vectors = defaultdict(dict)\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(title)\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"{'Rank':<6} {'Layer':<25} {'Step':<6} {'Strength':<12}\")\n",
    "            print(f\"{'-'*70}\")\n",
    "\n",
    "        for rank, (strength, li, step, diff) in enumerate(top_candidates, start=1):\n",
    "            direction = diff / (diff.norm() + 1e-8)\n",
    "            vectors[li][step] = direction\n",
    "            if verbose:\n",
    "                print(f\"{rank:<6} {str(li):<25} {step:<6} {strength:<12.4f}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{'='*70}\\n\")\n",
    "        return dict(vectors)\n",
    "\n",
    "    # ==================================================================\n",
    "    # APPLY VECTORS\n",
    "    # ==================================================================\n",
    "    @contextmanager\n",
    "    def apply_vectors(self, vectors, beta=2.0, clip_negative=True):\n",
    "        \"\"\"\n",
    "        Context manager to apply steering vectors during generation.\n",
    "\n",
    "        output' = output - beta * clamp(output . d, min=0) * d\n",
    "\n",
    "        Args:\n",
    "            vectors: dict from learn_vectors / learn_vectors_diverse\n",
    "            beta: Steering strength. Can be:\n",
    "                  - float: same beta for all layers\n",
    "                  - dict {\"clip\": float, \"attn\": float}: per-component beta\n",
    "                    \"clip\" applies to time_text_embed\n",
    "                    \"attn\" applies to add_k_proj, add_q_proj, context_embedder\n",
    "            clip_negative: if True, only remove positive projections\n",
    "        \"\"\"\n",
    "        # Parse per-component beta\n",
    "        if isinstance(beta, dict):\n",
    "            beta_clip = beta.get(\"clip\", 3.0)\n",
    "            beta_attn = beta.get(\"attn\", 12.0)\n",
    "            beta_default = beta.get(\"default\", beta_attn)\n",
    "        else:\n",
    "            beta_clip = beta\n",
    "            beta_attn = beta\n",
    "            beta_default = beta\n",
    "\n",
    "        def steer_hook(layer_vectors, layer_beta):\n",
    "            def hook(module, inputs, output):\n",
    "                step = self._current_step + 1\n",
    "                if step in layer_vectors:\n",
    "                    target_dir = layer_vectors[step].to(output.device, output.dtype)\n",
    "                    score = (output @ target_dir)\n",
    "                    if clip_negative:\n",
    "                        score = torch.clamp(score, min=0.0)\n",
    "                    update = (layer_beta * score).unsqueeze(-1) * target_dir\n",
    "                    return output - update\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        try:\n",
    "            self._clear_hooks()\n",
    "\n",
    "            for li, step_vecs in vectors.items():\n",
    "                li_str = str(li)\n",
    "\n",
    "                # Determine which beta to use based on layer name\n",
    "                if li_str == \"time_text_embed\":\n",
    "                    layer_beta = beta_clip\n",
    "                elif li_str in (\"context_embedder\",) or li_str.startswith(\"add_k_\") or li_str.startswith(\"add_q_\"):\n",
    "                    layer_beta = beta_attn\n",
    "                else:\n",
    "                    layer_beta = beta_default\n",
    "\n",
    "                # Entry-point layers\n",
    "                if li_str in (\"context_embedder\", \"time_text_embed\"):\n",
    "                    if li_str in self.target_layers:\n",
    "                        self._handles.append(\n",
    "                            self.target_layers[li_str].register_forward_hook(\n",
    "                                steer_hook(step_vecs, layer_beta)\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                # add_k_proj\n",
    "                elif li_str.startswith(\"add_k_\"):\n",
    "                    idx = int(li_str.split(\"_\")[-1])\n",
    "                    if idx in self.double_add_k:\n",
    "                        self._handles.append(\n",
    "                            self.double_add_k[idx].register_forward_hook(\n",
    "                                steer_hook(step_vecs, layer_beta)\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                # add_q_proj\n",
    "                elif li_str.startswith(\"add_q_\"):\n",
    "                    idx = int(li_str.split(\"_\")[-1])\n",
    "                    if idx in self.double_add_q:\n",
    "                        self._handles.append(\n",
    "                            self.double_add_q[idx].register_forward_hook(\n",
    "                                steer_hook(step_vecs, layer_beta)\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            yield\n",
    "        finally:\n",
    "            self._clear_hooks()\n",
    "\n",
    "    # ==================================================================\n",
    "    # GENERATE\n",
    "    # ==================================================================\n",
    "    def generate(self, prompt, seed, vectors=None, beta=2.0, clip_negative=True):\n",
    "        \"\"\"Generate image with optional steering.\n",
    "\n",
    "        Args:\n",
    "            prompt: Text prompt\n",
    "            seed: Random seed\n",
    "            vectors: Steering vectors dict\n",
    "            beta: Steering strength. Can be:\n",
    "                  - float: same beta for all layers\n",
    "                  - dict {\"clip\": 3.0, \"attn\": 12.0}: per-component beta\n",
    "                    (pincer_v2 recommended: gentle CLIP, aggressive K/Q)\n",
    "            clip_negative: If True, only remove positive projections\n",
    "        \"\"\"\n",
    "        if vectors:\n",
    "            with self.apply_vectors(vectors, beta=beta, clip_negative=clip_negative):\n",
    "                return self._run_pipe_base(prompt, seed)\n",
    "        else:\n",
    "            return self._run_pipe_base(prompt, seed)\n",
    "\n",
    "    # ==================================================================\n",
    "    # SAVE / LOAD\n",
    "    # ==================================================================\n",
    "    def save_vectors(self, vectors, filepath):\n",
    "        save_dict = {}\n",
    "        for layer_id, step_dict in vectors.items():\n",
    "            save_dict[layer_id] = {step: t.cpu() for step, t in step_dict.items()}\n",
    "        torch.save(save_dict, filepath)\n",
    "        print(f\"Saved steering vectors to: {filepath}\")\n",
    "\n",
    "    def load_vectors(self, filepath):\n",
    "        save_dict = torch.load(filepath, map_location=self.device)\n",
    "        vectors = {}\n",
    "        for layer_id, step_dict in save_dict.items():\n",
    "            vectors[layer_id] = {step: t.to(self.device) for step, t in step_dict.items()}\n",
    "        print(f\"Loaded steering vectors from: {filepath}\")\n",
    "        return vectors\n",
    "\n",
    "print(\"FluxSteering class defined!\")\n",
    "print(\"  Style unlearning:  mode='hybrid' (entry points + add_k/add_q)\")\n",
    "print(\"  Object unlearning: mode='pincer_v2' (CLIP + add_k/add_q, per-component beta)\")\n",
    "print(\"    Usage: beta={'clip': 3.0, 'attn': 12.0}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3b: VERIFY TEXT-EMBEDDING ENTRY POINTS (run after models are loaded)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Empirical verification of where text embeddings first enter the FLUX transformer.\n",
    "\n",
    "Method:\n",
    "  1. Hook EVERY named module in the transformer.\n",
    "  2. Run ONE denoising step with prompt A (same seed, same timestep schedule).\n",
    "  3. Run ONE denoising step with prompt B (same seed, same timestep schedule).\n",
    "  4. Compare outputs: modules whose output CHANGED are text-dependent.\n",
    "  5. The first such modules in forward-pass order are the entry points.\n",
    "\n",
    "This proves, via code, that context_embedder and time_text_embed are the\n",
    "only places where raw text embeddings are directly consumed.\n",
    "\"\"\"\n",
    "\n",
    "def verify_text_entry_points(pipe, device=\"cuda\", prompt_a=\"a dog in Van Gogh style\",\n",
    "                              prompt_b=\"a dog in Cartoon style\"):\n",
    "    \"\"\"\n",
    "    Empirically identify which transformer modules are text-dependent.\n",
    "    Returns a list of (module_name, output_changed: bool) in forward-pass order.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    transformer = pipe.transformer\n",
    "\n",
    "    # Storage for outputs from two runs\n",
    "    outputs_a = OrderedDict()\n",
    "    outputs_b = OrderedDict()\n",
    "\n",
    "    def make_hook(storage, name):\n",
    "        def hook(module, inputs, output):\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                storage[name] = output.detach().cpu().float()\n",
    "            elif isinstance(output, tuple) and len(output) > 0 and isinstance(output[0], torch.Tensor):\n",
    "                storage[name] = output[0].detach().cpu().float()\n",
    "        return hook\n",
    "\n",
    "    def run_one_step(prompt, storage):\n",
    "        \"\"\"Run exactly 1 denoising step with hooks on all modules.\"\"\"\n",
    "        handles = []\n",
    "        try:\n",
    "            for name, mod in transformer.named_modules():\n",
    "                if name == \"\":  # skip root\n",
    "                    continue\n",
    "                handles.append(mod.register_forward_hook(make_hook(storage, name)))\n",
    "\n",
    "            g = torch.Generator(device=device).manual_seed(42)\n",
    "            pipe(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=1,\n",
    "                generator=g,\n",
    "                output_type=\"latent\",\n",
    "            )\n",
    "        finally:\n",
    "            for h in handles:\n",
    "                h.remove()\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"VERIFYING TEXT-EMBEDDING ENTRY POINTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Prompt A: \\\"{prompt_a}\\\"\")\n",
    "    print(f\"  Prompt B: \\\"{prompt_b}\\\"\")\n",
    "    print(f\"  Same seed (42), same scheduler, 1 step each\")\n",
    "    print()\n",
    "\n",
    "    # Run both prompts\n",
    "    print(\"Running prompt A...\")\n",
    "    run_one_step(prompt_a, outputs_a)\n",
    "    print(\"Running prompt B...\")\n",
    "    run_one_step(prompt_b, outputs_b)\n",
    "\n",
    "    # Compare outputs\n",
    "    common = [n for n in outputs_a if n in outputs_b]\n",
    "    text_dependent = []\n",
    "    text_independent = []\n",
    "\n",
    "    for name in common:\n",
    "        a, b = outputs_a[name], outputs_b[name]\n",
    "        if a.shape == b.shape:\n",
    "            diff = (a - b).abs().max().item()\n",
    "            changed = diff > 1e-6\n",
    "        else:\n",
    "            changed = True\n",
    "            diff = float(\"inf\")\n",
    "\n",
    "        if changed:\n",
    "            text_dependent.append((name, diff))\n",
    "        else:\n",
    "            text_independent.append(name)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS: {len(text_dependent)} text-dependent modules (out of {len(common)} total)\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\n--- TEXT-INDEPENDENT modules (output identical for both prompts): ---\")\n",
    "    if text_independent:\n",
    "        for n in text_independent[:10]:\n",
    "            print(f\"  {n}\")\n",
    "        if len(text_independent) > 10:\n",
    "            print(f\"  ... and {len(text_independent) - 10} more\")\n",
    "    else:\n",
    "        print(\"  (none \u2014 all modules are text-dependent)\")\n",
    "\n",
    "    print(f\"\\n--- TEXT-DEPENDENT modules (output changed between prompts): ---\")\n",
    "    # Sort by max diff to highlight strongest\n",
    "    text_dependent.sort(key=lambda x: -x[1])\n",
    "    for name, diff in text_dependent:\n",
    "        marker = \"\"\n",
    "        if name in (\"context_embedder\", \"time_text_embed\"):\n",
    "            marker = \"  \u25c0 TEXT ENTRY POINT\"\n",
    "        elif name.startswith(\"context_embedder.\") or name.startswith(\"time_text_embed.\"):\n",
    "            marker = \"  (sub-module of entry point)\"\n",
    "        print(f\"  {name:<60} max_diff={diff:.6f}{marker}\")\n",
    "\n",
    "    # Identify true entry points: text-dependent modules that are NOT children\n",
    "    # of other text-dependent modules (i.e., the roots of text-dependent subtrees)\n",
    "    dep_names = set(n for n, _ in text_dependent)\n",
    "    entry_points = []\n",
    "    for name, diff in text_dependent:\n",
    "        # Check if any proper parent is also text-dependent\n",
    "        parts = name.split(\".\")\n",
    "        is_child = False\n",
    "        for i in range(1, len(parts)):\n",
    "            parent = \".\".join(parts[:i])\n",
    "            if parent in dep_names:\n",
    "                is_child = True\n",
    "                break\n",
    "        if not is_child:\n",
    "            entry_points.append(name)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ENTRY POINTS (root text-dependent modules):\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for ep in entry_points:\n",
    "        mod = dict(transformer.named_modules())[ep]\n",
    "        print(f\"  transformer.{ep}\")\n",
    "        print(f\"    type: {mod.__class__.__name__}\")\n",
    "        # Print shape info if it's a Linear layer\n",
    "        if hasattr(mod, 'in_features'):\n",
    "            print(f\"    shape: Linear({mod.in_features} \u2192 {mod.out_features})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    return entry_points, text_dependent, text_independent\n",
    "\n",
    "# NOTE: Run this AFTER Cell 6 (model loading). Uncomment and execute:\n",
    "# entry_points, dep, indep = verify_text_entry_points(pipe, device=DEVICE)\n",
    "\n",
    "print(\"\u2713 verify_text_entry_points() defined. Run it after loading models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: QUALITY METRICS (FID, CLIP Score)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Quality metrics following UnlearnCanvas evaluation protocol.\n",
    "\"\"\"\n",
    "\n",
    "class QualityMetrics:\n",
    "    \"\"\"Calculate image quality metrics for UnlearnCanvas evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        print(\"Loading quality metric models...\")\n",
    "        \n",
    "        # Load CLIP for text-image alignment\n",
    "        try:\n",
    "            self.clip_model, self.clip_preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "            self.clip_model.eval()\n",
    "            print(\"  \u2713 CLIP ViT-L/14 loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \u2717 CLIP loading failed: {e}\")\n",
    "            self.clip_model = None\n",
    "\n",
    "    def calculate_clip_score(self, images, prompts):\n",
    "        \"\"\"\n",
    "        Calculate CLIP score between images and text prompts.\n",
    "        Higher = better text-image alignment.\n",
    "        \"\"\"\n",
    "        if self.clip_model is None:\n",
    "            return None\n",
    "\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for img, prompt in zip(images, prompts):\n",
    "                image_input = self.clip_preprocess(img).unsqueeze(0).to(self.device)\n",
    "                text_input = clip.tokenize([prompt], truncate=True).to(self.device)\n",
    "                \n",
    "                image_features = self.clip_model.encode_image(image_input)\n",
    "                text_features = self.clip_model.encode_text(text_input)\n",
    "                \n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                similarity = (image_features @ text_features.T).item()\n",
    "                scores.append(similarity)\n",
    "\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def calculate_fid(self, real_path, generated_path):\n",
    "        \"\"\"\n",
    "        Calculate FID between two image directories.\n",
    "        Lower = better (generated images closer to real distribution).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            score = fid.compute_fid(\n",
    "                real_path,\n",
    "                generated_path,\n",
    "                mode=\"clean\",\n",
    "                num_workers=0,\n",
    "                batch_size=8,\n",
    "                device=torch.device(self.device)\n",
    "            )\n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0 FID calculation error: {e}\")\n",
    "            return None\n",
    "\n",
    "print(\"\u2713 QualityMetrics class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4B: LLAVA CLASSIFIER (Alternative to CLIP - More Accurate)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LLaVA-based classification for more accurate style/object recognition.\n",
    "\n",
    "Pros:\n",
    "- More nuanced understanding of artistic styles\n",
    "- Can handle ambiguous cases better\n",
    "- Similar to human judgment\n",
    "\n",
    "Cons:\n",
    "- Slower (~2-5s per image vs 0.1s for CLIP)\n",
    "- Requires more VRAM (~14GB additional)\n",
    "\n",
    "Usage:\n",
    "  Set USE_LLAVA = True in the configuration cell to use LLaVA instead of CLIP.\n",
    "\"\"\"\n",
    "\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "class LLaVAClassifier:\n",
    "    \"\"\"\n",
    "    LLaVA-based image classifier for UnlearnCanvas evaluation.\n",
    "    \n",
    "    This implementation follows the EXACT methodology from the TRACE paper\n",
    "    (Appendix E.4, Figures 6-7), which uses numbered options and expects\n",
    "    the model to respond with ONLY a number for reliable parsing.\n",
    "    \n",
    "    Reference: TRACE: Transcoder-based Concept Editing (ICLR 2026)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_id=\"llava-hf/llava-v1.6-vicuna-7b-hf\", device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self.model_id = model_id\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"Load LLaVA model (call separately to manage VRAM).\"\"\"\n",
    "        if self.model is not None:\n",
    "            return\n",
    "            \n",
    "        print(f\"Loading LLaVA: {self.model_id}...\")\n",
    "        self.processor = LlavaNextProcessor.from_pretrained(self.model_id)\n",
    "        self.model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            self.model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"\u2713 LLaVA loaded!\")\n",
    "    \n",
    "    def unload(self):\n",
    "        \"\"\"Unload model to free VRAM.\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            del self.processor\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"\u2713 LLaVA unloaded\")\n",
    "    \n",
    "    def _generate_response(self, image, prompt):\n",
    "        \"\"\"Generate response from LLaVA given image and prompt.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.load()\n",
    "        \n",
    "        # Load image if path provided, ensure RGB\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "        elif isinstance(image, Image.Image):\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        # Format for LLaVA\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        prompt_formatted = self.processor.apply_chat_template(\n",
    "            conversation, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = self.processor(images=image, text=prompt_formatted, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,  # Only need a number\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated part\n",
    "        response = self.processor.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "    \n",
    "    def _parse_number_response(self, response, max_options):\n",
    "        \"\"\"\n",
    "        Parse a number from LLaVA response.\n",
    "        Returns 0-indexed class index or None if invalid.\n",
    "        \"\"\"\n",
    "        # Try to extract a number from the response\n",
    "        import re\n",
    "        numbers = re.findall(r'\\d+', response)\n",
    "        if numbers:\n",
    "            num = int(numbers[0])\n",
    "            if 1 <= num <= max_options:\n",
    "                return num - 1  # Convert to 0-indexed\n",
    "        return None\n",
    "    \n",
    "    def classify_style(self, image, styles=None, debug=False):\n",
    "        \"\"\"\n",
    "        Classify image style using TRACE paper's exact prompt format.\n",
    "        \n",
    "        From TRACE paper Figure 6:\n",
    "        'You are an image classifier. Classify the artistic style of the given image.\n",
    "         Instruction: Choose exactly one option from the numbered list below. \n",
    "         Respond with only the number.'\n",
    "        \"\"\"\n",
    "        styles = styles or STYLES\n",
    "        \n",
    "        # Build numbered options list (TRACE paper format)\n",
    "        options_list = '\\n'.join([f\"{i+1}. {s.replace('_', ' ')}\" for i, s in enumerate(styles)])\n",
    "        \n",
    "        prompt = (\n",
    "            \"You are an image classifier. Classify the artistic style of the given image.\\n\"\n",
    "            \"Instruction: Choose exactly one option from the numbered list below. \"\n",
    "            \"Respond with only the number.\\n\"\n",
    "            f\"Options:\\n{options_list}\"\n",
    "        )\n",
    "        \n",
    "        response = self._generate_response(image, prompt)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"LLaVA style response: '{response}'\")\n",
    "        \n",
    "        idx = self._parse_number_response(response, len(styles))\n",
    "        if idx is not None:\n",
    "            return styles[idx]\n",
    "        \n",
    "        # Fallback: try to match style name in response\n",
    "        response_lower = response.lower()\n",
    "        for style in styles:\n",
    "            if style.lower().replace('_', ' ') in response_lower:\n",
    "                return style\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def classify_object(self, image, objects=None, debug=False):\n",
    "        \"\"\"\n",
    "        Classify image object using TRACE paper's exact prompt format.\n",
    "        \n",
    "        From TRACE paper Figure 7:\n",
    "        'Classify the object depicted in this image.\n",
    "         Choose exactly one option from the numbered list.\n",
    "         Respond with only the number.'\n",
    "        \"\"\"\n",
    "        objects = objects or OBJECTS\n",
    "        \n",
    "        # Build numbered options list (TRACE paper format)\n",
    "        options_list = '\\n'.join([f\"{i+1}. {o.replace('_', ' ')}\" for i, o in enumerate(objects)])\n",
    "        \n",
    "        prompt = (\n",
    "            \"Classify the object depicted in this image.\\n\"\n",
    "            \"Choose exactly one option from the numbered list.\\n\"\n",
    "            \"Respond with only the number.\\n\"\n",
    "            f\"Object categories:\\n{options_list}\"\n",
    "        )\n",
    "        \n",
    "        response = self._generate_response(image, prompt)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"LLaVA object response: '{response}'\")\n",
    "        \n",
    "        idx = self._parse_number_response(response, len(objects))\n",
    "        if idx is not None:\n",
    "            return objects[idx]\n",
    "        \n",
    "        # Fallback: try to match object name in response\n",
    "        response_lower = response.lower()\n",
    "        for obj in objects:\n",
    "            if obj.lower().replace('_', ' ') in response_lower:\n",
    "                return obj\n",
    "        \n",
    "        return None\n",
    "\n",
    "print(\"\u2713 LLaVAClassifier class defined (TRACE paper format)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: UNLEARNCANVAS EVALUATOR (Supports both CLIP and LLaVA)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "UnlearnCanvas-style evaluation with configurable classifier.\n",
    "\n",
    "Classifier Options:\n",
    "- CLIP: Fast zero-shot classification (~0.1s/image)\n",
    "- LLaVA: More accurate VLM-based classification (~2-5s/image)\n",
    "\n",
    "Set USE_LLAVA = True to use LLaVA, False for CLIP.\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================================================\n",
    "# CLASSIFIER CONFIGURATION - CHANGE THIS\n",
    "# ==========================================================================\n",
    "USE_LLAVA = True  # True = LLaVA (more accurate), False = CLIP (faster)\n",
    "\n",
    "class UnlearnCanvasEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate unlearning performance using UnlearnCanvas metrics.\n",
    "    \n",
    "    Supports both CLIP (fast) and LLaVA (accurate) classification.\n",
    "    \n",
    "    Metrics:\n",
    "    - UA (Unlearning Accuracy): 1 - accuracy on target concept\n",
    "    - IRA (In-domain Retain Accuracy): accuracy on same-domain concepts\n",
    "    - CRA (Cross-domain Retain Accuracy): accuracy on other-domain concepts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cuda\", use_llava=None):\n",
    "        self.device = device\n",
    "        self.use_llava = use_llava if use_llava is not None else USE_LLAVA\n",
    "        \n",
    "        print(f\"Initializing UnlearnCanvas Evaluator (classifier: {'LLaVA' if self.use_llava else 'CLIP'})...\")\n",
    "        \n",
    "        if self.use_llava:\n",
    "            self.llava = LLaVAClassifier(device=device)\n",
    "            print(\"  \u2192 LLaVA classifier selected (will load on first use)\")\n",
    "        else:\n",
    "            self.llava = None\n",
    "            print(\"  \u2192 CLIP classifier selected (fast mode)\")\n",
    "        \n",
    "        # Load CLIP (also needed for CLIP Score metric)\n",
    "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "        self.clip_model.eval()\n",
    "        \n",
    "        # Pre-compute text embeddings only for CLIP classifier mode\n",
    "        if not self.use_llava:\n",
    "            self._precompute_text_embeddings()\n",
    "        print(\"\u2713 UnlearnCanvas Evaluator ready!\")\n",
    "    \n",
    "    def _precompute_text_embeddings(self):\n",
    "        \"\"\"Pre-compute CLIP text embeddings for all styles and objects.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Style embeddings\n",
    "            style_texts = [f\"A painting in {s.replace('_', ' ')} style\" for s in STYLES]\n",
    "            style_tokens = clip.tokenize(style_texts).to(self.device)\n",
    "            self.style_embeddings = self.clip_model.encode_text(style_tokens)\n",
    "            self.style_embeddings = self.style_embeddings / self.style_embeddings.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Object embeddings\n",
    "            object_texts = [f\"A painting of {o.replace('_', ' ')}\" for o in OBJECTS]\n",
    "            object_tokens = clip.tokenize(object_texts).to(self.device)\n",
    "            self.object_embeddings = self.clip_model.encode_text(object_tokens)\n",
    "            self.object_embeddings = self.object_embeddings / self.object_embeddings.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    def classify_image(self, image, domain=\"style\"):\n",
    "        \"\"\"\n",
    "        Classify an image into style or object category.\n",
    "        Uses LLaVA if configured, otherwise CLIP.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            domain: \"style\" or \"object\"\n",
    "            \n",
    "        Returns:\n",
    "            Predicted class name\n",
    "        \"\"\"\n",
    "        # Use LLaVA if configured\n",
    "        if self.use_llava and self.llava is not None:\n",
    "            if domain == \"style\":\n",
    "                result = self.llava.classify_style(image)\n",
    "            else:\n",
    "                result = self.llava.classify_object(image)\n",
    "            # Return if valid, otherwise fallback to CLIP\n",
    "            if result is not None:\n",
    "                return result\n",
    "        \n",
    "        # CLIP classification (default or fallback)\n",
    "        if not hasattr(self, 'style_embeddings'):\n",
    "            self._precompute_text_embeddings()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            image_input = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
    "            image_features = self.clip_model.encode_image(image_input)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            if domain == \"style\":\n",
    "                similarities = (image_features @ self.style_embeddings.T).squeeze(0)\n",
    "                pred_idx = similarities.argmax().item()\n",
    "                return STYLES[pred_idx]\n",
    "            else:\n",
    "                similarities = (image_features @ self.object_embeddings.T).squeeze(0)\n",
    "                pred_idx = similarities.argmax().item()\n",
    "                return OBJECTS[pred_idx]\n",
    "    \n",
    "    def evaluate_unlearning(\n",
    "        self,\n",
    "        steerer,\n",
    "        vectors,\n",
    "        target_concept,\n",
    "        target_type=\"style\",\n",
    "        beta=2.0,\n",
    "        clip_negative=True,\n",
    "        eval_seeds=None,\n",
    "        save_images=True,\n",
    "        output_dir=None,\n",
    "        generate_baselines=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate unlearning performance following UnlearnCanvas protocol.\n",
    "        \n",
    "        TWO-PHASE approach to manage VRAM:\n",
    "          Phase 1: Generate ALL images with FLUX (steerer) and save to disk.\n",
    "                   Skips images that already exist (resume support).\n",
    "          Phase 2: Unload FLUX from VRAM, load LLaVA, classify all saved\n",
    "                   images from disk to compute UA, IRA, CRA.\n",
    "        \n",
    "        Full grid: ALL styles x ALL objects x ALL eval seeds.\n",
    "        \"\"\"\n",
    "        eval_seeds = eval_seeds or EVAL_SEEDS\n",
    "        output_dir = output_dir or os.path.join(STEERED_DIR, f\"{target_concept}_{steerer.mode}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Baseline dir for comparison images (without steering)\n",
    "        baseline_dir = os.path.join(BASELINE_DIR, target_concept)\n",
    "        if generate_baselines:\n",
    "            os.makedirs(baseline_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EVALUATING UNLEARNING: {target_concept} ({target_type})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Build full grid of test cases\n",
    "        test_cases = []\n",
    "        for style in STYLES:\n",
    "            for obj in OBJECTS:\n",
    "                for seed in eval_seeds:\n",
    "                    filename = f\"{style}_{obj}_seed{seed}.jpg\"\n",
    "                    prompt = f\"A {obj.replace('_', ' ')} image in {style.replace('_', ' ')} style.\"\n",
    "                    test_cases.append({\n",
    "                        \"prompt\": prompt,\n",
    "                        \"seed\": seed,\n",
    "                        \"gt_style\": style,\n",
    "                        \"gt_object\": obj,\n",
    "                        \"filename\": filename,\n",
    "                    })\n",
    "        \n",
    "        total_images = len(test_cases)\n",
    "        \n",
    "        # ==============================================================\n",
    "        # PHASE 1: Generate all images with FLUX (with resume support)\n",
    "        # ==============================================================\n",
    "        skipped = 0\n",
    "        generated = 0\n",
    "        print(f\"\\n--- PHASE 1: IMAGE GENERATION ---\")\n",
    "        print(f\"Grid: {len(STYLES)} styles x {len(OBJECTS)} objects x {len(eval_seeds)} seeds = {total_images} images\")\n",
    "        print(f\"Steering: beta={beta}\")\n",
    "        print(f\"Output: {output_dir}\")\n",
    "        \n",
    "        for i, case in enumerate(tqdm(test_cases, desc=\"Phase 1: Generating\")):\n",
    "            save_path = os.path.join(output_dir, case[\"filename\"])\n",
    "            \n",
    "            # Resume support: skip if image already exists\n",
    "            if os.path.exists(save_path):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            img = steerer.generate(\n",
    "                case[\"prompt\"],\n",
    "                case[\"seed\"],\n",
    "                vectors=vectors,\n",
    "                beta=beta,\n",
    "                clip_negative=clip_negative\n",
    "            )\n",
    "            img.save(save_path)\n",
    "            generated += 1\n",
    "        \n",
    "        print(f\"Phase 1 done: {generated} generated, {skipped} skipped (already existed)\")\n",
    "        \n",
    "        # Generate baseline comparison images (without steering)\n",
    "        if generate_baselines:\n",
    "            print(\"Generating baseline images (no steering) for comparison...\")\n",
    "            sample_configs = []\n",
    "            if target_type == \"style\":\n",
    "                sample_configs = [\n",
    "                    (target_concept, \"Dog\"), (target_concept, \"Cat\"), (target_concept, \"Bird\")\n",
    "                ]\n",
    "            else:\n",
    "                sample_configs = [\n",
    "                    (\"Van_Gogh\", target_concept), (\"Cartoon\", target_concept), (\"Pop_Art\", target_concept)\n",
    "                ]\n",
    "            for s, o in sample_configs:\n",
    "                fname = f\"{s}_{o}_seed{eval_seeds[0]}.jpg\"\n",
    "                base_path = os.path.join(baseline_dir, fname)\n",
    "                if not os.path.exists(base_path):\n",
    "                    prompt = f\"A {o.replace('_', ' ')} image in {s.replace('_', ' ')} style.\"\n",
    "                    img_base = steerer.generate(prompt, seed=eval_seeds[0], vectors=None)\n",
    "                    img_base.save(base_path)\n",
    "        \n",
    "        # ==============================================================\n",
    "        # FREE FLUX VRAM before loading LLaVA\n",
    "        # ==============================================================\n",
    "        print(\"\\nFreeing FLUX VRAM before classification phase...\")\n",
    "        steerer.pipe.to('cpu')\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"FLUX moved to CPU. VRAM freed for LLaVA.\")\n",
    "        \n",
    "        # ==============================================================\n",
    "        # PHASE 2: Classify all saved images from disk\n",
    "        # ==============================================================\n",
    "        print(f\"\\n--- PHASE 2: CLASSIFICATION ({total_images} images) ---\")\n",
    "        \n",
    "        results = {\n",
    "            \"target_correct\": 0, \"target_total\": 0,\n",
    "            \"ira_correct\": 0, \"ira_total\": 0,\n",
    "            \"cra_correct\": 0, \"cra_total\": 0,\n",
    "            \"prompts\": []\n",
    "        }\n",
    "        \n",
    "        for i, case in enumerate(tqdm(test_cases, desc=\"Phase 2: Classifying\")):\n",
    "            img_path = os.path.join(output_dir, case[\"filename\"])\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"  WARNING: Missing image {case['filename']}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Load image from disk (not keeping in RAM)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            results[\"prompts\"].append(case[\"prompt\"])\n",
    "            \n",
    "            gt_style = case[\"gt_style\"]\n",
    "            gt_object = case[\"gt_object\"]\n",
    "            \n",
    "            if target_type == \"style\":\n",
    "                pred_style = self.classify_image(img, domain=\"style\")\n",
    "                pred_object = self.classify_image(img, domain=\"object\")\n",
    "                \n",
    "                # UA: images whose prompt IS the target style\n",
    "                if gt_style == target_concept:\n",
    "                    results[\"target_total\"] += 1\n",
    "                    if pred_style == target_concept:\n",
    "                        results[\"target_correct\"] += 1\n",
    "                # IRA: other styles in same domain\n",
    "                else:\n",
    "                    results[\"ira_total\"] += 1\n",
    "                    if pred_style == gt_style:\n",
    "                        results[\"ira_correct\"] += 1\n",
    "                \n",
    "                # CRA: object accuracy across ALL images\n",
    "                results[\"cra_total\"] += 1\n",
    "                if pred_object == gt_object:\n",
    "                    results[\"cra_correct\"] += 1\n",
    "            \n",
    "            else:  # target_type == \"object\"\n",
    "                pred_object = self.classify_image(img, domain=\"object\")\n",
    "                pred_style = self.classify_image(img, domain=\"style\")\n",
    "                \n",
    "                if gt_object == target_concept:\n",
    "                    results[\"target_total\"] += 1\n",
    "                    if pred_object == target_concept:\n",
    "                        results[\"target_correct\"] += 1\n",
    "                else:\n",
    "                    results[\"ira_total\"] += 1\n",
    "                    if pred_object == gt_object:\n",
    "                        results[\"ira_correct\"] += 1\n",
    "                \n",
    "                results[\"cra_total\"] += 1\n",
    "                if pred_style == gt_style:\n",
    "                    results[\"cra_correct\"] += 1\n",
    "            \n",
    "            # Progress log every 50 images\n",
    "            if (i + 1) % 50 == 0:\n",
    "                _ua = 1.0 - (results[\"target_correct\"] / max(results[\"target_total\"], 1))\n",
    "                _ira = results[\"ira_correct\"] / max(results[\"ira_total\"], 1)\n",
    "                _cra = results[\"cra_correct\"] / max(results[\"cra_total\"], 1)\n",
    "                print(f\"  [{i+1}/{total_images}] Running UA={_ua:.1%} IRA={_ira:.1%} CRA={_cra:.1%}\")\n",
    "        \n",
    "        # ==============================================================\n",
    "        # PHASE 2 DONE: Unload LLaVA, reload FLUX back to GPU\n",
    "        # ==============================================================\n",
    "        print(\"\\nClassification done. Unloading LLaVA, reloading FLUX to GPU...\")\n",
    "        if self.use_llava and self.llava is not None:\n",
    "            self.llava.unload()\n",
    "        steerer.pipe.to(steerer.device)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"FLUX reloaded to GPU.\")\n",
    "        \n",
    "        # ==============================================================\n",
    "        # Calculate final metrics\n",
    "        # ==============================================================\n",
    "        ua = 1.0 - (results[\"target_correct\"] / max(results[\"target_total\"], 1))\n",
    "        ira = results[\"ira_correct\"] / max(results[\"ira_total\"], 1)\n",
    "        cra = results[\"cra_correct\"] / max(results[\"cra_total\"], 1)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FINAL RESULTS: {target_concept}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"UA  (Unlearning Accuracy):     {ua:.2%}  ({results['target_total'] - results['target_correct']}/{results['target_total']} not classified as target)\")\n",
    "        print(f\"IRA (In-Domain Retain):        {ira:.2%}  ({results['ira_correct']}/{results['ira_total']} correct in same domain)\")\n",
    "        print(f\"CRA (Cross-Domain Retain):     {cra:.2%}  ({results['cra_correct']}/{results['cra_total']} correct in cross domain)\")\n",
    "        print(f\"Total images:                  {total_images}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        return {\n",
    "            \"UA\": ua,\n",
    "            \"IRA\": ira,\n",
    "            \"CRA\": cra,\n",
    "            \"target_concept\": target_concept,\n",
    "            \"target_type\": target_type,\n",
    "            \"beta\": beta,\n",
    "            \"n_images\": total_images,\n",
    "            \"prompts\": results[\"prompts\"]\n",
    "        }\n",
    "\n",
    "print(\"\u2713 UnlearnCanvasEvaluator class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: LOAD MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# HuggingFace login (for gated models)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    from huggingface_hub import login\n",
    "    hf_token = userdata.get(\"hf_token\")\n",
    "    login(hf_token)\n",
    "    print(\"\u2713 HuggingFace authenticated\")\n",
    "except:\n",
    "    hf_token = None\n",
    "    print(\"\u26a0 No HuggingFace token found, some models may not load\")\n",
    "\n",
    "# Load FLUX pipeline\n",
    "print(f\"\\nLoading FLUX pipeline: {MODEL_ID}...\")\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=hf_token,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(DEVICE)\n",
    "print(\"\u2713 FLUX pipeline loaded\")\n",
    "\n",
    "# Initialize FluxSteering\n",
    "# Available modes (pick ONE):\n",
    "#   \"hybrid\"     -> entry points + add_k/add_q (STYLE unlearning)\n",
    "#   \"pincer_v2\"  -> CLIP + add_k/add_q with per-component beta (OBJECT unlearning)\n",
    "#                   Use beta={\"clip\": 3.0, \"attn\": 12.0} for independent control\n",
    "STEERING_MODE = \"pincer_v2\"  # Change to \"hybrid\" for style unlearning\n",
    "print(f\"\\nInitializing FluxSteering (mode={STEERING_MODE})...\")\n",
    "steerer = FluxSteering(pipe, device=DEVICE, n_steps=N_STEPS, mode=STEERING_MODE)\n",
    "\n",
    "# Initialize evaluators\n",
    "print(\"\\nInitializing evaluators...\")\n",
    "evaluator = UnlearnCanvasEvaluator(device=DEVICE)\n",
    "quality_metrics = QualityMetrics(device=DEVICE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 ALL MODELS LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: EXPERIMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Configure which concept to unlearn.\n",
    "Change TARGET_CONCEPT and TARGET_TYPE for different experiments.\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================================================\n",
    "# EXPERIMENT CONFIGURATION - MODIFY THESE\n",
    "# ==========================================================================\n",
    "TARGET_CONCEPT = \"Dog\"        # Concept to unlearn (e.g., \"Dog\", \"Van_Gogh\")\n",
    "TARGET_TYPE = \"object\"        # \"style\" or \"object\"\n",
    "# Steering strength:\n",
    "#   Style (hybrid mode): beta=2-5 works well (single float)\n",
    "#   Objects (pincer_v2 mode): per-component beta dict\n",
    "#     \"clip\"  -> gentle (CLIP is fragile, carries all conditioning)\n",
    "#     \"attn\"  -> aggressive (K/Q projections are concept-specific)\n",
    "if TARGET_TYPE == \"style\":\n",
    "    BETA = 2.0\n",
    "else:\n",
    "    BETA = {\"clip\": 3.0, \"attn\": 12.0}\n",
    "\n",
    "# ==========================================================================\n",
    "# Automatic configuration\n",
    "# ==========================================================================\n",
    "# Include mode in output dir to prevent cross-mode caching!\n",
    "OUTPUT_DIR = os.path.join(RESULTS_DIR, f\"{TARGET_CONCEPT}_{STEERING_MODE}\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- PROMPT CONFIGURATION ----\n",
    "# All object modes now use diverse CASteer-style prompt pairs.\n",
    "# The 50 diverse contexts cancel out, isolating the target concept direction.\n",
    "# This is CRITICAL for objects: ensures the vector captures \"Dog\" specifically,\n",
    "# not \"general content\" or \"things about this particular scene.\"\n",
    "if TARGET_TYPE == \"style\":\n",
    "    DIVERSE_PROMPT_PAIRS = make_style_prompts(TARGET_CONCEPT.replace('_', ' '), NUM_DIVERSE_PROMPTS)\n",
    "    pos_prompt = f\"{TARGET_CONCEPT.replace('_', ' ')} style\"\n",
    "    neg_prompt = \"neutral style\"\n",
    "else:\n",
    "    DIVERSE_PROMPT_PAIRS = make_object_prompts(TARGET_CONCEPT.replace('_', ' '), NUM_DIVERSE_PROMPTS)\n",
    "    pos_prompt = f\"{TARGET_CONCEPT.replace('_', ' ')}\"\n",
    "    neg_prompt = \"Object\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target Concept:    {TARGET_CONCEPT}\")\n",
    "print(f\"Target Type:       {TARGET_TYPE}\")\n",
    "print(f\"Steering Mode:     {STEERING_MODE}\")\n",
    "print(f\"Steering Strength: \\u03b2 = {BETA}\")\n",
    "if TARGET_TYPE == \"object\":\n",
    "    print(f\"Prompt Strategy:   {len(DIVERSE_PROMPT_PAIRS)} diverse pairs (CASteer methodology)\")\n",
    "    print(f\"  Example pos:     '{DIVERSE_PROMPT_PAIRS[0][0]}'\")\n",
    "    print(f\"  Example neg:     '{DIVERSE_PROMPT_PAIRS[0][1]}'\")\n",
    "    print(f\"  Rationale:       Averaging across 50 contexts cancels non-Dog info,\")\n",
    "    print(f\"                   isolating the 'Dog' direction so ONLY the dog is removed.\")\n",
    "else:\n",
    "    print(f\"Prompt Strategy:   {len(DIVERSE_PROMPT_PAIRS)} diverse pairs\")\n",
    "    print(f\"  Example pos:     '{DIVERSE_PROMPT_PAIRS[0][0]}'\")\n",
    "    print(f\"  Example neg:     '{DIVERSE_PROMPT_PAIRS[0][1]}'\")\n",
    "print(f\"Eval Seeds:        {len(EVAL_SEEDS)}\")\n",
    "print(f\"Output Directory:  {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# CELL 8: LEARN STEERING VECTORS\n# ============================================================================\n\nprint(f\"Learning steering vectors for: {TARGET_CONCEPT}\")\nprint(f\"Mode: {STEERING_MODE}\")\n\n# ===========================================================================\n# ALL modes now use diverse prompt pairs for object unlearning.\n# This is the CASteer methodology: 50 diverse contexts (\"tench with Dog\" vs\n# \"tench\", \"goldfish with Dog\" vs \"goldfish\", etc.) with the SAME seed.\n# Averaging cancels context-specific noise, isolating the target concept.\n# ===========================================================================\nif TARGET_TYPE == \"object\":\n    print(f\"Using {len(DIVERSE_PROMPT_PAIRS)} diverse prompt pairs (CASteer methodology)\")\n    print(f\"  Example: '{DIVERSE_PROMPT_PAIRS[0][0]}' vs '{DIVERSE_PROMPT_PAIRS[0][1]}'\\n\")\n    vectors = steerer.learn_vectors_diverse(\n        prompt_pairs=DIVERSE_PROMPT_PAIRS,\n        seed=0,\n        top_k=TOP_K_VECTORS,\n        verbose=True\n    )\n    vector_path = os.path.join(VECTOR_DIR, f\"{TARGET_CONCEPT}_{STEERING_MODE}_diverse_vectors.pt\")\nelse:\n    # Style unlearning: diverse prompts also work well\n    print(f\"Using {len(DIVERSE_PROMPT_PAIRS)} diverse prompt pairs\\n\")\n    vectors = steerer.learn_vectors_diverse(\n        prompt_pairs=DIVERSE_PROMPT_PAIRS,\n        seed=0,\n        top_k=TOP_K_VECTORS,\n        verbose=True\n    )\n    vector_path = os.path.join(VECTOR_DIR, f\"{TARGET_CONCEPT}_{STEERING_MODE}_diverse_vectors.pt\")\n\n# Save vectors for reproducibility\nsteerer.save_vectors(vectors, vector_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8B: QUICK STEERING TEST \u2014 Run BEFORE full evaluation!\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Tests steering with multiple beta values + clip_negative settings.\n",
    "Generates 6 images total \u2014 takes ~30 seconds. Shows immediate visual results.\n",
    "\n",
    "KEY: If ALL images look identical to baseline, there's a fundamental issue.\n",
    "     If higher beta or clip_negative=False produces different images, we have\n",
    "     the right settings. Then run the full evaluation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "DIAG_PROMPT = f\"A {TARGET_CONCEPT.replace('_', ' ')} image in Van Gogh style.\"\n",
    "DIAG_SEED = 42\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"QUICK STEERING TEST: {TARGET_CONCEPT} ({STEERING_MODE})\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: '{DIAG_PROMPT}'\")\n",
    "print(f\"Vectors: {len(vectors)} layers, \"\n",
    "      f\"{sum(len(v) for v in vectors.values())} (layer,step) pairs\\n\")\n",
    "\n",
    "# --- Test 0a: Zero context_embedder (T5) - should have minimal effect ---\n",
    "print(\"TEST 0a: Zero context_embedder (T5) - expect minimal change...\")\n",
    "_handle_t5 = steerer.target_layers[\"context_embedder\"].register_forward_hook(\n",
    "    lambda m, i, o: o * 0\n",
    ")\n",
    "t5_zeroed_img = steerer._run_pipe_base(DIAG_PROMPT, DIAG_SEED)\n",
    "_handle_t5.remove()\n",
    "\n",
    "# --- Test 0b: Zero time_text_embed (CLIP) - should produce noise ---\n",
    "print(\"TEST 0b: Zero time_text_embed (CLIP) - expect pure noise...\")\n",
    "_handle_clip = steerer.target_layers[\"time_text_embed\"].register_forward_hook(\n",
    "    lambda m, i, o: o * 0\n",
    ")\n",
    "clip_zeroed_img = steerer._run_pipe_base(DIAG_PROMPT, DIAG_SEED)\n",
    "_handle_clip.remove()\n",
    "\n",
    "# --- Generate baseline ---\n",
    "print(\"\\nGenerating baseline (no steering)...\")\n",
    "baseline_img = steerer.generate(DIAG_PROMPT, DIAG_SEED, vectors=None)\n",
    "\n",
    "# --- Test configurations ---\n",
    "# For pincer_v2: test per-component beta dicts\n",
    "# For hybrid: test scalar betas\n",
    "if STEERING_MODE == \"pincer_v2\":\n",
    "    configs = [\n",
    "        (\"clip=2,attn=8\",   {\"clip\": 2.0, \"attn\": 8.0},   True),\n",
    "        (\"clip=3,attn=12\",  {\"clip\": 3.0, \"attn\": 12.0},  True),\n",
    "        (\"clip=5,attn=15\",  {\"clip\": 5.0, \"attn\": 15.0},  True),\n",
    "        (\"clip=3,attn=20\",  {\"clip\": 3.0, \"attn\": 20.0},  True),\n",
    "        (\"clip=5,attn=25\",  {\"clip\": 5.0, \"attn\": 25.0},  True),\n",
    "    ]\n",
    "else:\n",
    "    configs = [\n",
    "        (\"\u03b2=2, clip_neg=True\",   2.0,  True),\n",
    "        (\"\u03b2=5, clip_neg=True\",   5.0,  True),\n",
    "        (\"\u03b2=10, clip_neg=True\",  10.0, True),\n",
    "        (\"\u03b2=5, clip_neg=False\",  5.0,  False),\n",
    "        (\"\u03b2=10, clip_neg=False\", 10.0, False),\n",
    "    ]\n",
    "\n",
    "test_images = []\n",
    "for label, beta_val, clip_val in configs:\n",
    "    print(f\"  Generating: {label}...\")\n",
    "    img = steerer.generate(DIAG_PROMPT, DIAG_SEED, vectors=vectors,\n",
    "                           beta=beta_val, clip_negative=clip_val)\n",
    "    test_images.append((label, img))\n",
    "\n",
    "# --- Visual comparison ---\n",
    "n_imgs = 3 + len(test_images)  # t5_zero + clip_zero + baseline + tests\n",
    "fig, axes = plt.subplots(2, 4, figsize=(28, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# T5 zeroed test\n",
    "axes[0].imshow(t5_zeroed_img)\n",
    "axes[0].set_title(\"TEST 0a: T5=0\\n(expect minimal change)\", fontsize=10, color='blue')\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# CLIP zeroed test\n",
    "axes[1].imshow(clip_zeroed_img)\n",
    "axes[1].set_title(\"TEST 0b: CLIP=0\\n(expect pure noise)\", fontsize=10, color='red')\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Baseline\n",
    "axes[2].imshow(baseline_img)\n",
    "axes[2].set_title(\"Baseline (no steering)\", fontsize=10)\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "# Test images\n",
    "baseline_arr = np.array(baseline_img).astype(float)\n",
    "for i, (label, img) in enumerate(test_images):\n",
    "    ax = axes[i + 3]\n",
    "    ax.imshow(img)\n",
    "    # Compute pixel difference\n",
    "    diff = np.abs(np.array(img).astype(float) - baseline_arr)\n",
    "    mean_diff = diff.mean()\n",
    "    max_diff = diff.max()\n",
    "    pct_changed = (diff > 1.0).mean() * 100\n",
    "    ax.set_title(f\"{label}\\ndiff: mean={mean_diff:.1f}, {pct_changed:.0f}% changed\", fontsize=10,\n",
    "                 color='green' if pct_changed > 10 else 'orange' if pct_changed > 1 else 'red')\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Hide unused axes\n",
    "for i in range(n_imgs, len(axes)):\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Object Steering Test: {TARGET_CONCEPT} ({STEERING_MODE})\\n\"\n",
    "             f\"Vectors: {sum(len(v) for v in vectors.values())} total\",\n",
    "             fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check CLIP zeroing test (should be dramatically different)\n",
    "clip_diff = np.abs(np.array(clip_zeroed_img).astype(float) - baseline_arr).mean()\n",
    "if clip_diff > 10:\n",
    "    print(\"\u2713 TEST 0 PASSED: Hooks DO fire (destructive test produced different image)\")\n",
    "else:\n",
    "    print(\"\u2717 TEST 0 FAILED: Hooks DON'T fire! context_embedder\u00d70 had no effect!\")\n",
    "    print(\"  \u2192 This means PyTorch hooks don't work with this FLUX pipeline.\")\n",
    "    print(\"  \u2192 Try: steerer.pipe.transformer = torch.compile(steerer.pipe.transformer, mode='reduce-overhead')\")\n",
    "    print(\"  \u2192 Or: monkey-patch the forward method directly instead of using hooks.\")\n",
    "\n",
    "print()\n",
    "for label, img in test_images:\n",
    "    diff = np.abs(np.array(img).astype(float) - baseline_arr)\n",
    "    mean_diff = diff.mean()\n",
    "    pct = (diff > 1.0).mean() * 100\n",
    "    status = \"\u2713 WORKING\" if pct > 10 else \"\u26a0 WEAK\" if pct > 1 else \"\u2717 NO EFFECT\"\n",
    "    print(f\"  {status}: {label:<25} mean_diff={mean_diff:>6.1f}  pixels_changed={pct:>5.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"NEXT STEPS:\")\n",
    "best_config = None\n",
    "for label, img in test_images:\n",
    "    diff = np.abs(np.array(img).astype(float) - baseline_arr)\n",
    "    pct = (diff > 1.0).mean() * 100\n",
    "    if pct > 10:\n",
    "        best_config = label\n",
    "        break\n",
    "if best_config:\n",
    "    print(f\"  \u2192 Best config: {best_config}\")\n",
    "    print(f\"  \u2192 Update BETA and CLIP_NEGATIVE in Cell 7, then run full evaluation\")\n",
    "else:\n",
    "    print(\"  \u2192 No configuration produced visible steering effect\")\n",
    "    print(\"  \u2192 If TEST 0 passed: the learned DIRECTIONS don't capture 'Dog'\")\n",
    "    print(\"     at the evaluation prompt format. Try different prompt templates.\")\n",
    "    print(\"  \u2192 If TEST 0 failed: hooks don't work, need different approach.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: UNLEARNCANVAS EVALUATION (UA, IRA, CRA)\n",
    "# ============================================================================\n",
    "# TWO-PHASE EVALUATION:\n",
    "#   Phase 1: Generate ALL images with FLUX (skips existing = resume support)\n",
    "#   Phase 2: Unload FLUX, load LLaVA, classify all images from disk\n",
    "# Total images = len(STYLES) * len(OBJECTS) * len(EVAL_SEEDS)\n",
    "# e.g., 10 styles x 20 objects x 3 seeds = 600 images\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Running UnlearnCanvas evaluation (two-phase: generate then classify)...\")\n",
    "print(f\"This will generate {len(STYLES) * len(OBJECTS) * len(EVAL_SEEDS)} images.\\n\")\n",
    "\n",
    "# Evaluate unlearning on full grid\n",
    "# clip_negative=False for objects: allows steering even if dot products are negative\n",
    "# (learned direction from \"tench with Dog\" may be flipped relative to \"A Dog image in...\")\n",
    "CLIP_NEGATIVE = True if TARGET_TYPE == \"style\" else False\n",
    "\n",
    "eval_results = evaluator.evaluate_unlearning(\n",
    "    steerer=steerer,\n",
    "    vectors=vectors,\n",
    "    target_concept=TARGET_CONCEPT,\n",
    "    target_type=TARGET_TYPE,\n",
    "    beta=BETA,\n",
    "    clip_negative=CLIP_NEGATIVE,\n",
    "    eval_seeds=EVAL_SEEDS,\n",
    "    save_images=True,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    generate_baselines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: QUALITY METRICS (FID, CLIP Score)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CALCULATING QUALITY METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "quality_results = {}\n",
    "\n",
    "# Load generated images from disk for quality metrics\n",
    "print(\"Loading generated images from disk for quality metrics...\")\n",
    "_gen_images = []\n",
    "_gen_prompts = []\n",
    "for fname in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if fname.endswith(\".jpg\") or fname.endswith(\".png\"):\n",
    "        _gen_images.append(Image.open(os.path.join(OUTPUT_DIR, fname)).convert(\"RGB\"))\n",
    "        # Reconstruct prompt from filename: Style_Object_seedN.jpg\n",
    "        parts = fname.rsplit(\"_seed\", 1)[0]  # \"Style_Object\"\n",
    "        style_obj = parts.split(\"_\", 1) if \"_\" in parts else [parts, \"\"]\n",
    "        _gen_prompts.append(f\"A {style_obj[-1].replace('_', ' ')} image in {style_obj[0].replace('_', ' ')} style.\")\n",
    "print(f\"Loaded {len(_gen_images)} images from {OUTPUT_DIR}\")\n",
    "\n",
    "# CLIP Score\n",
    "print(\"\\n1. CLIP Score (text-image alignment)...\")\n",
    "clip_score = quality_metrics.calculate_clip_score(\n",
    "    _gen_images,\n",
    "    eval_results[\"prompts\"] if eval_results[\"prompts\"] else _gen_prompts\n",
    ")\n",
    "quality_results[\"CLIP_Score\"] = clip_score\n",
    "if clip_score:\n",
    "    print(f\"   \u2713 CLIP Score: {clip_score:.4f}\")\n",
    "\n",
    "# FID (requires baseline images)\n",
    "print(\"\\n2. FID Score (image quality)...\")\n",
    "baseline_path = os.path.join(BASELINE_DIR, TARGET_CONCEPT)\n",
    "if os.path.exists(baseline_path) and len(os.listdir(baseline_path)) > 0:\n",
    "    fid_score = quality_metrics.calculate_fid(baseline_path, OUTPUT_DIR)\n",
    "    quality_results[\"FID\"] = fid_score\n",
    "    if fid_score:\n",
    "        print(f\"   \u2713 FID: {fid_score:.2f}\")\n",
    "else:\n",
    "    print(f\"   \u26a0 Baseline images not found at {baseline_path}\")\n",
    "    print(\"   \u2192 Generate baseline images first (without steering)\")\n",
    "    quality_results[\"FID\"] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: COMPILE AND SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "# Compile all results\n",
    "final_results = {\n",
    "    \"Target_Concept\": TARGET_CONCEPT,\n",
    "    \"Target_Type\": TARGET_TYPE,\n",
    "    \"Beta\": BETA,\n",
    "    \"UA\": eval_results[\"UA\"],\n",
    "    \"IRA\": eval_results[\"IRA\"],\n",
    "    \"CRA\": eval_results[\"CRA\"],\n",
    "    \"CLIP_Score\": quality_results.get(\"CLIP_Score\"),\n",
    "    \"FID\": quality_results.get(\"FID\"),\n",
    "    \"n_images\": eval_results[\"n_images\"],\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save to CSV (append mode)\n",
    "df_new = pd.DataFrame([final_results])\n",
    "if os.path.exists(RESULTS_CSV):\n",
    "    df_existing = pd.read_csv(RESULTS_CSV)\n",
    "    df_all = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "else:\n",
    "    df_all = df_new\n",
    "df_all.to_csv(RESULTS_CSV, index=False)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTarget: {TARGET_CONCEPT} ({TARGET_TYPE})\")\n",
    "print(f\"Steering \u03b2: {BETA}\")\n",
    "print(f\"\\n--- UnlearnCanvas Metrics ---\")\n",
    "print(f\"UA  (Unlearning Accuracy):     {eval_results['UA']:.2%}\")\n",
    "print(f\"IRA (In-Domain Retain):        {eval_results['IRA']:.2%}\")\n",
    "print(f\"CRA (Cross-Domain Retain):     {eval_results['CRA']:.2%}\")\n",
    "print(f\"\\n--- Quality Metrics ---\")\n",
    "if quality_results.get(\"CLIP_Score\"):\n",
    "    print(f\"CLIP Score:                    {quality_results['CLIP_Score']:.4f}\")\n",
    "if quality_results.get(\"FID\"):\n",
    "    print(f\"FID:                           {quality_results['FID']:.2f}\")\n",
    "print(f\"\\n--- Files ---\")\n",
    "print(f\"Results CSV: {RESULTS_CSV}\")\n",
    "print(f\"Vectors:     {vector_path}\")\n",
    "print(f\"Images:      {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: GENERATE COMPARISON TABLE (vs Baselines)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Compare our results against baselines from the TRACE paper (ICLR 2026).\n",
    "- Table 1 (FLUX baselines): LOCOEDIT, UCE, TRACE - most relevant comparison\n",
    "- Table 2 (SD1.5 baselines): broader context from UnlearnCanvas benchmark\n",
    "\"\"\"\n",
    "\n",
    "# --- Table A: FLUX baselines from TRACE Table 1 (most relevant) ---\n",
    "flux_baselines = {\n",
    "    \"Method\": [\"LOCOEDIT (Flux)\", \"UCE (Flux)\", \"TRACE (Flux)\", \"Ours (Steering)\"],\n",
    "    \"UA\": [66.45, 67.43, 88.60, eval_results[\"UA\"]*100],\n",
    "    \"IRA\": [33.23, 34.78, 36.10, eval_results[\"IRA\"]*100],\n",
    "    \"CRA\": [83.44, 76.56, 96.40, eval_results[\"CRA\"]*100],\n",
    "    \"FID\": [55.56, 58.90, 51.67, quality_results.get(\"FID\", \"N/A\")]\n",
    "}\n",
    "\n",
    "df_flux = pd.DataFrame(flux_baselines)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON WITH FLUX BASELINES (TRACE Table 1 - Style Removal)\")\n",
    "print(\"=\"*70)\n",
    "print(df_flux.to_string(index=False))\n",
    "print(\"\\nSource: TRACE paper (ICLR 2026), Table 1\")\n",
    "print(\"Higher UA = better unlearning, Higher IRA/CRA = better retention\")\n",
    "print(\"Lower FID = better image quality\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Table B: SD1.5 baselines from TRACE Table 2 (broader context) ---\n",
    "sd15_baselines = {\n",
    "    \"Method\": [\"ESD\", \"FMN\", \"UCE\", \"CA\", \"SalUn\", \"SEOT\", \"SPM\", \"EDiff\", \"SHS\", \"SAeUron\", \"TRACE\"],\n",
    "    \"UA\": [98.58, 88.48, 98.40, 60.82, 86.26, 56.90, 60.94, 92.42, 95.84, 95.80, 95.02],\n",
    "    \"IRA\": [80.97, 56.77, 60.22, 96.01, 90.39, 94.68, 92.39, 73.91, 80.42, 99.10, 93.84],\n",
    "    \"CRA\": [93.96, 46.60, 47.71, 92.70, 95.08, 84.31, 84.33, 98.93, 43.27, 99.40, 86.22],\n",
    "}\n",
    "\n",
    "df_sd15 = pd.DataFrame(sd15_baselines)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SD1.5 BASELINES FOR BROADER CONTEXT (TRACE Table 2)\")\n",
    "print(\"=\"*70)\n",
    "print(df_sd15.to_string(index=False))\n",
    "print(\"\\nNote: SD1.5 numbers are NOT directly comparable to FLUX results.\")\n",
    "print(\"They are provided for broader context only.\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save comparison tables\n",
    "comparison_path = os.path.join(TABLES_DIR, f\"comparison_{TARGET_CONCEPT}_flux.csv\")\n",
    "df_flux.to_csv(comparison_path, index=False)\n",
    "sd15_path = os.path.join(TABLES_DIR, f\"comparison_sd15_baselines.csv\")\n",
    "df_sd15.to_csv(sd15_path, index=False)\n",
    "print(f\"\\nSaved FLUX comparison: {comparison_path}\")\n",
    "print(f\"Saved SD1.5 baselines: {sd15_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: RUN FULL BENCHMARK (Multiple Concepts)\n",
    "# ============================================================================\n",
    "# Controlled by RUN_FULL_BENCHMARK flag in config cell.\n",
    "# Two-phase approach per target: generate ALL images, then classify ALL.\n",
    "# Supports resume: if interrupted, re-run and existing images are skipped.\n",
    "# ============================================================================\n",
    "\n",
    "if not RUN_FULL_BENCHMARK:\n",
    "    print(\"Skipping full benchmark. Set RUN_FULL_BENCHMARK = True in config cell to run.\")\n",
    "else:\n",
    "    import time as _time\n",
    "    STYLES_TO_EVAL = STYLES  # All 10 styles from TRACE paper\n",
    "    all_results = []\n",
    "    _bench_start = _time.time()\n",
    "    \n",
    "    for style_idx, style in enumerate(STYLES_TO_EVAL):\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# [{style_idx+1}/{len(STYLES_TO_EVAL)}] EVALUATING: {style}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        # Check if results already exist (resume support)\n",
    "        if os.path.exists(RESULTS_CSV):\n",
    "            existing_df = pd.read_csv(RESULTS_CSV)\n",
    "            if style in existing_df['style'].values:\n",
    "                print(f\"  Results for {style} already in CSV, skipping.\")\n",
    "                row = existing_df[existing_df['style'] == style].iloc[0]\n",
    "                all_results.append({\n",
    "                    'target_concept': style, 'UA': row['ua']/100,\n",
    "                    'IRA': row['ira']/100, 'CRA': row['cra']/100\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        # Ensure FLUX is on GPU for vector learning\n",
    "        steerer.pipe.to(steerer.device)\n",
    "        \n",
    "        # Check for saved vectors first (resume support)\n",
    "        vpath = os.path.join(VECTOR_DIR, f\"{style}_{STEERING_MODE}_diverse_vectors.pt\")\n",
    "        if os.path.exists(vpath):\n",
    "            print(f\"  Loading saved vectors from {vpath}\")\n",
    "            vectors = steerer.load_vectors(vpath)\n",
    "        else:\n",
    "            # Use diverse prompt pairs (CASteer methodology)\n",
    "            style_pairs = make_style_prompts(style.replace('_', ' '), NUM_DIVERSE_PROMPTS)\n",
    "            vectors = steerer.learn_vectors_diverse(\n",
    "                prompt_pairs=style_pairs,\n",
    "                seed=0,\n",
    "                top_k=TOP_K_VECTORS,\n",
    "                verbose=False\n",
    "            )\n",
    "            steerer.save_vectors(vectors, vpath)\n",
    "        \n",
    "        # evaluate_unlearning handles: generate -> free FLUX -> classify -> reload FLUX\n",
    "        results = evaluator.evaluate_unlearning(\n",
    "            steerer=steerer,\n",
    "            vectors=vectors,\n",
    "            target_concept=style,\n",
    "            target_type=\"style\",\n",
    "            beta=BETA,\n",
    "            eval_seeds=EVAL_SEEDS,\n",
    "            save_images=True,\n",
    "            generate_baselines=(style_idx == 0)  # baselines only for first style\n",
    "        )\n",
    "        \n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Append per-style results to CSV incrementally\n",
    "        pd.DataFrame([{\n",
    "            \"style\": style,\n",
    "            \"ua\": results[\"UA\"] * 100,\n",
    "            \"ira\": results[\"IRA\"] * 100,\n",
    "            \"cra\": results[\"CRA\"] * 100\n",
    "        }]).to_csv(RESULTS_CSV, mode='a', header=not os.path.exists(RESULTS_CSV), index=False)\n",
    "        \n",
    "        elapsed = _time.time() - _bench_start\n",
    "        eta = elapsed / (style_idx + 1) * (len(STYLES_TO_EVAL) - style_idx - 1)\n",
    "        print(f\"  Elapsed: {elapsed/60:.1f} min | ETA: {eta/60:.1f} min\")\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Summary table\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FULL BENCHMARK SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    df_summary = pd.DataFrame([{\n",
    "        \"Concept\": r[\"target_concept\"],\n",
    "        \"UA%\": f\"{r['UA']*100:.1f}\",\n",
    "        \"IRA%\": f\"{r['IRA']*100:.1f}\",\n",
    "        \"CRA%\": f\"{r['CRA']*100:.1f}\"\n",
    "    } for r in all_results])\n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    avg_ua = np.mean([r['UA'] for r in all_results]) * 100\n",
    "    avg_ira = np.mean([r['IRA'] for r in all_results]) * 100\n",
    "    avg_cra = np.mean([r['CRA'] for r in all_results]) * 100\n",
    "    total_time = (_time.time() - _bench_start) / 60\n",
    "    print(f\"\\nAVERAGE:  UA={avg_ua:.1f}%  IRA={avg_ira:.1f}%  CRA={avg_cra:.1f}%\")\n",
    "    print(f\"Total time: {total_time:.1f} minutes\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: VISUALIZATION - BASELINE vs STEERED COMPARISON\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Side-by-side visual comparison of baseline (no steering) vs steered images.\n",
    "Uses the same sample prompts and seeds as the evaluator's baseline generation.\n",
    "Run AFTER Cell 9 (evaluation) so that both baseline and steered images exist.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING COMPARISON VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_concept_dir = os.path.join(BASELINE_DIR, TARGET_CONCEPT)\n",
    "vis_seed = EVAL_SEEDS[0]  # same seed used for baseline generation in evaluator\n",
    "\n",
    "# Use the same sample configs as evaluate_unlearning's baseline generation\n",
    "if TARGET_TYPE == \"style\":\n",
    "    vis_configs = [\n",
    "        (TARGET_CONCEPT, \"Dog\"),\n",
    "        (TARGET_CONCEPT, \"Cat\"),\n",
    "        (TARGET_CONCEPT, \"Bird\"),\n",
    "    ]\n",
    "else:\n",
    "    vis_configs = [\n",
    "        (\"Van_Gogh\", TARGET_CONCEPT),\n",
    "        (\"Cartoon\", TARGET_CONCEPT),\n",
    "        (\"Pop_Art\", TARGET_CONCEPT),\n",
    "    ]\n",
    "\n",
    "# Collect valid pairs (both baseline and steered must exist)\n",
    "pairs = []\n",
    "for style, obj in vis_configs:\n",
    "    filename = f\"{style}_{obj}_seed{vis_seed}.jpg\"\n",
    "    prompt = f\"A {obj.replace('_', ' ')} image in {style.replace('_', ' ')} style.\"\n",
    "    b_path = os.path.join(baseline_concept_dir, filename)\n",
    "    s_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    has_baseline = os.path.exists(b_path)\n",
    "    has_steered = os.path.exists(s_path)\n",
    "\n",
    "    if has_baseline and has_steered:\n",
    "        pairs.append((prompt, b_path, s_path))\n",
    "    else:\n",
    "        missing = []\n",
    "        if not has_baseline:\n",
    "            missing.append(f\"baseline ({b_path})\")\n",
    "        if not has_steered:\n",
    "            missing.append(f\"steered  ({s_path})\")\n",
    "        print(f\"  Skipping '{filename}' \u2014 missing: {', '.join(missing)}\")\n",
    "\n",
    "if len(pairs) == 0:\n",
    "    print(\"\\nNo matching baseline/steered pairs found.\")\n",
    "    print(\"Make sure you ran Cell 9 (evaluation) with generate_baselines=True first.\")\n",
    "else:\n",
    "    n = len(pairs)\n",
    "    fig, axes = plt.subplots(n, 2, figsize=(12, 5 * n))\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, (prompt, b_path, s_path) in enumerate(pairs):\n",
    "        baseline_img = Image.open(b_path).convert(\"RGB\")\n",
    "        steered_img = Image.open(s_path).convert(\"RGB\")\n",
    "\n",
    "        axes[i, 0].imshow(baseline_img)\n",
    "        axes[i, 0].set_title(f\"Baseline (no steering)\\n{prompt}\", fontsize=10)\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(steered_img)\n",
    "        axes[i, 1].set_title(f\"Steered (beta={BETA})\\n{prompt}\", fontsize=10)\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Unlearning: {TARGET_CONCEPT} ({TARGET_TYPE}) | Mode: {STEERING_MODE} | beta={BETA}\",\n",
    "        fontsize=14, fontweight=\"bold\", y=1.01\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    vis_path = os.path.join(OUTPUT_DIR, f\"comparison_{TARGET_CONCEPT}.png\")\n",
    "    plt.savefig(vis_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nSaved comparison figure: {vis_path}\")\n",
    "    print(f\"Displayed {n} baseline vs steered pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Results\n",
    "- **UA (Unlearning Accuracy)**: Measures how well the target concept is removed\n",
    "- **IRA (In-Domain Retain Accuracy)**: Measures preservation of related concepts\n",
    "- **CRA (Cross-Domain Retain Accuracy)**: Measures preservation of unrelated concepts\n",
    "\n",
    "### Comparison with Traditional Unlearning Methods\n",
    "Our steering vectors approach is inference-time and does NOT require:\n",
    "- Model retraining\n",
    "- Access to training data\n",
    "- Gradient computation\n",
    "\n",
    "This makes it significantly more efficient than methods like ESD, SalUn, etc.\n",
    "\n",
    "### Notes for Publication\n",
    "1. Use the same prompt format as UnlearnCanvas: `\"A painting of {object} in {style} style\"`\n",
    "2. Report metrics averaged over multiple concepts for robustness\n",
    "3. Consider using the official UnlearnCanvas classifiers for exact comparison"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}